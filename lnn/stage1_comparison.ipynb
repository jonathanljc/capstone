{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "776fb279",
   "metadata": {},
   "source": "# Stage 1 Comparison: Single-Channel vs Multi-Channel Dataset\n## Justification for Multi-Channel Data Collection\n\nTrains the same **DualCircuit_PI_HLNN** on two 600-sample datasets under **equal sample conditions**:\n- **Single-Channel**: channel 5 only — 100 samples per scenario\n- **Multi-Channel**: 4 channels with distinct center frequencies (c1, c3, c4, c7) — **25 samples per channel per scenario**\n\nChannels selected to maximise diversity across both center frequency and bandwidth:\n\n| Channel | Center Freq | Bandwidth | Character |\n|---|---|---|---|\n| c1 | 3494.4 MHz | 499.2 MHz | Low-freq, narrowband |\n| c3 | 4492.8 MHz | 499.2 MHz | Mid-freq, narrowband |\n| c4 | 3993.6 MHz | 1331.2 MHz | Mid-freq, **wideband** |\n| c7 | 6489.6 MHz | 1081.6 MHz | High-freq, **wideband** |\n\nAll four channels have **distinct center frequencies**, spanning 3.5–6.5 GHz with bandwidths ranging from 499 MHz to 1331 MHz.\n\nThree levels of evaluation:\n1. Single 70/15/15 split\n2. Stratified 5-Fold CV (mean ± std)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffea97f",
   "metadata": {},
   "outputs": [],
   "source": "CONFIG = {\n    \"pre_crop\": 10, \"post_crop\": 50, \"total_len\": 60,\n    \"search_start\": 740, \"search_end\": 890,\n    \"hidden_size\": 32, \"input_size\": 1, \"dropout\": 0.2, \"ode_unfolds\": 6,\n    \"batch_size\": 64, \"max_epochs\": 50, \"lr\": 1e-3,\n    \"weight_decay\": 1e-4, \"warmup_epochs\": 3, \"patience\": 40,\n    \"grad_clip\": 1.0, \"val_ratio\": 0.15, \"test_ratio\": 0.15, \"seed\": 42,\n}\nDATA_DIR = \"../dataset/channels/\""
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "903cf6f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import copy, math, contextlib, io\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, ConfusionMatrixDisplay,\n",
    "    classification_report, roc_curve, auc\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "torch.manual_seed(CONFIG[\"seed\"])\n",
    "np.random.seed(CONFIG[\"seed\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ef4cc6",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 1: Data Loading & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ced0eefa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loading functions ready (with FP_AMPL extraction).\n"
     ]
    }
   ],
   "source": [
    "def get_roi_alignment(sig,\n",
    "                      search_start=CONFIG['search_start'],\n",
    "                      search_end=CONFIG['search_end']):\n",
    "    region = sig[search_start:search_end]\n",
    "    if len(region) == 0:\n",
    "        return np.argmax(sig)\n",
    "    peak_idx = search_start + np.argmax(region)\n",
    "    peak_val = sig[peak_idx]\n",
    "    noise_section = sig[:search_start]\n",
    "    if len(noise_section) > 10:\n",
    "        threshold = max(np.mean(noise_section) + 3*np.std(noise_section), 0.05*peak_val)\n",
    "    else:\n",
    "        threshold = 0.05 * peak_val\n",
    "    leading_edge = peak_idx\n",
    "    for i in range(peak_idx, max(search_start - 20, 0), -1):\n",
    "        if sig[i] < threshold:\n",
    "            leading_edge = i + 1\n",
    "            break\n",
    "    return leading_edge\n",
    "\n",
    "\n",
    "def _process_rows(df):\n",
    "    \"\"\"Shared CIR + FP_AMPL preprocessing.\"\"\"\n",
    "    PRE = CONFIG['pre_crop']; TOTAL = CONFIG['total_len']\n",
    "    cir_cols = sorted([c for c in df.columns if c.startswith('CIR')],\n",
    "                      key=lambda x: int(x.replace('CIR', '')))\n",
    "    seqs, labels, fp_features = [], [], []\n",
    "    for _, row in df.iterrows():\n",
    "        sig = pd.to_numeric(row[cir_cols], errors='coerce').fillna(0).astype(float).values\n",
    "        rxpacc = float(row.get('RXPACC', 128.0))\n",
    "        if rxpacc > 0:\n",
    "            sig = sig / rxpacc\n",
    "        # FP_AMPL conditioning features\n",
    "        f1 = float(row.get('FP_AMPL1', 0)) / max(rxpacc, 1) / 64.0\n",
    "        f2 = float(row.get('FP_AMPL2', 0)) / max(rxpacc, 1) / 64.0\n",
    "        f3 = float(row.get('FP_AMPL3', 0)) / max(rxpacc, 1) / 64.0\n",
    "        fp_features.append([f1, f2, f3])\n",
    "        le = get_roi_alignment(sig)\n",
    "        start = max(0, le - PRE); end = start + TOTAL\n",
    "        if end > len(sig): end = len(sig); start = max(0, end - TOTAL)\n",
    "        crop = sig[start:end]\n",
    "        if len(crop) < TOTAL:\n",
    "            crop = np.pad(crop, (0, TOTAL - len(crop)), mode='constant')\n",
    "        lo, hi = crop.min(), crop.max()\n",
    "        crop = (crop - lo) / (hi - lo) if hi > lo else np.zeros(TOTAL)\n",
    "        seqs.append(crop); labels.append(float(row['Label']))\n",
    "    X = np.array(seqs).reshape(-1, TOTAL, 1).astype(np.float32)\n",
    "    y = np.array(labels).astype(np.float32)\n",
    "    F = np.array(fp_features).astype(np.float32)\n",
    "    return X, y, F\n",
    "\n",
    "\n",
    "def load_cir_dataset(filepath):\n",
    "    print(f'Loading: {filepath}')\n",
    "    df = pd.read_csv(filepath)\n",
    "    print(f'  Samples: {len(df)}')\n",
    "    X, y, F = _process_rows(df)\n",
    "    print(f'  Output: X={X.shape}, y={y.shape}, F={F.shape} | LOS={int((y==0).sum())}, NLOS={int((y==1).sum())}')\n",
    "    return X, y, F\n",
    "\n",
    "print(\"Data loading functions ready (with FP_AMPL extraction).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c877ac54",
   "metadata": {},
   "source": "---\n## Section 2: DualCircuit_PI_HLNN Architecture\nIdentical to Stage 1 main notebook — includes FP_AMPL conditioning, tau tracking, and state clamping."
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff69ffad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DualCircuit_PI_HLNN | params: 17,219 | embed_dim=64\n",
      "  FP_AMPL conditioning: Linear(3->32) x 2 circuits\n"
     ]
    }
   ],
   "source": [
    "class PILiquidCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, ode_unfolds=6):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.ode_unfolds = ode_unfolds\n",
    "        self.gleak = nn.Parameter(torch.empty(hidden_size).uniform_(0.001, 1.0))\n",
    "        self.vleak = nn.Parameter(torch.empty(hidden_size).uniform_(-0.2, 0.2))\n",
    "        self.cm    = nn.Parameter(torch.empty(hidden_size).uniform_(0.4, 0.6))\n",
    "        self.w     = nn.Parameter(torch.empty(hidden_size, hidden_size).uniform_(0.001, 1.0))\n",
    "        self.erev  = nn.Parameter(torch.empty(hidden_size, hidden_size).uniform_(-0.2, 0.2))\n",
    "        self.mu    = nn.Parameter(torch.empty(hidden_size, hidden_size).uniform_(0.3, 0.8))\n",
    "        self.sigma = nn.Parameter(torch.empty(hidden_size, hidden_size).uniform_(3, 8))\n",
    "        self.sensory_w     = nn.Parameter(torch.empty(input_size, hidden_size).uniform_(0.001, 1.0))\n",
    "        self.sensory_mu    = nn.Parameter(torch.empty(input_size, hidden_size).uniform_(0.3, 0.8))\n",
    "        self.sensory_sigma = nn.Parameter(torch.empty(input_size, hidden_size).uniform_(3, 8))\n",
    "\n",
    "    def forward(self, x_t, h_prev, dt=1.0):\n",
    "        gleak     = F.softplus(self.gleak)\n",
    "        cm        = F.softplus(self.cm)\n",
    "        w         = F.softplus(self.w)\n",
    "        sensory_w = F.softplus(self.sensory_w)\n",
    "        sensory_gate    = torch.sigmoid(self.sensory_sigma * (x_t.unsqueeze(-1) - self.sensory_mu))\n",
    "        sensory_current = (sensory_w * sensory_gate * x_t.unsqueeze(-1)).sum(dim=1)\n",
    "        cm_t = cm / (dt / self.ode_unfolds)\n",
    "        v = h_prev\n",
    "        for _ in range(self.ode_unfolds):\n",
    "            rg = torch.sigmoid(self.sigma.unsqueeze(0) * (v.unsqueeze(2) - self.mu.unsqueeze(0)))\n",
    "            wg = w.unsqueeze(0) * rg\n",
    "            w_num = (wg * self.erev.unsqueeze(0)).sum(dim=1)\n",
    "            w_den = wg.sum(dim=1)\n",
    "            v = (cm_t*v + gleak*self.vleak + w_num + sensory_current) / (cm_t + gleak + w_den + 1e-8)\n",
    "            v = torch.clamp(v, -1.0, 1.0)\n",
    "        return v, cm / (gleak + w_den + 1e-8)\n",
    "\n",
    "\n",
    "class DualCircuit_PI_HLNN(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_size=32, dropout=0.4, ode_unfolds=6):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.cell_los  = PILiquidCell(input_size, hidden_size, ode_unfolds)\n",
    "        self.cell_nlos = PILiquidCell(input_size, hidden_size, ode_unfolds)\n",
    "        # FP_AMPL conditioning\n",
    "        self.fp_to_los_init  = nn.Linear(3, hidden_size)\n",
    "        self.fp_to_nlos_init = nn.Linear(3, hidden_size)\n",
    "        # Cross-circuit projections\n",
    "        self.P_nlos2los = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.P_los2nlos = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.gate_los   = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.gate_nlos  = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        # Attention pooling\n",
    "        self.los_attn   = nn.Linear(hidden_size, 1)\n",
    "        self.nlos_attn  = nn.Linear(hidden_size, 1)\n",
    "        # Classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size*2, hidden_size), nn.SiLU(),\n",
    "            nn.Dropout(dropout), nn.Linear(hidden_size, 1), nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def _run_circuits(self, x_seq, fp_features=None):\n",
    "        batch_size, seq_len, _ = x_seq.size()\n",
    "        if fp_features is not None:\n",
    "            h_los  = 0.1 * torch.tanh(self.fp_to_los_init(fp_features))\n",
    "            h_nlos = 0.1 * torch.tanh(self.fp_to_nlos_init(fp_features))\n",
    "        else:\n",
    "            h_los  = torch.zeros(batch_size, self.hidden_size, device=x_seq.device)\n",
    "            h_nlos = torch.zeros(batch_size, self.hidden_size, device=x_seq.device)\n",
    "        los_states, nlos_states = [], []\n",
    "        tau_los_sum  = torch.zeros_like(h_los)\n",
    "        tau_nlos_sum = torch.zeros_like(h_nlos)\n",
    "        tau_los_hist_list, tau_nlos_hist_list = [], []\n",
    "        for t in range(seq_len):\n",
    "            x_t = x_seq[:, t, :]\n",
    "            p_n2l = self.P_nlos2los(h_nlos); p_l2n = self.P_los2nlos(h_los)\n",
    "            gl = torch.sigmoid(self.gate_los( torch.cat([h_los,  p_n2l], dim=1)))\n",
    "            gn = torch.sigmoid(self.gate_nlos(torch.cat([h_nlos, p_l2n], dim=1)))\n",
    "            h_los,  tau_los  = self.cell_los( x_t, h_los  + gl * p_n2l)\n",
    "            h_nlos, tau_nlos = self.cell_nlos(x_t, h_nlos + gn * p_l2n)\n",
    "            los_states.append(h_los.unsqueeze(1))\n",
    "            nlos_states.append(h_nlos.unsqueeze(1))\n",
    "            tau_los_sum  += tau_los\n",
    "            tau_nlos_sum += tau_nlos\n",
    "            tau_los_hist_list.append(tau_los.unsqueeze(1))\n",
    "            tau_nlos_hist_list.append(tau_nlos.unsqueeze(1))\n",
    "        los_all  = torch.cat(los_states, dim=1)\n",
    "        nlos_all = torch.cat(nlos_states, dim=1)\n",
    "        tau_los_mean  = tau_los_sum  / seq_len\n",
    "        tau_nlos_mean = tau_nlos_sum / seq_len\n",
    "        tau_los_hist  = torch.cat(tau_los_hist_list, dim=1)\n",
    "        tau_nlos_hist = torch.cat(tau_nlos_hist_list, dim=1)\n",
    "        return los_all, nlos_all, tau_los_hist, tau_nlos_hist, tau_los_mean, tau_nlos_mean\n",
    "\n",
    "    def _pool_and_fuse(self, la, na):\n",
    "        lw = F.softmax(self.los_attn(la).squeeze(-1),  dim=1).unsqueeze(-1)\n",
    "        nw = F.softmax(self.nlos_attn(na).squeeze(-1), dim=1).unsqueeze(-1)\n",
    "        return torch.cat([(la*lw).sum(1), (na*nw).sum(1)], dim=1)\n",
    "\n",
    "    def forward(self, x_seq, fp_features=None, return_dynamics=False):\n",
    "        los_all, nlos_all, tau_los_hist, tau_nlos_hist, tau_los_mean, tau_nlos_mean = \\\n",
    "            self._run_circuits(x_seq, fp_features=fp_features)\n",
    "        pred = self.classifier(self._pool_and_fuse(los_all, nlos_all))\n",
    "        if return_dynamics:\n",
    "            return pred, los_all, nlos_all, tau_los_hist, tau_nlos_hist, tau_los_mean, tau_nlos_mean\n",
    "        return pred, tau_los_mean, tau_nlos_mean\n",
    "\n",
    "    def embed(self, x_seq, fp_features=None):\n",
    "        los_all, nlos_all, _, _, _, _ = self._run_circuits(x_seq, fp_features=fp_features)\n",
    "        return self._pool_and_fuse(los_all, nlos_all)\n",
    "\n",
    "_m = DualCircuit_PI_HLNN()\n",
    "print(f'DualCircuit_PI_HLNN | params: {sum(p.numel() for p in _m.parameters()):,} | embed_dim=64')\n",
    "print(f'  FP_AMPL conditioning: Linear(3->{_m.hidden_size}) x 2 circuits')\n",
    "del _m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33340d00",
   "metadata": {},
   "source": "---\n## Section 3: Training Function\nWith FP_AMPL conditioning, early stopping (patience=10), and best-model restoration."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747b90a1",
   "metadata": {},
   "outputs": [],
   "source": "def train_model(X_train, y_train, X_val, y_val, F_train=None, F_val=None,\n                config=CONFIG, verbose=True, seed=None):\n    _seed = seed if seed is not None else config['seed']\n    torch.manual_seed(_seed)\n    np.random.seed(_seed)\n    fp_enabled = F_train is not None\n\n    X_tr = torch.tensor(X_train).to(device)\n    y_tr = torch.tensor(y_train).unsqueeze(1).to(device)\n    X_va = torch.tensor(X_val).to(device)\n    y_va = torch.tensor(y_val).unsqueeze(1).to(device)\n\n    if fp_enabled:\n        F_tr = torch.tensor(F_train).to(device)\n        F_va = torch.tensor(F_val).to(device)\n        loader = DataLoader(TensorDataset(X_tr, y_tr, F_tr),\n                            batch_size=config['batch_size'], shuffle=True)\n    else:\n        loader = DataLoader(TensorDataset(X_tr, y_tr),\n                            batch_size=config['batch_size'], shuffle=True)\n\n    model = DualCircuit_PI_HLNN(\n        input_size=config['input_size'], hidden_size=config['hidden_size'],\n        dropout=config['dropout'], ode_unfolds=config['ode_unfolds']\n    ).to(device)\n    criterion = nn.BCELoss()\n    optimizer = optim.AdamW(model.parameters(), lr=config['lr'],\n                            weight_decay=config['weight_decay'])\n    T = config['max_epochs']; W = config['warmup_epochs']\n    def lr_lambda(e):\n        if e < W: return (e+1)/W\n        return max(0.01, 0.5*(1+math.cos(math.pi*(e-W)/max(1,T-W))))\n    scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n\n    history = {'train_loss':[], 'val_loss':[], 'train_acc':[], 'val_acc':[]}\n    best_val_acc = 0; best_state = None; patience_counter = 0\n\n    for epoch in range(T):\n        model.train(); tl, tc, tt = 0, 0, 0\n        for batch in loader:\n            if fp_enabled:\n                bx, by, bf = batch\n            else:\n                bx, by = batch; bf = None\n            optimizer.zero_grad()\n            pred, _, _ = model(bx, fp_features=bf)\n            loss = criterion(pred, by); loss.backward()\n            nn.utils.clip_grad_norm_(model.parameters(), config['grad_clip'])\n            optimizer.step()\n            tl += loss.item()*len(bx); tc += ((pred>0.5).float()==by).sum().item(); tt += len(bx)\n        model.eval()\n        with torch.no_grad():\n            val_fp = F_va if fp_enabled else None\n            vp, _, _ = model(X_va, fp_features=val_fp)\n            vl = criterion(vp, y_va).item()\n            va = ((vp>0.5).float()==y_va).float().mean().item()\n        history['train_loss'].append(tl/tt); history['val_loss'].append(vl)\n        history['train_acc'].append(tc/tt);  history['val_acc'].append(va)\n        scheduler.step()\n\n        if va > best_val_acc:\n            best_val_acc = va; best_state = copy.deepcopy(model.state_dict()); patience_counter = 0\n        else:\n            patience_counter += 1\n\n        if verbose and (epoch%10==0 or epoch==T-1):\n            print(f'  Ep {epoch:>3} | Loss: {tl/tt:.4f} | Val Acc: {100*va:.2f}% | Best: {100*best_val_acc:.2f}%')\n        if patience_counter >= config.get('patience', T):\n            if verbose: print(f'  Early stopping at epoch {epoch}')\n            break\n\n    model.load_state_dict(best_state)\n    if verbose: print(f'  Best Val Acc: {100*best_val_acc:.2f}%')\n    return model, history\n\nprint('train_model() ready — 40 epochs max, early stopping with patience=10, FP_AMPL support.')"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6752d8bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluate() helper ready (with FP_AMPL support).\n"
     ]
    }
   ],
   "source": [
    "def evaluate(model, X, y_true, F=None):\n",
    "    \"\"\"Returns metrics dict for a given model and data.\"\"\"\n",
    "    model.eval()\n",
    "    X_t = torch.tensor(X).to(device)\n",
    "    F_t = torch.tensor(F).to(device) if F is not None else None\n",
    "    with torch.no_grad():\n",
    "        pred, _, _ = model(X_t, fp_features=F_t)\n",
    "    y_prob = pred.cpu().numpy().flatten()\n",
    "    y_pred = (y_prob > 0.5).astype(float)\n",
    "    y_true = y_true.flatten()\n",
    "    rep  = classification_report(y_true, y_pred, target_names=['LOS','NLOS'],\n",
    "                                 output_dict=True, zero_division=0)\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
    "    return {\n",
    "        'acc':      rep['accuracy'],\n",
    "        'f1_macro': rep['macro avg']['f1-score'],\n",
    "        'f1_los':   rep['LOS']['f1-score'],\n",
    "        'f1_nlos':  rep['NLOS']['f1-score'],\n",
    "        'auc':      auc(fpr, tpr),\n",
    "        'fpr': fpr, 'tpr': tpr,\n",
    "        'cm':  confusion_matrix(y_true, y_pred),\n",
    "    }\n",
    "\n",
    "print('evaluate() helper ready (with FP_AMPL support).')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b08ba1",
   "metadata": {},
   "source": "---\n## Section 4: Single 70/15/15 Split Comparison\nQuick baseline comparison using one fixed split."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5709038f",
   "metadata": {},
   "outputs": [],
   "source": "def run_experiment(csv_name, label, config=CONFIG, seed=42):\n    print(f\"\\n{'='*60}\\nExperiment: {label}\\n{'='*60}\")\n    X_all, y_all, F_all = load_cir_dataset(DATA_DIR + csv_name)\n    X_tr, X_tmp, y_tr, y_tmp, F_tr, F_tmp = train_test_split(\n        X_all, y_all, F_all, test_size=config['val_ratio']+config['test_ratio'],\n        stratify=y_all, random_state=seed)\n    X_va, X_te, y_va, y_te, F_va, F_te = train_test_split(\n        X_tmp, y_tmp, F_tmp,\n        test_size=config['test_ratio']/(config['val_ratio']+config['test_ratio']),\n        stratify=y_tmp, random_state=seed)\n    print(f'  Train={len(X_tr)}, Val={len(X_va)}, Test={len(X_te)}')\n    model, history = train_model(X_tr, y_tr, X_va, y_va, F_train=F_tr, F_val=F_va, config=config)\n    m = evaluate(model, X_te, y_te, F=F_te)\n    m['label'] = label; m['history'] = history\n    print(f\"\\n  Test Acc={m['acc']:.4f} | Macro F1={m['f1_macro']:.4f} | AUC={m['auc']:.4f}\")\n    return m\n\n\nresults_single = run_experiment('single_channel5_dataset.csv',  'Single-Channel (c5 only)')\nresults_multi  = run_experiment('multi_channel4_dataset.csv',   'Multi-Channel (c1,c3,c4,c7)')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae1df4d",
   "metadata": {},
   "outputs": [],
   "source": "# ── Single-split plots ──────────────────────────────────────────────────\n_res = [results_single, results_multi]\n_colors = ['#3498db', '#e74c3c']\nwidth = 0.3\n\nfig, axs = plt.subplots(1, 4, figsize=(24, 6))\n\n# Bar: key metrics\nax = axs[0]\nmk = ['acc','f1_macro','auc']; mn = ['Accuracy','Macro F1','AUC']\nfor i, (r, c) in enumerate(zip(_res, _colors)):\n    vals = [r[k] for k in mk]\n    bars = ax.bar(np.arange(3)+i*width, vals, width, label=r['label'], color=c, alpha=0.85)\n    for bar, v in zip(bars, vals):\n        ax.text(bar.get_x()+bar.get_width()/2, bar.get_height()+0.005,\n                f'{v:.3f}', ha='center', va='bottom', fontsize=8, fontweight='bold')\nax.set_xticks(np.arange(3)+width/2); ax.set_xticklabels(mn)\nax.set_ylim([0.4,1.12]); ax.set_title('Metric Comparison', fontweight='bold')\nax.legend(fontsize=8); ax.grid(True, alpha=0.3, axis='y')\n\n# ROC\nax = axs[1]\nfor r, c in zip(_res, _colors):\n    ax.plot(r['fpr'], r['tpr'], color=c, lw=2, label=f\"{r['label']} (AUC={r['auc']:.3f})\")\nax.plot([0,1],[0,1],'k--',lw=1,alpha=0.5)\nax.set_title('ROC Curves', fontweight='bold'); ax.set_xlabel('FPR'); ax.set_ylabel('TPR')\nax.legend(fontsize=8); ax.grid(True, alpha=0.3)\n\n# Val acc curves\nax = axs[2]\nfor r, c in zip(_res, _colors):\n    ax.plot(r['history']['val_acc'], color=c, lw=2, label=r['label'])\nax.set_title('Val Accuracy per Epoch', fontweight='bold')\nax.set_xlabel('Epoch'); ax.set_ylabel('Val Accuracy')\nax.set_ylim([0.4,1.05]); ax.legend(fontsize=8); ax.grid(True, alpha=0.3)\n\n# Per-class F1\nax = axs[3]\nfor i, (r, c) in enumerate(zip(_res, _colors)):\n    vals = [r['f1_los'], r['f1_nlos']]\n    bars = ax.bar(np.arange(2)+i*width, vals, width, label=r['label'], color=c, alpha=0.85)\n    for bar, v in zip(bars, vals):\n        ax.text(bar.get_x()+bar.get_width()/2, bar.get_height()+0.005,\n                f'{v:.3f}', ha='center', va='bottom', fontsize=8, fontweight='bold')\nax.set_xticks(np.arange(2)+width/2); ax.set_xticklabels(['LOS F1','NLOS F1'])\nax.set_ylim([0.4,1.12]); ax.set_title('Per-Class F1', fontweight='bold')\nax.legend(fontsize=8); ax.grid(True, alpha=0.3, axis='y')\n\nplt.suptitle('Stage 1: Single-Channel vs Multi-Channel — Single Split',\n             fontsize=14, fontweight='bold', y=1.02)\nplt.tight_layout(); plt.show()\n\nprint('\\n' + '='*55)\nprint(f\"{'Metric':<20} {'Single-Ch':>12} {'Multi-Ch':>12}  Delta\")\nprint('='*55)\nfor k, n in zip(['acc','f1_los','f1_nlos','f1_macro','auc'],\n                ['Accuracy','F1 LOS','F1 NLOS','Macro F1','AUC']):\n    sv=results_single[k]; mv=results_multi[k]; d=mv-sv\n    print(f\"{n:<20} {sv:>12.4f} {mv:>12.4f}   {'▲' if d>0 else '▼'}{abs(d):.4f}\")\nprint('='*55)"
  },
  {
   "cell_type": "markdown",
   "id": "c1ed560e",
   "metadata": {},
   "source": "---\n## Section 5: Stratified 5-Fold Cross-Validation\nMore reliable estimate: averages over 5 different train/test splits.\nEpoch prints are suppressed — only one summary line per fold is shown."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3c7118",
   "metadata": {},
   "outputs": [],
   "source": "def run_kfold(csv_name, label, n_splits=5, config=CONFIG, seed=42):\n    print(f\"\\n{'='*60}\\n5-Fold CV: {label}\\n{'='*60}\")\n    X_all, y_all, F_all = load_cir_dataset(DATA_DIR + csv_name)\n    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n    fold_metrics = []\n    for fold, (tv_idx, te_idx) in enumerate(skf.split(X_all, y_all)):\n        X_tv, X_te = X_all[tv_idx], X_all[te_idx]\n        y_tv, y_te = y_all[tv_idx], y_all[te_idx]\n        F_tv, F_te = F_all[tv_idx], F_all[te_idx]\n        X_tr, X_va, y_tr, y_va, F_tr, F_va = train_test_split(\n            X_tv, y_tv, F_tv, test_size=0.15, stratify=y_tv, random_state=seed)\n        fold_seed = seed + fold\n        with contextlib.redirect_stdout(io.StringIO()):\n            model, _ = train_model(X_tr, y_tr, X_va, y_va, F_train=F_tr, F_val=F_va,\n                                   config=config, verbose=False, seed=fold_seed)\n        fm = evaluate(model, X_te, y_te, F=F_te)\n        fold_metrics.append(fm)\n        collapsed = \" [COLLAPSED]\" if fm['acc'] <= 0.51 else \"\"\n        print(f\"  Fold {fold+1}/{n_splits} | Acc={fm['acc']:.4f} | F1={fm['f1_macro']:.4f} | AUC={fm['auc']:.4f}{collapsed}\")\n    summary = {'label': label}\n    print(f\"\\n  {'─'*45}\")\n    print(f\"  {'Metric':<12} {'Mean':>8} {'Std':>8}\")\n    print(f\"  {'─'*45}\")\n    for key in ['acc','f1_macro','f1_los','f1_nlos','auc']:\n        vals = np.array([m[key] for m in fold_metrics])\n        summary[key] = {'mean': vals.mean(), 'std': vals.std(), 'all': vals.tolist()}\n        print(f\"  {key:<12} {vals.mean():>8.4f} {vals.std():>8.4f}\")\n    print(f\"  {'─'*45}\")\n    return summary\n\n\nkfold_single = run_kfold('single_channel5_dataset.csv', 'Single-Channel (c5 only)')\nkfold_multi  = run_kfold('multi_channel4_dataset.csv',  'Multi-Channel (c1,c3,c4,c7)')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81727faf",
   "metadata": {},
   "outputs": [],
   "source": "# ── K-Fold comparison plots ────────────────────────────────────────────\n_kf = [kfold_single, kfold_multi]\n_colors = ['#3498db', '#e74c3c']\n_mk = ['acc','f1_macro','f1_los','f1_nlos','auc']\n_mn = ['Accuracy','Macro F1','F1 LOS','F1 NLOS','AUC']\nwidth = 0.3\n\nfig, axs = plt.subplots(1, 2, figsize=(18, 6))\n\n# Bar with error bars\nax = axs[0]\nx = np.arange(len(_mk))\nfor i, (kf, c) in enumerate(zip(_kf, _colors)):\n    means = [kf[m]['mean'] for m in _mk]\n    stds  = [kf[m]['std']  for m in _mk]\n    bars = ax.bar(x+i*width, means, width, yerr=stds, label=kf['label'],\n                  color=c, alpha=0.85, capsize=4, error_kw={'elinewidth':1.5})\n    for bar, m, s in zip(bars, means, stds):\n        ax.text(bar.get_x()+bar.get_width()/2, bar.get_height()+s+0.008,\n                f'{m:.3f}', ha='center', va='bottom', fontsize=7.5, fontweight='bold')\nax.set_xticks(x+width/2); ax.set_xticklabels(_mn, rotation=10)\nax.set_ylim([0.4,1.15]); ax.set_title('5-Fold CV: Mean ± Std', fontweight='bold')\nax.set_ylabel('Score'); ax.legend(fontsize=9); ax.grid(True, alpha=0.3, axis='y')\n\n# Box plots\nax = axs[1]\nall_data, all_colors, positions = [], [], []\nfor mi, metric in enumerate(_mk):\n    for ki, (kf, c) in enumerate(zip(_kf, _colors)):\n        positions.append(mi*3 + ki)\n        all_data.append(kf[metric]['all'])\n        all_colors.append(c)\nbp = ax.boxplot(all_data, positions=positions, widths=0.7, patch_artist=True,\n                medianprops={'color':'black','linewidth':2})\nfor patch, c in zip(bp['boxes'], all_colors):\n    patch.set_facecolor(c); patch.set_alpha(0.7)\nax.set_xticks([mi*3+0.5 for mi in range(len(_mk))])\nax.set_xticklabels(_mn, rotation=10)\nax.set_title('Score Distribution per Fold (blue=single, red=multi)', fontweight='bold')\nax.set_ylabel('Score'); ax.grid(True, alpha=0.3, axis='y')\n\nplt.suptitle('Stage 1: Stratified 5-Fold CV — Single vs Multi Channel',\n             fontsize=14, fontweight='bold', y=1.02)\nplt.tight_layout(); plt.show()\n\nprint('\\n' + '='*65)\nprint(f\"{'Metric':<12} {'Single Mean±Std':>20} {'Multi Mean±Std':>20}  Delta\")\nprint('='*65)\nfor k, n in zip(_mk, _mn):\n    sm,ss = kfold_single[k]['mean'], kfold_single[k]['std']\n    mm,ms = kfold_multi[k]['mean'],  kfold_multi[k]['std']\n    d = mm - sm\n    print(f\"{n:<12} {sm:>8.4f}±{ss:.4f}      {mm:>8.4f}±{ms:.4f}   {'▲' if d>0 else '▼'}{abs(d):.4f}\")\nprint('='*65)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}