{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 1: Physics-Informed Hybrid Liquid Neural Network (PI-HLNN)\n",
    "## LOS / NLOS Classification from Raw CIR\n",
    "\n",
    "**Architecture**: Pure Liquid Neural Network with input+state driven time constants (tau).\n",
    "**Physics-Informed**: RXPACC normalization, ODE dynamics, tau via `softplus` (no hardcoded targets).\n",
    "**Hybrid**: Combines physics priors (preprocessing, loss, ODE structure) with data-driven learned dynamics.\n",
    "**Input**: Raw 60-sample CIR window â€” no hand-crafted features. The LNN learns temporal dynamics directly from the signal.\n",
    "**Training**: `combined_uwb_dataset.csv` (3600 samples, 6 UWB channels), 70/15/15 train/val/test split.\n",
    "**Pipeline**: **Stage 1 (LNN â†’ LOS/NLOS)** â†’ Stage 2 (MLP â†’ single/multi bounce) â†’ Stage 3 (MLP â†’ ranging error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    \"pre_crop\": 10,\n",
    "    \"post_crop\": 50,\n",
    "    \"total_len\": 60,\n",
    "    \"search_start\": 740,\n",
    "    \"search_end\": 890,\n",
    "    \"hidden_size\": 32,        # per circuit; total embedding = 64 (2 × 32)\n",
    "    \"input_size\": 1,\n",
    "    \"dropout\": 0.2,\n",
    "    \"ode_unfolds\": 6,\n",
    "    \"batch_size\": 64,\n",
    "    \"max_epochs\": 40,\n",
    "    \"lr\": 1e-3,\n",
    "    \"weight_decay\": 1e-4,\n",
    "    \"warmup_epochs\": 3,\n",
    "    \"patience\": 10,\n",
    "    \"grad_clip\": 1.0,\n",
    "    \"val_ratio\": 0.15,\n",
    "    \"test_ratio\": 0.15,\n",
    "    \"seed\": 42,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 2: Data Loading, ROI Alignment & 70/15/15 Split\n",
    "\n",
    "**ROI search range**: `[740, 890]` â€” derived from empirical CIR peak analysis:\n",
    "- All CIR peaks across 36 CSV files (LOS + NLOS, 6 channels) fall within indices **743â€“807**.\n",
    "- `search_start=740`: 3 indices before the earliest observed peak (743), provides margin.\n",
    "- `search_end=890`: ~80 indices past the latest observed peak (807), captures any multipath tail.\n",
    "- Noise floor estimated from indices 0â€“739 (740 samples of pure noise = robust estimate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# SMART ROI ALIGNMENT\n",
    "# ==========================================\n",
    "def get_roi_alignment(sig, search_start=CONFIG[\"search_start\"],\n",
    "                      search_end=CONFIG[\"search_end\"]):\n",
    "    \"\"\"\n",
    "    Find the leading edge of the pulse by backtracking from peak.\n",
    "    Uses noise floor estimation (mean + 3*std) or 5% of peak.\n",
    "\n",
    "    Search range [740, 890] derived from empirical CIR peak analysis:\n",
    "      - All peaks across 36 files fall within indices 743-807\n",
    "      - 740 start = margin before earliest peak\n",
    "      - 890 end = margin past latest multipath outlier\n",
    "    \"\"\"\n",
    "    region = sig[search_start:search_end]\n",
    "    if len(region) == 0:\n",
    "        return np.argmax(sig)\n",
    "\n",
    "    peak_local = np.argmax(region)\n",
    "    peak_idx = search_start + peak_local\n",
    "    peak_val = sig[peak_idx]\n",
    "\n",
    "    # Noise floor from samples before the search region\n",
    "    noise_section = sig[:search_start]\n",
    "    if len(noise_section) > 10:\n",
    "        noise_mean = np.mean(noise_section)\n",
    "        noise_std = np.std(noise_section)\n",
    "        threshold = max(noise_mean + 3 * noise_std, 0.05 * peak_val)\n",
    "    else:\n",
    "        threshold = 0.05 * peak_val\n",
    "\n",
    "    # Backtrack from peak to find leading edge\n",
    "    leading_edge = peak_idx\n",
    "    for i in range(peak_idx, max(search_start - 20, 0), -1):\n",
    "        if sig[i] < threshold:\n",
    "            leading_edge = i + 1\n",
    "            break\n",
    "\n",
    "    return leading_edge\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# DATASET LOADER (CIR-ONLY)\n",
    "# ==========================================\n",
    "def load_cir_dataset(filepath=\"../dataset/channels/combined_uwb_dataset.csv\"):\n",
    "    \"\"\"\n",
    "    Load and preprocess CIR data. Returns only the CIR sequence and labels.\n",
    "    No hand-crafted features â€” the LNN learns directly from the signal.\n",
    "    \"\"\"\n",
    "    PRE = CONFIG[\"pre_crop\"]\n",
    "    POST = CONFIG[\"post_crop\"]\n",
    "    TOTAL = CONFIG[\"total_len\"]\n",
    "\n",
    "    processed_seqs = []\n",
    "    labels = []\n",
    "\n",
    "    print(f\"Loading: {filepath}\")\n",
    "    df = pd.read_csv(filepath)\n",
    "    cir_cols = sorted(\n",
    "        [c for c in df.columns if c.startswith('CIR')],\n",
    "        key=lambda x: int(x.replace('CIR', ''))\n",
    "    )\n",
    "    print(f\"  Samples: {len(df)}, CIR columns: {len(cir_cols)}\")\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        # 1. Parse raw signal\n",
    "        sig = pd.to_numeric(row[cir_cols], errors='coerce').fillna(0).astype(float).values\n",
    "\n",
    "        # 2. PHYSICS NORMALIZATION: divide by RXPACC\n",
    "        rxpacc_col = 'RXPACC' if 'RXPACC' in row.index else 'RX_PACC'\n",
    "        rxpacc = float(row.get(rxpacc_col, 128.0))\n",
    "        if rxpacc > 0:\n",
    "            sig = sig / rxpacc\n",
    "\n",
    "        # 3. Smart ROI alignment\n",
    "        leading_edge = get_roi_alignment(sig)\n",
    "\n",
    "        # 4. Crop around leading edge\n",
    "        start = max(0, leading_edge - PRE)\n",
    "        end = start + TOTAL\n",
    "        if end > len(sig):\n",
    "            end = len(sig)\n",
    "            start = max(0, end - TOTAL)\n",
    "\n",
    "        crop = sig[start:end]\n",
    "        if len(crop) < TOTAL:\n",
    "            crop = np.pad(crop, (0, TOTAL - len(crop)), mode='constant')\n",
    "\n",
    "        # 5. Instance normalization [0, 1]\n",
    "        local_min = np.min(crop)\n",
    "        local_max = np.max(crop)\n",
    "        rng = local_max - local_min\n",
    "        if rng > 0:\n",
    "            crop = (crop - local_min) / rng\n",
    "        else:\n",
    "            crop = np.zeros(TOTAL)\n",
    "\n",
    "        processed_seqs.append(crop)\n",
    "        labels.append(float(row['Label']))\n",
    "\n",
    "    X = np.array(processed_seqs).reshape(-1, TOTAL, 1).astype(np.float32)\n",
    "    y = np.array(labels).astype(np.float32)\n",
    "\n",
    "    print(f\"  Output shape: X={X.shape}, y={y.shape}\")\n",
    "    print(f\"  LOS: {int(np.sum(y == 0))}, NLOS: {int(np.sum(y == 1))}\")\n",
    "    return X, y\n",
    "\n",
    "\n",
    "# Load and split 70/15/15\n",
    "X_all, y_all = load_cir_dataset(\"../dataset/channels/combined_uwb_dataset.csv\")\n",
    "\n",
    "# 70/15/15 stratified split\n",
    "val_ratio = CONFIG[\"val_ratio\"]\n",
    "test_ratio = CONFIG[\"test_ratio\"]\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X_all, y_all,\n",
    "    test_size=val_ratio + test_ratio,\n",
    "    stratify=y_all,\n",
    "    random_state=CONFIG[\"seed\"]\n",
    ")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp,\n",
    "    test_size=test_ratio / (val_ratio + test_ratio),\n",
    "    stratify=y_temp,\n",
    "    random_state=CONFIG[\"seed\"]\n",
    ")\n",
    "\n",
    "print(f\"\\nSplit (70/15/15):\")\n",
    "print(f\"  Train: {X_train.shape[0]} (LOS: {int(np.sum(y_train==0))}, NLOS: {int(np.sum(y_train==1))})\")\n",
    "print(f\"  Val:   {X_val.shape[0]} (LOS: {int(np.sum(y_val==0))}, NLOS: {int(np.sum(y_val==1))})\")\n",
    "print(f\"  Test:  {X_test.shape[0]} (LOS: {int(np.sum(y_test==0))}, NLOS: {int(np.sum(y_test==1))})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 3: Model Architecture — Dual-Circuit PI-HLNN\n",
    "\n",
    "### Design: Two Parallel PILiquidCell Circuits with Cross-Circuit Communication\n",
    "\n",
    "Two specialised LTC circuits run in parallel on the same CIR sequence:\n",
    "\n",
    "- **cell_los** — develops dynamics suited to LOS-type CIR (sharp, single-peak)\n",
    "- **cell_nlos** — develops dynamics suited to NLOS-type CIR (broader, multi-peak)\n",
    "\n",
    "At every timestep the circuits **talk to each other** via gated projection matrices:\n",
    "\n",
    "$$h_{\\text{los,in}} = h_{\\text{los}} + g_{\\text{los}} \\cdot P_{\\text{nlos}\\to\\text{los}}(h_{\\text{nlos}})$$\n",
    "$$h_{\\text{nlos,in}} = h_{\\text{nlos}} + g_{\\text{nlos}} \\cdot P_{\\text{los}\\to\\text{nlos}}(h_{\\text{los}})$$\n",
    "\n",
    "where $g = \\sigma(\\text{Linear}([h_{\\text{own}},\\, h_{\\text{cross}}]))$ is a learned sigmoid gate.\n",
    "\n",
    "Each circuit's hidden state is attention-pooled independently, then **concatenated** to form a 64-dim fused embedding:\n",
    "\n",
    "```\n",
    "cell_los  (hidden=32) → attn → h_los_pooled  (32)  ┐\n",
    "                                                      ├─ cat → 64-dim → classifier\n",
    "cell_nlos (hidden=32) → attn → h_nlos_pooled (32)  ┘\n",
    "```\n",
    "\n",
    "**Parameter count (32+32 dual)**: ~17 k — comparable to the original single-circuit (64-hidden, ~19 k).  \n",
    "**Embedding dim = 64** — same as before, so Stages 2 & 3 are unchanged.\n",
    "\n",
    "### Why τ is still adaptive\n",
    "\n",
    "In each PILiquidCell:  $\\tau = C_m / (g_{\\text{leak}} + \\sum w \\cdot \\text{gate}(v))$\n",
    "\n",
    "τ adapts per timestep to the signal energy. The cross-circuit gate additionally allows the NLOS circuit to slow down the LOS circuit's dynamics (increase τ) when it detects a complex multipath structure — implementing the FP-to-LOS threshold insight from the EDA.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class PILiquidCell(nn.Module):\n",
    "    \"\"\"\n",
    "    Conductance-based LTC cell (Hasani et al. 2020).\n",
    "    - Recurrent synapses: full conductance with reversal potentials\n",
    "    - Sensory synapses: gated additive input\n",
    "    - Softplus on conductances only\n",
    "    - ODE solved via semi-implicit Euler\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, ode_unfolds=6):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size  = input_size\n",
    "        self.ode_unfolds = ode_unfolds\n",
    "\n",
    "        self.gleak = nn.Parameter(torch.empty(hidden_size).uniform_(0.001, 1.0))\n",
    "        self.vleak = nn.Parameter(torch.empty(hidden_size).uniform_(-0.2, 0.2))\n",
    "        self.cm    = nn.Parameter(torch.empty(hidden_size).uniform_(0.4, 0.6))\n",
    "\n",
    "        self.w     = nn.Parameter(torch.empty(hidden_size, hidden_size).uniform_(0.001, 1.0))\n",
    "        self.erev  = nn.Parameter(torch.empty(hidden_size, hidden_size).uniform_(-0.2, 0.2))\n",
    "        self.mu    = nn.Parameter(torch.empty(hidden_size, hidden_size).uniform_(0.3, 0.8))\n",
    "        self.sigma = nn.Parameter(torch.empty(hidden_size, hidden_size).uniform_(3, 8))\n",
    "\n",
    "        self.sensory_w     = nn.Parameter(torch.empty(input_size, hidden_size).uniform_(0.001, 1.0))\n",
    "        self.sensory_mu    = nn.Parameter(torch.empty(input_size, hidden_size).uniform_(0.3, 0.8))\n",
    "        self.sensory_sigma = nn.Parameter(torch.empty(input_size, hidden_size).uniform_(3, 8))\n",
    "\n",
    "    def forward(self, x_t, h_prev, dt=1.0):\n",
    "        gleak     = F.softplus(self.gleak)\n",
    "        cm        = F.softplus(self.cm)\n",
    "        w         = F.softplus(self.w)\n",
    "        sensory_w = F.softplus(self.sensory_w)\n",
    "\n",
    "        sensory_gate    = torch.sigmoid(self.sensory_sigma * (x_t.unsqueeze(-1) - self.sensory_mu))\n",
    "        sensory_current = (sensory_w * sensory_gate * x_t.unsqueeze(-1)).sum(dim=1)\n",
    "\n",
    "        cm_t = cm / (dt / self.ode_unfolds)\n",
    "        v    = h_prev\n",
    "\n",
    "        for _ in range(self.ode_unfolds):\n",
    "            recurrent_gate = torch.sigmoid(self.sigma.unsqueeze(0) * (v.unsqueeze(2) - self.mu.unsqueeze(0)))\n",
    "            w_gate = w.unsqueeze(0) * recurrent_gate\n",
    "            w_num  = (w_gate * self.erev.unsqueeze(0)).sum(dim=1)\n",
    "            w_den  = w_gate.sum(dim=1)\n",
    "            numerator   = cm_t * v + gleak * self.vleak + w_num + sensory_current\n",
    "            denominator = cm_t + gleak + w_den + 1e-8\n",
    "            v = numerator / denominator\n",
    "\n",
    "        tau = cm / (gleak + w_den + 1e-8)\n",
    "        return v, tau\n",
    "\n",
    "\n",
    "class DualCircuit_PI_HLNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Dual-circuit PI-HLNN with cross-circuit communication.\n",
    "    Two parallel PILiquidCell circuits (hidden_size each):\n",
    "      - cell_los:  specialises in LOS channel dynamics (sharp, single-peak CIR)\n",
    "      - cell_nlos: specialises in NLOS channel dynamics (broader, multi-peak CIR)\n",
    "    At each timestep, circuits exchange information via gated projection matrices.\n",
    "    Output: 2*hidden_size fused embedding (32+32 = 64-dim) → same as single-circuit baseline.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size=1, hidden_size=32, dropout=0.4, ode_unfolds=6):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size  # per circuit\n",
    "\n",
    "        # Two specialised circuits\n",
    "        self.cell_los  = PILiquidCell(input_size, hidden_size, ode_unfolds)\n",
    "        self.cell_nlos = PILiquidCell(input_size, hidden_size, ode_unfolds)\n",
    "\n",
    "        # Cross-circuit projection matrices (no bias — pure linear mixing)\n",
    "        self.P_nlos2los = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.P_los2nlos = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "\n",
    "        # Gated cross-circuit mixing: [own_state | projected_cross] → gate\n",
    "        self.gate_los  = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.gate_nlos = nn.Linear(hidden_size * 2, hidden_size)\n",
    "\n",
    "        # Per-circuit attention pooling\n",
    "        self.los_attn  = nn.Linear(hidden_size, 1)\n",
    "        self.nlos_attn = nn.Linear(hidden_size, 1)\n",
    "\n",
    "        # Classifier: 2*hidden → hidden → 1\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2, hidden_size),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def _run_circuits(self, x_seq):\n",
    "        batch_size, seq_len, _ = x_seq.size()\n",
    "        h_los  = torch.zeros(batch_size, self.hidden_size, device=x_seq.device)\n",
    "        h_nlos = torch.zeros(batch_size, self.hidden_size, device=x_seq.device)\n",
    "\n",
    "        los_states, nlos_states = [], []\n",
    "        tau_los_sum  = torch.zeros_like(h_los)\n",
    "        tau_nlos_sum = torch.zeros_like(h_nlos)\n",
    "        tau_los_hist_list, tau_nlos_hist_list = [], []\n",
    "\n",
    "        for t in range(seq_len):\n",
    "            x_t = x_seq[:, t, :]\n",
    "\n",
    "            proj_nlos_to_los = self.P_nlos2los(h_nlos)\n",
    "            proj_los_to_nlos = self.P_los2nlos(h_los)\n",
    "\n",
    "            g_los  = torch.sigmoid(self.gate_los( torch.cat([h_los,  proj_nlos_to_los], dim=1)))\n",
    "            g_nlos = torch.sigmoid(self.gate_nlos(torch.cat([h_nlos, proj_los_to_nlos], dim=1)))\n",
    "\n",
    "            h_los_in  = h_los  + g_los  * proj_nlos_to_los\n",
    "            h_nlos_in = h_nlos + g_nlos * proj_los_to_nlos\n",
    "\n",
    "            h_los,  tau_los  = self.cell_los( x_t, h_los_in)\n",
    "            h_nlos, tau_nlos = self.cell_nlos(x_t, h_nlos_in)\n",
    "\n",
    "            los_states.append(h_los.unsqueeze(1))\n",
    "            nlos_states.append(h_nlos.unsqueeze(1))\n",
    "            tau_los_sum  += tau_los\n",
    "            tau_nlos_sum += tau_nlos\n",
    "            tau_los_hist_list.append(tau_los.unsqueeze(1))\n",
    "            tau_nlos_hist_list.append(tau_nlos.unsqueeze(1))\n",
    "\n",
    "        los_all  = torch.cat(los_states,  dim=1)\n",
    "        nlos_all = torch.cat(nlos_states, dim=1)\n",
    "        tau_los_mean  = tau_los_sum  / seq_len\n",
    "        tau_nlos_mean = tau_nlos_sum / seq_len\n",
    "        tau_los_hist  = torch.cat(tau_los_hist_list,  dim=1)\n",
    "        tau_nlos_hist = torch.cat(tau_nlos_hist_list, dim=1)\n",
    "        return los_all, nlos_all, tau_los_hist, tau_nlos_hist, tau_los_mean, tau_nlos_mean\n",
    "\n",
    "    def _pool_and_fuse(self, los_all, nlos_all):\n",
    "        los_w  = F.softmax(self.los_attn(los_all).squeeze(-1),   dim=1).unsqueeze(-1)\n",
    "        nlos_w = F.softmax(self.nlos_attn(nlos_all).squeeze(-1), dim=1).unsqueeze(-1)\n",
    "        h_los_pooled  = (los_all  * los_w).sum(dim=1)\n",
    "        h_nlos_pooled = (nlos_all * nlos_w).sum(dim=1)\n",
    "        return torch.cat([h_los_pooled, h_nlos_pooled], dim=1)   # (batch, 2*hidden)\n",
    "\n",
    "    def forward(self, x_seq, return_dynamics=False):\n",
    "        los_all, nlos_all, tau_los_hist, tau_nlos_hist, tau_los_mean, tau_nlos_mean = \\\n",
    "            self._run_circuits(x_seq)\n",
    "        h_fused = self._pool_and_fuse(los_all, nlos_all)\n",
    "        pred = self.classifier(h_fused)\n",
    "        if return_dynamics:\n",
    "            return pred, los_all, nlos_all, tau_los_hist, tau_nlos_hist, tau_los_mean, tau_nlos_mean\n",
    "        return pred, tau_los_mean, tau_nlos_mean\n",
    "\n",
    "    def embed(self, x_seq):\n",
    "        \"\"\"Return 64-dim fused embedding for Stage 2/3.\"\"\"\n",
    "        los_all, nlos_all, _, _, _, _ = self._run_circuits(x_seq)\n",
    "        return self._pool_and_fuse(los_all, nlos_all)\n",
    "\n",
    "\n",
    "# Parameter count\n",
    "_m = DualCircuit_PI_HLNN(input_size=1, hidden_size=32)\n",
    "_total = sum(p.numel() for p in _m.parameters())\n",
    "print(f\"DualCircuit_PI_HLNN parameter count: {_total:,}\")\n",
    "print(f\"  cell_los + cell_nlos:  {sum(p.numel() for p in _m.cell_los.parameters()) * 2:,}\")\n",
    "print(f\"  Projection matrices:   {sum(p.numel() for p in [*_m.P_nlos2los.parameters(), *_m.P_los2nlos.parameters()]):,}\")\n",
    "print(f\"  Gates:                 {sum(p.numel() for p in [*_m.gate_los.parameters(), *_m.gate_nlos.parameters()]):,}\")\n",
    "print(f\"  Attention + classifier:{sum(p.numel() for p in [*_m.los_attn.parameters(), *_m.nlos_attn.parameters(), *_m.classifier.parameters()]):,}\")\n",
    "print(f\"  Embedding dim: {_m.hidden_size * 2} (2 × {_m.hidden_size})\")\n",
    "del _m\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 4: Loss Function\n",
    "\n",
    "$$\\mathcal{L} = \\mathcal{L}_{BCE}$$\n",
    "\n",
    "Pure binary cross-entropy. The time constant $\\tau$ is **not** constrained by the loss â€” it emerges entirely from the ODE dynamics driven by `[x_t, h_t]`. This lets the LNN discover its own temporal behavior from the signal.\n",
    "\n",
    "The tau values are still computed and available for post-training diagnostics (tau distribution, temporal evolution)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# LOSS FUNCTION (Pure BCE)\n",
    "# ==========================================\n",
    "criterion_fn = nn.BCELoss()\n",
    "\n",
    "print(\"Loss: Binary Cross-Entropy (pure BCE, no tau constraint)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 5: Training Pipeline (70/15/15 Split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==========================================\n",
    "# TRAINING PIPELINE (70/15/15)\n",
    "# ==========================================\n",
    "import math\n",
    "\n",
    "def train_model(X_train, y_train, X_val, y_val, config=CONFIG):\n",
    "    print(f\"Training on {len(X_train)} samples, validating on {len(X_val)}\")\n",
    "\n",
    "    X_tr = torch.tensor(X_train).to(device)\n",
    "    y_tr = torch.tensor(y_train).unsqueeze(1).to(device)\n",
    "    X_va = torch.tensor(X_val).to(device)\n",
    "    y_va = torch.tensor(y_val).unsqueeze(1).to(device)\n",
    "\n",
    "    train_ds     = TensorDataset(X_tr, y_tr)\n",
    "    train_loader = DataLoader(train_ds, batch_size=config[\"batch_size\"], shuffle=True)\n",
    "\n",
    "    model = DualCircuit_PI_HLNN(\n",
    "        input_size=config[\"input_size\"],\n",
    "        hidden_size=config[\"hidden_size\"],\n",
    "        dropout=config[\"dropout\"],\n",
    "        ode_unfolds=config.get(\"ode_unfolds\", 6)\n",
    "    ).to(device)\n",
    "\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=config[\"lr\"],\n",
    "                            weight_decay=config[\"weight_decay\"])\n",
    "\n",
    "    warmup_epochs = config[\"warmup_epochs\"]\n",
    "    total_epochs  = config[\"max_epochs\"]\n",
    "\n",
    "    def lr_lambda(epoch):\n",
    "        if epoch < warmup_epochs:\n",
    "            return (epoch + 1) / warmup_epochs\n",
    "        progress = (epoch - warmup_epochs) / max(1, total_epochs - warmup_epochs)\n",
    "        return max(0.01, 0.5 * (1.0 + math.cos(math.pi * progress)))\n",
    "\n",
    "    scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda)\n",
    "\n",
    "    history = {\"train_loss\": [], \"val_loss\": [], \"train_acc\": [], \"val_acc\": [], \"lr\": []}\n",
    "    best_val_acc      = 0\n",
    "    best_model_state  = None\n",
    "    patience_counter  = 0\n",
    "\n",
    "    for epoch in range(config[\"max_epochs\"]):\n",
    "        model.train()\n",
    "        train_loss_sum = 0\n",
    "        train_correct, train_total = 0, 0\n",
    "\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            pred, tau_los, tau_nlos = model(batch_x)\n",
    "            loss = criterion(pred, batch_y)\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), config[\"grad_clip\"])\n",
    "            optimizer.step()\n",
    "            train_loss_sum += loss.item() * len(batch_x)\n",
    "            train_correct  += ((pred > 0.5).float() == batch_y).sum().item()\n",
    "            train_total    += len(batch_x)\n",
    "\n",
    "        train_loss = train_loss_sum / train_total\n",
    "        train_acc  = train_correct / train_total\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_pred, val_tau_los, val_tau_nlos = model(X_va)\n",
    "            val_loss = criterion(val_pred, y_va)\n",
    "            val_acc  = ((val_pred > 0.5).float() == y_va).float().mean().item()\n",
    "\n",
    "        lr_now = optimizer.param_groups[0]['lr']\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"val_loss\"].append(val_loss.item())\n",
    "        history[\"train_acc\"].append(train_acc)\n",
    "        history[\"val_acc\"].append(val_acc)\n",
    "        history[\"lr\"].append(lr_now)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc     = val_acc\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if epoch % 5 == 0 or epoch == config[\"max_epochs\"] - 1:\n",
    "            print(f\"  Ep {epoch:>3} | Loss: {train_loss:.4f} | Val Acc: {100*val_acc:.2f}% | Best: {100*best_val_acc:.2f}% | LR: {lr_now:.1e}\")\n",
    "\n",
    "        if patience_counter >= config[\"patience\"]:\n",
    "            print(f\"  Early stopping at epoch {epoch}\")\n",
    "            break\n",
    "\n",
    "    model.load_state_dict(best_model_state)\n",
    "    print(f\"\\nBest Validation Accuracy: {100*best_val_acc:.2f}%\")\n",
    "    return model, (X_va, y_va), history\n",
    "\n",
    "\n",
    "best_model, best_data, best_history = train_model(X_train, y_train, X_val, y_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 6: Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==========================================\n",
    "# DIAGNOSTIC GRID (3x2) — DualCircuit_PI_HLNN\n",
    "# ==========================================\n",
    "def plot_diagnostics(model, val_data, history):\n",
    "    X_va, y_va = val_data\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        preds, los_hist, nlos_hist, tau_los_hist, tau_nlos_hist, tau_los_mean, tau_nlos_mean = \\\n",
    "            model(X_va, return_dynamics=True)\n",
    "\n",
    "    y_true = y_va.cpu().numpy().flatten()\n",
    "    y_prob = preds.cpu().numpy().flatten()\n",
    "    y_pred = (y_prob > 0.5).astype(float)\n",
    "\n",
    "    # Average tau across neurons for each sample\n",
    "    tau_los_np  = tau_los_mean.cpu().numpy().mean(axis=1)   # (batch,)\n",
    "    tau_nlos_np = tau_nlos_mean.cpu().numpy().mean(axis=1)\n",
    "\n",
    "    los_hist_np  = los_hist.cpu().numpy()   # (batch, seq_len, 32)\n",
    "    nlos_hist_np = nlos_hist.cpu().numpy()\n",
    "\n",
    "    fig, axs = plt.subplots(2, 3, figsize=(24, 14))\n",
    "    plt.subplots_adjust(hspace=0.35, wspace=0.3)\n",
    "\n",
    "    # --- 1. LEARNING CURVES ---\n",
    "    ax = axs[0, 0]\n",
    "    ax.plot(history[\"train_loss\"], label='Train Loss', color='#3498db', lw=2)\n",
    "    ax.plot(history[\"val_loss\"],   label='Val Loss',   color='#e74c3c', lw=2, ls='--')\n",
    "    ax.set_title(\"Learning Curves\")\n",
    "    ax.set_xlabel(\"Epoch\"); ax.set_ylabel(\"Loss (BCE)\")\n",
    "    ax.legend(fontsize=9); ax.grid(True, alpha=0.3)\n",
    "\n",
    "    # --- 2. ACCURACY CURVES ---\n",
    "    ax = axs[0, 1]\n",
    "    ax.plot(history[\"train_acc\"], label='Train Acc', color='#3498db', lw=2)\n",
    "    ax.plot(history[\"val_acc\"],   label='Val Acc',   color='#e74c3c', lw=2, ls='--')\n",
    "    ax.set_title(\"Accuracy Curves\")\n",
    "    ax.set_xlabel(\"Epoch\"); ax.set_ylabel(\"Accuracy\")\n",
    "    ax.set_ylim([0.4, 1.05]); ax.legend(); ax.grid(True, alpha=0.3)\n",
    "\n",
    "    # --- 3. CONFUSION MATRIX ---\n",
    "    ax = axs[0, 2]\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(cm, display_labels=['LOS', 'NLOS'])\n",
    "    disp.plot(ax=ax, cmap='Blues', colorbar=False)\n",
    "    acc = (y_true == y_pred).mean()\n",
    "    ax.set_title(f\"Confusion Matrix (Acc: {100*acc:.1f}%)\")\n",
    "\n",
    "    # --- 4. ROC CURVE ---\n",
    "    ax = axs[1, 0]\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    ax.plot(fpr, tpr, color='#e74c3c', lw=2, label=f'AUC = {roc_auc:.4f}')\n",
    "    ax.plot([0, 1], [0, 1], 'k--', lw=1, alpha=0.5)\n",
    "    ax.set_title(\"ROC Curve\")\n",
    "    ax.set_xlabel(\"False Positive Rate\"); ax.set_ylabel(\"True Positive Rate\")\n",
    "    ax.legend(loc='lower right'); ax.grid(True, alpha=0.3)\n",
    "\n",
    "    # --- 5. PCA PHASE SPACE (fused 64-dim embedding) ---\n",
    "    ax = axs[1, 1]\n",
    "    batch_size, seq_len, h = los_hist_np.shape\n",
    "    los_idx  = np.where(y_true == 0)[0]\n",
    "    nlos_idx = np.where(y_true == 1)[0]\n",
    "    n_show   = min(len(los_idx), len(nlos_idx), 25)\n",
    "    show_idx = np.concatenate([los_idx[:n_show], nlos_idx[:n_show]])\n",
    "\n",
    "    # Concatenate both circuit histories for PCA\n",
    "    fused_hist = np.concatenate([los_hist_np, nlos_hist_np], axis=2)  # (batch, seq, 64)\n",
    "    h_flat = fused_hist.reshape(-1, h * 2)\n",
    "    pca    = PCA(n_components=2)\n",
    "    h_pca  = pca.fit_transform(h_flat).reshape(batch_size, seq_len, 2)\n",
    "\n",
    "    for i in show_idx:\n",
    "        color = '#2ecc71' if y_true[i] == 0 else '#e74c3c'\n",
    "        ax.plot(h_pca[i, :, 0], h_pca[i, :, 1], color=color, alpha=0.3, lw=0.8)\n",
    "        ax.scatter(h_pca[i, -1, 0], h_pca[i, -1, 1], color=color, s=15, zorder=5)\n",
    "\n",
    "    ax.set_title(f\"Fused Phase Space (PCA on 64-dim, n={n_show*2})\")\n",
    "    ax.set_xlabel(f\"PC1 ({100*pca.explained_variance_ratio_[0]:.1f}%)\")\n",
    "    ax.set_ylabel(f\"PC2 ({100*pca.explained_variance_ratio_[1]:.1f}%)\")\n",
    "    ax.grid(True, alpha=0.2)\n",
    "\n",
    "    # --- 6. TAU DISTRIBUTION — LOS vs NLOS circuits ---\n",
    "    ax = axs[1, 2]\n",
    "    sns.kdeplot(tau_los_np[y_true == 0],  ax=ax, fill=True, color='#27ae60',\n",
    "                label=f'LOS circuit / LOS sample (mean={tau_los_np[y_true==0].mean():.2f})',   alpha=0.5)\n",
    "    sns.kdeplot(tau_los_np[y_true == 1],  ax=ax, fill=True, color='#82e0aa',\n",
    "                label=f'LOS circuit / NLOS sample (mean={tau_los_np[y_true==1].mean():.2f})', alpha=0.5)\n",
    "    sns.kdeplot(tau_nlos_np[y_true == 0], ax=ax, fill=True, color='#e74c3c',\n",
    "                label=f'NLOS circuit / LOS sample (mean={tau_nlos_np[y_true==0].mean():.2f})', alpha=0.5, ls='--')\n",
    "    sns.kdeplot(tau_nlos_np[y_true == 1], ax=ax, fill=True, color='#c0392b',\n",
    "                label=f'NLOS circuit / NLOS sample (mean={tau_nlos_np[y_true==1].mean():.2f})', alpha=0.5, ls='--')\n",
    "    ax.set_title(\"Emergent Tau: LOS vs NLOS circuits\")\n",
    "    ax.set_xlabel(\"Mean Tau\"); ax.legend(fontsize=7); ax.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.suptitle(\"DualCircuit_PI_HLNN — Stage 1 Diagnostics\",\n",
    "                 fontsize=16, fontweight='bold', y=1.01)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_diagnostics(best_model, best_data, best_history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==========================================\n",
    "# TAU TEMPORAL EVOLUTION — both circuits\n",
    "# ==========================================\n",
    "def plot_tau_temporal(model, val_data, n_samples=5):\n",
    "    X_va, y_va = val_data\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        _, _, _, tau_los_hist, tau_nlos_hist, _, _ = model(X_va, return_dynamics=True)\n",
    "\n",
    "    y_true = y_va.cpu().numpy().flatten()\n",
    "    tau_los_t  = tau_los_hist.cpu().numpy().mean(axis=2)   # (batch, seq_len)\n",
    "    tau_nlos_t = tau_nlos_hist.cpu().numpy().mean(axis=2)\n",
    "    x_input    = X_va.cpu().numpy().squeeze(-1)\n",
    "\n",
    "    los_idx  = np.where(y_true == 0)[0][:n_samples]\n",
    "    nlos_idx = np.where(y_true == 1)[0][:n_samples]\n",
    "\n",
    "    fig, axs = plt.subplots(2, 3, figsize=(22, 10))\n",
    "    plt.subplots_adjust(hspace=0.4, wspace=0.3)\n",
    "\n",
    "    titles = [\n",
    "        ('LOS signal',          los_idx,  '#2ecc71', x_input),\n",
    "        ('LOS τ — LOS circuit', los_idx,  '#27ae60', tau_los_t),\n",
    "        ('LOS τ — NLOS circuit',los_idx,  '#82e0aa', tau_nlos_t),\n",
    "        ('NLOS signal',         nlos_idx, '#e74c3c', x_input),\n",
    "        ('NLOS τ — LOS circuit',nlos_idx, '#c0392b', tau_los_t),\n",
    "        ('NLOS τ — NLOS circuit',nlos_idx,'#8e44ad', tau_nlos_t),\n",
    "    ]\n",
    "\n",
    "    for ax, (title, idx, color, data) in zip(axs.flat, titles):\n",
    "        for i in idx:\n",
    "            ax.plot(data[i], alpha=0.55, color=color, lw=1.3)\n",
    "        ax.set_title(title, fontsize=10, fontweight='bold')\n",
    "        ax.set_xlabel(\"Timestep\"); ax.grid(True, alpha=0.3)\n",
    "        if 'signal' in title:\n",
    "            ax.set_ylabel(\"Normalised CIR\")\n",
    "        else:\n",
    "            ax.set_ylabel(\"Mean Tau\"); ax.set_ylim([0.3, 7.0])\n",
    "\n",
    "    plt.suptitle(\"DualCircuit Tau Temporal Evolution — How Each Circuit Adapts\",\n",
    "                 fontsize=13, fontweight='bold', y=1.01)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_tau_temporal(best_model, best_data, n_samples=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 7: Test Set Evaluation & Save Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==========================================\n",
    "# TEST SET EVALUATION\n",
    "# ==========================================\n",
    "best_model.eval()\n",
    "X_te = torch.tensor(X_test).to(device)\n",
    "y_te = torch.tensor(y_test).unsqueeze(1).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_pred, _, _ = best_model(X_te)\n",
    "    test_acc = ((test_pred > 0.5).float() == y_te).float().mean().item()\n",
    "    test_pred_np = (test_pred.cpu().numpy().flatten() > 0.5).astype(float)\n",
    "    test_true_np = y_test.flatten()\n",
    "\n",
    "print(f\"Test Accuracy: {100*test_acc:.2f}%\")\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(test_true_np, test_pred_np, target_names=['LOS', 'NLOS']))\n",
    "\n",
    "cm = confusion_matrix(test_true_np, test_pred_np)\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 5))\n",
    "disp = ConfusionMatrixDisplay(cm, display_labels=['LOS', 'NLOS'])\n",
    "disp.plot(ax=ax, cmap='Blues', colorbar=False)\n",
    "ax.set_title(f\"Test Set Confusion Matrix — DualCircuit_PI_HLNN (Acc: {100*test_acc:.1f}%)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nModel summary:\")\n",
    "print(f\"  Architecture: DualCircuit_PI_HLNN (hidden={best_model.hidden_size} per circuit)\")\n",
    "print(f\"  Embedding dim: {best_model.hidden_size * 2} (2 × {best_model.hidden_size})\")\n",
    "print(f\"  Total params: {sum(p.numel() for p in best_model.parameters()):,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# SAVE ARTIFACTS\n",
    "# ==========================================\n",
    "# 1. Model weights\n",
    "torch.save(best_model.state_dict(), \"stage1_pi_hlnn_best.pt\")\n",
    "print(\"Saved: stage1_pi_hlnn_best.pt\")\n",
    "\n",
    "# 2. Configuration (for reproducibility)\n",
    "torch.save({\"config\": CONFIG}, \"stage1_config.pt\")\n",
    "print(\"Saved: stage1_config.pt\")\n",
    "\n",
    "print(f\"\\nArtifacts ready for Stage 2.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Stage 1 complete.\")\n",
    "print(\"Model artifact: stage1_pi_hlnn_best.pt\")\n",
    "print(\"Config artifact: stage1_config.pt\")\n",
    "print(\"\\nReady for Stage 2.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
