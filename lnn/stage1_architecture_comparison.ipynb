{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Stage 1 Architecture Comparison: LNN vs CNN vs LSTM vs Transformer\n",
    "## LOS/NLOS Classification from Raw CIR\n",
    "\n",
    "Compares **4 model architectures** on the combined 3600-sample dataset under **identical conditions**:\n",
    "\n",
    "| Model | Readout (canonical) | Inductive Bias |\n",
    "|---|---|---|\n",
    "| **LNN** (DualCircuit_PI_HLNN) | Attention-pooled ODE trajectory | Continuous-time dynamics, physics priors |\n",
    "| **CNN** (1D-CNN) | Global Average Pooling | Local patterns, translation invariance |\n",
    "| **LSTM** | Last hidden state | Sequential gating, selective memory |\n",
    "| **Transformer Encoder** | [CLS] token | All-pairs self-attention, positional encoding |\n",
    "\n",
    "**Controlled variables** (identical across all models):\n",
    "- Same inputs: 60-sample CIR window + FP_AMPL1/2/3 conditioning\n",
    "- Same classifier head: Linear(64\\u219232) \\u2192 SiLU \\u2192 Dropout \\u2192 Linear(32\\u21921) \\u2192 Sigmoid\n",
    "- Same training: BCE loss, AdamW, cosine LR with warmup, grad clipping\n",
    "- Same data splits, same seeds, same evaluation metrics\n",
    "\n",
    "Each architecture uses its own **canonical readout** \\u2014 the readout is integral to the architecture's inductive bias.\n",
    "\n",
    "**Evaluation**: Single 70/15/15 split + Stratified 5-Fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    \"pre_crop\": 10, \"post_crop\": 50, \"total_len\": 60,\n",
    "    \"search_start\": 740, \"search_end\": 890,\n",
    "    \"hidden_size\": 32, \"input_size\": 1, \"dropout\": 0.2, \"ode_unfolds\": 6,\n",
    "    \"batch_size\": 64, \"max_epochs\": 50, \"lr\": 1e-3,\n",
    "    \"weight_decay\": 1e-4, \"warmup_epochs\": 3, \"patience\": 40,\n",
    "    \"grad_clip\": 1.0, \"val_ratio\": 0.15, \"test_ratio\": 0.15, \"seed\": 42,\n",
    "}\n",
    "DATA_DIR = \"../dataset/channels/\"\n",
    "\n",
    "# ── Model registry ──────────────────────────────────────────────────────\n",
    "MODEL_CONFIGS = {\n",
    "    'LNN': {\n",
    "        'class': 'DualCircuit_PI_HLNN',\n",
    "        'kwargs': {\n",
    "            'input_size': CONFIG['input_size'],\n",
    "            'hidden_size': CONFIG['hidden_size'],\n",
    "            'dropout': CONFIG['dropout'],\n",
    "            'ode_unfolds': CONFIG['ode_unfolds'],\n",
    "        },\n",
    "    },\n",
    "    'CNN': {\n",
    "        'class': 'CNN1D_Classifier',\n",
    "        'kwargs': {'dropout': CONFIG['dropout']},\n",
    "    },\n",
    "    'LSTM': {\n",
    "        'class': 'LSTM_Classifier',\n",
    "        'kwargs': {'lstm_hidden': 48, 'dropout': CONFIG['dropout']},\n",
    "    },\n",
    "    'Transformer': {\n",
    "        'class': 'TransformerEncoder_Classifier',\n",
    "        'kwargs': {\n",
    "            'd_model': 32, 'nhead': 4, 'num_layers': 2,\n",
    "            'dim_feedforward': 64, 'dropout': CONFIG['dropout'],\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "MODEL_COLORS = {\n",
    "    'LNN': '#e74c3c',         # Red\n",
    "    'CNN': '#3498db',         # Blue\n",
    "    'LSTM': '#2ecc71',        # Green\n",
    "    'Transformer': '#9b59b6', # Purple\n",
    "}\n",
    "MODEL_ORDER = ['LNN', 'CNN', 'LSTM', 'Transformer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy, math, contextlib, io\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, ConfusionMatrixDisplay,\n",
    "    classification_report, roc_curve, auc\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "torch.manual_seed(CONFIG[\"seed\"])\n",
    "np.random.seed(CONFIG[\"seed\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-data",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 1: Data Loading & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data-loading",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_roi_alignment(sig,\n",
    "                      search_start=CONFIG['search_start'],\n",
    "                      search_end=CONFIG['search_end']):\n",
    "    region = sig[search_start:search_end]\n",
    "    if len(region) == 0:\n",
    "        return np.argmax(sig)\n",
    "    peak_idx = search_start + np.argmax(region)\n",
    "    peak_val = sig[peak_idx]\n",
    "    noise_section = sig[:search_start]\n",
    "    if len(noise_section) > 10:\n",
    "        threshold = max(np.mean(noise_section) + 3*np.std(noise_section), 0.05*peak_val)\n",
    "    else:\n",
    "        threshold = 0.05 * peak_val\n",
    "    leading_edge = peak_idx\n",
    "    for i in range(peak_idx, max(search_start - 20, 0), -1):\n",
    "        if sig[i] < threshold:\n",
    "            leading_edge = i + 1\n",
    "            break\n",
    "    return leading_edge\n",
    "\n",
    "\n",
    "def _process_rows(df):\n",
    "    \"\"\"Shared CIR + FP_AMPL preprocessing.\"\"\"\n",
    "    PRE = CONFIG['pre_crop']; TOTAL = CONFIG['total_len']\n",
    "    cir_cols = sorted([c for c in df.columns if c.startswith('CIR')],\n",
    "                      key=lambda x: int(x.replace('CIR', '')))\n",
    "    seqs, labels, fp_features = [], [], []\n",
    "    for _, row in df.iterrows():\n",
    "        sig = pd.to_numeric(row[cir_cols], errors='coerce').fillna(0).astype(float).values\n",
    "        rxpacc = float(row.get('RXPACC', 128.0))\n",
    "        if rxpacc > 0:\n",
    "            sig = sig / rxpacc\n",
    "        f1 = float(row.get('FP_AMPL1', 0)) / max(rxpacc, 1) / 64.0\n",
    "        f2 = float(row.get('FP_AMPL2', 0)) / max(rxpacc, 1) / 64.0\n",
    "        f3 = float(row.get('FP_AMPL3', 0)) / max(rxpacc, 1) / 64.0\n",
    "        fp_features.append([f1, f2, f3])\n",
    "        le = get_roi_alignment(sig)\n",
    "        start = max(0, le - PRE); end = start + TOTAL\n",
    "        if end > len(sig): end = len(sig); start = max(0, end - TOTAL)\n",
    "        crop = sig[start:end]\n",
    "        if len(crop) < TOTAL:\n",
    "            crop = np.pad(crop, (0, TOTAL - len(crop)), mode='constant')\n",
    "        lo, hi = crop.min(), crop.max()\n",
    "        crop = (crop - lo) / (hi - lo) if hi > lo else np.zeros(TOTAL)\n",
    "        seqs.append(crop); labels.append(float(row['Label']))\n",
    "    X = np.array(seqs).reshape(-1, TOTAL, 1).astype(np.float32)\n",
    "    y = np.array(labels).astype(np.float32)\n",
    "    F = np.array(fp_features).astype(np.float32)\n",
    "    return X, y, F\n",
    "\n",
    "\n",
    "def load_cir_dataset(filepath):\n",
    "    print(f'Loading: {filepath}')\n",
    "    df = pd.read_csv(filepath)\n",
    "    print(f'  Samples: {len(df)}')\n",
    "    X, y, F = _process_rows(df)\n",
    "    print(f'  Output: X={X.shape}, y={y.shape}, F={F.shape} | LOS={int((y==0).sum())}, NLOS={int((y==1).sum())}')\n",
    "    return X, y, F\n",
    "\n",
    "print(\"Data loading functions ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-models",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 2: Model Architectures\n",
    "\n",
    "Four architectures with their canonical readouts:\n",
    "- **LNN**: Dual ODE circuits \\u2192 attention pooling \\u2192 64-dim\n",
    "- **CNN**: Conv1d stack \\u2192 Global Average Pooling \\u2192 64-dim\n",
    "- **LSTM**: Recurrent processing \\u2192 last hidden state \\u2192 64-dim\n",
    "- **Transformer**: Self-attention \\u2192 [CLS] token output \\u2192 64-dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model-lnn",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# LNN: DualCircuit_PI_HLNN (ODE-based)\n",
    "# ==========================================\n",
    "class PILiquidCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, ode_unfolds=6):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.ode_unfolds = ode_unfolds\n",
    "        self.gleak = nn.Parameter(torch.empty(hidden_size).uniform_(0.001, 1.0))\n",
    "        self.vleak = nn.Parameter(torch.empty(hidden_size).uniform_(-0.2, 0.2))\n",
    "        self.cm    = nn.Parameter(torch.empty(hidden_size).uniform_(0.4, 0.6))\n",
    "        self.w     = nn.Parameter(torch.empty(hidden_size, hidden_size).uniform_(0.001, 1.0))\n",
    "        self.erev  = nn.Parameter(torch.empty(hidden_size, hidden_size).uniform_(-0.2, 0.2))\n",
    "        self.mu    = nn.Parameter(torch.empty(hidden_size, hidden_size).uniform_(0.3, 0.8))\n",
    "        self.sigma = nn.Parameter(torch.empty(hidden_size, hidden_size).uniform_(3, 8))\n",
    "        self.sensory_w     = nn.Parameter(torch.empty(input_size, hidden_size).uniform_(0.001, 1.0))\n",
    "        self.sensory_mu    = nn.Parameter(torch.empty(input_size, hidden_size).uniform_(0.3, 0.8))\n",
    "        self.sensory_sigma = nn.Parameter(torch.empty(input_size, hidden_size).uniform_(3, 8))\n",
    "\n",
    "    def forward(self, x_t, h_prev, dt=1.0):\n",
    "        gleak     = F.softplus(self.gleak)\n",
    "        cm        = F.softplus(self.cm)\n",
    "        w         = F.softplus(self.w)\n",
    "        sensory_w = F.softplus(self.sensory_w)\n",
    "        sensory_gate    = torch.sigmoid(self.sensory_sigma * (x_t.unsqueeze(-1) - self.sensory_mu))\n",
    "        sensory_current = (sensory_w * sensory_gate * x_t.unsqueeze(-1)).sum(dim=1)\n",
    "        cm_t = cm / (dt / self.ode_unfolds)\n",
    "        v = h_prev\n",
    "        for _ in range(self.ode_unfolds):\n",
    "            rg = torch.sigmoid(self.sigma.unsqueeze(0) * (v.unsqueeze(2) - self.mu.unsqueeze(0)))\n",
    "            wg = w.unsqueeze(0) * rg\n",
    "            w_num = (wg * self.erev.unsqueeze(0)).sum(dim=1)\n",
    "            w_den = wg.sum(dim=1)\n",
    "            v = (cm_t*v + gleak*self.vleak + w_num + sensory_current) / (cm_t + gleak + w_den + 1e-8)\n",
    "            v = torch.clamp(v, -1.0, 1.0)\n",
    "        return v, cm / (gleak + w_den + 1e-8)\n",
    "\n",
    "\n",
    "class DualCircuit_PI_HLNN(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_size=32, dropout=0.4, ode_unfolds=6):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.cell_los  = PILiquidCell(input_size, hidden_size, ode_unfolds)\n",
    "        self.cell_nlos = PILiquidCell(input_size, hidden_size, ode_unfolds)\n",
    "        self.fp_to_los_init  = nn.Linear(3, hidden_size)\n",
    "        self.fp_to_nlos_init = nn.Linear(3, hidden_size)\n",
    "        self.P_nlos2los = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.P_los2nlos = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.gate_los   = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.gate_nlos  = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.los_attn   = nn.Linear(hidden_size, 1)\n",
    "        self.nlos_attn  = nn.Linear(hidden_size, 1)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size*2, hidden_size), nn.SiLU(),\n",
    "            nn.Dropout(dropout), nn.Linear(hidden_size, 1), nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def _run_circuits(self, x_seq, fp_features=None):\n",
    "        batch_size, seq_len, _ = x_seq.size()\n",
    "        if fp_features is not None:\n",
    "            h_los  = 0.1 * torch.tanh(self.fp_to_los_init(fp_features))\n",
    "            h_nlos = 0.1 * torch.tanh(self.fp_to_nlos_init(fp_features))\n",
    "        else:\n",
    "            h_los  = torch.zeros(batch_size, self.hidden_size, device=x_seq.device)\n",
    "            h_nlos = torch.zeros(batch_size, self.hidden_size, device=x_seq.device)\n",
    "        los_states, nlos_states = [], []\n",
    "        tau_los_sum  = torch.zeros_like(h_los)\n",
    "        tau_nlos_sum = torch.zeros_like(h_nlos)\n",
    "        tau_los_hist_list, tau_nlos_hist_list = [], []\n",
    "        for t in range(seq_len):\n",
    "            x_t = x_seq[:, t, :]\n",
    "            p_n2l = self.P_nlos2los(h_nlos); p_l2n = self.P_los2nlos(h_los)\n",
    "            gl = torch.sigmoid(self.gate_los( torch.cat([h_los,  p_n2l], dim=1)))\n",
    "            gn = torch.sigmoid(self.gate_nlos(torch.cat([h_nlos, p_l2n], dim=1)))\n",
    "            h_los,  tau_los  = self.cell_los( x_t, h_los  + gl * p_n2l)\n",
    "            h_nlos, tau_nlos = self.cell_nlos(x_t, h_nlos + gn * p_l2n)\n",
    "            los_states.append(h_los.unsqueeze(1))\n",
    "            nlos_states.append(h_nlos.unsqueeze(1))\n",
    "            tau_los_sum  += tau_los\n",
    "            tau_nlos_sum += tau_nlos\n",
    "            tau_los_hist_list.append(tau_los.unsqueeze(1))\n",
    "            tau_nlos_hist_list.append(tau_nlos.unsqueeze(1))\n",
    "        los_all  = torch.cat(los_states, dim=1)\n",
    "        nlos_all = torch.cat(nlos_states, dim=1)\n",
    "        tau_los_mean  = tau_los_sum  / seq_len\n",
    "        tau_nlos_mean = tau_nlos_sum / seq_len\n",
    "        tau_los_hist  = torch.cat(tau_los_hist_list, dim=1)\n",
    "        tau_nlos_hist = torch.cat(tau_nlos_hist_list, dim=1)\n",
    "        return los_all, nlos_all, tau_los_hist, tau_nlos_hist, tau_los_mean, tau_nlos_mean\n",
    "\n",
    "    def _pool_and_fuse(self, la, na):\n",
    "        lw = F.softmax(self.los_attn(la).squeeze(-1),  dim=1).unsqueeze(-1)\n",
    "        nw = F.softmax(self.nlos_attn(na).squeeze(-1), dim=1).unsqueeze(-1)\n",
    "        return torch.cat([(la*lw).sum(1), (na*nw).sum(1)], dim=1)\n",
    "\n",
    "    def forward(self, x_seq, fp_features=None, return_dynamics=False):\n",
    "        los_all, nlos_all, tau_los_hist, tau_nlos_hist, tau_los_mean, tau_nlos_mean = \\\n",
    "            self._run_circuits(x_seq, fp_features=fp_features)\n",
    "        pred = self.classifier(self._pool_and_fuse(los_all, nlos_all))\n",
    "        if return_dynamics:\n",
    "            return pred, los_all, nlos_all, tau_los_hist, tau_nlos_hist, tau_los_mean, tau_nlos_mean\n",
    "        return pred, tau_los_mean, tau_nlos_mean\n",
    "\n",
    "    def embed(self, x_seq, fp_features=None):\n",
    "        los_all, nlos_all, _, _, _, _ = self._run_circuits(x_seq, fp_features=fp_features)\n",
    "        return self._pool_and_fuse(los_all, nlos_all)\n",
    "\n",
    "print('LNN: DualCircuit_PI_HLNN defined.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model-others",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# BASE CLASSIFIER (shared FP conditioning + classifier head)\n",
    "# ==========================================\n",
    "class BaseClassifier(nn.Module):\n",
    "    \"\"\"Base class for non-LNN classifiers.\n",
    "\n",
    "    Subclasses override _encode(x_seq) -> (batch, 64).\n",
    "    forward() returns (pred, None, None) to match DualCircuit_PI_HLNN's 3-tuple.\n",
    "    \"\"\"\n",
    "    def __init__(self, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.fp_cond = nn.Sequential(nn.Linear(67, 64), nn.ReLU())\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(64, 32), nn.SiLU(), nn.Dropout(dropout),\n",
    "            nn.Linear(32, 1), nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def _encode(self, x_seq):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _condition_fp(self, cir_emb, fp_features):\n",
    "        if fp_features is not None:\n",
    "            return self.fp_cond(torch.cat([cir_emb, fp_features], dim=1))\n",
    "        return cir_emb\n",
    "\n",
    "    def embed(self, x_seq, fp_features=None):\n",
    "        return self._condition_fp(self._encode(x_seq), fp_features)\n",
    "\n",
    "    def forward(self, x_seq, fp_features=None, return_dynamics=False):\n",
    "        emb = self.embed(x_seq, fp_features=fp_features)\n",
    "        pred = self.classifier(emb)\n",
    "        return pred, None, None\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# CNN1D CLASSIFIER — Global Average Pooling readout\n",
    "# ==========================================\n",
    "class CNN1D_Classifier(BaseClassifier):\n",
    "    \"\"\"1D-CNN: Conv1d stack -> GlobalAvgPool -> 64-dim -> classifier.\"\"\"\n",
    "    def __init__(self, dropout=0.2):\n",
    "        super().__init__(dropout=dropout)\n",
    "        self.backbone = nn.Sequential(\n",
    "            nn.Conv1d(1, 16, kernel_size=7, padding=3),\n",
    "            nn.BatchNorm1d(16), nn.ReLU(),\n",
    "            nn.Conv1d(16, 32, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm1d(32), nn.ReLU(),\n",
    "            nn.Conv1d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(64), nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool1d(1),\n",
    "        )\n",
    "\n",
    "    def _encode(self, x_seq):\n",
    "        x = x_seq.permute(0, 2, 1)       # (B, 1, 60)\n",
    "        x = self.backbone(x)              # (B, 64, 1)\n",
    "        return x.squeeze(-1)              # (B, 64)\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# LSTM CLASSIFIER — Last hidden state readout\n",
    "# ==========================================\n",
    "class LSTM_Classifier(BaseClassifier):\n",
    "    \"\"\"LSTM: process 60-step CIR -> last hidden state h_T -> project to 64-dim.\"\"\"\n",
    "    def __init__(self, lstm_hidden=48, dropout=0.2):\n",
    "        super().__init__(dropout=dropout)\n",
    "        self.lstm = nn.LSTM(input_size=1, hidden_size=lstm_hidden,\n",
    "                            num_layers=1, batch_first=True)\n",
    "        self.proj = nn.Linear(lstm_hidden, 64)\n",
    "\n",
    "    def _encode(self, x_seq):\n",
    "        _, (h_n, _) = self.lstm(x_seq)    # h_n: (1, B, 48)\n",
    "        h_T = h_n.squeeze(0)              # (B, 48)\n",
    "        return self.proj(h_T)             # (B, 64)\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# TRANSFORMER ENCODER CLASSIFIER — [CLS] token readout\n",
    "# ==========================================\n",
    "class SinusoidalPE(nn.Module):\n",
    "    \"\"\"Fixed sinusoidal positional encoding.\"\"\"\n",
    "    def __init__(self, d_model, max_len=128):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float()\n",
    "                             * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1), :]\n",
    "\n",
    "\n",
    "class TransformerEncoder_Classifier(BaseClassifier):\n",
    "    \"\"\"Transformer Encoder: Linear proj -> SinusoidalPE -> [CLS] + encoder layers -> [CLS] output.\"\"\"\n",
    "    def __init__(self, d_model=32, nhead=4, num_layers=2, dim_feedforward=64, dropout=0.2):\n",
    "        super().__init__(dropout=dropout)\n",
    "        self.input_proj = nn.Linear(1, d_model)\n",
    "        self.pos_enc = SinusoidalPE(d_model, max_len=128)\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, d_model))\n",
    "        nn.init.normal_(self.cls_token, std=0.02)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward,\n",
    "            dropout=0.1, batch_first=True, activation='gelu',\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.out_proj = nn.Linear(d_model, 64)\n",
    "\n",
    "    def _encode(self, x_seq):\n",
    "        B = x_seq.size(0)\n",
    "        x = self.input_proj(x_seq)                    # (B, 60, d_model)\n",
    "        cls = self.cls_token.expand(B, -1, -1)         # (B, 1, d_model)\n",
    "        x = torch.cat([cls, x], dim=1)                 # (B, 61, d_model)\n",
    "        x = self.pos_enc(x)                             # (B, 61, d_model)\n",
    "        x = self.encoder(x)                              # (B, 61, d_model)\n",
    "        return self.out_proj(x[:, 0, :])                 # (B, 64) — [CLS]\n",
    "\n",
    "\n",
    "print('CNN, LSTM, Transformer models defined.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model-factory",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Model factory + verification ────────────────────────────────────────\n",
    "_CLS_MAP = {\n",
    "    'DualCircuit_PI_HLNN': DualCircuit_PI_HLNN,\n",
    "    'CNN1D_Classifier': CNN1D_Classifier,\n",
    "    'LSTM_Classifier': LSTM_Classifier,\n",
    "    'TransformerEncoder_Classifier': TransformerEncoder_Classifier,\n",
    "}\n",
    "\n",
    "def build_model(model_name, config=CONFIG):\n",
    "    mc = MODEL_CONFIGS[model_name]\n",
    "    return _CLS_MAP[mc['class']](**mc['kwargs']).to(device)\n",
    "\n",
    "\n",
    "# Verify all models\n",
    "x_test = torch.randn(4, 60, 1)\n",
    "f_test = torch.randn(4, 3)\n",
    "\n",
    "print(f\"{'Model':>15} | {'Params':>8} | {'pred':>12} | {'embed':>12}\")\n",
    "print(f\"{'-'*55}\")\n",
    "for name in MODEL_ORDER:\n",
    "    m = build_model(name)\n",
    "    pred, _, _ = m(x_test.to(device), fp_features=f_test.to(device))\n",
    "    emb = m.embed(x_test.to(device), fp_features=f_test.to(device))\n",
    "    n_params = sum(p.numel() for p in m.parameters())\n",
    "    assert pred.shape == (4, 1), f\"{name}: pred shape {pred.shape}\"\n",
    "    assert emb.shape == (4, 64), f\"{name}: embed shape {emb.shape}\"\n",
    "    print(f'{name:>15} | {n_params:>8,} | {str(tuple(pred.shape)):>12} | {str(tuple(emb.shape)):>12}')\n",
    "    del m\n",
    "\n",
    "print('\\nAll models verified: forward returns 3-tuple, embed returns (batch, 64).')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-training",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 3: Training & Evaluation\n",
    "Model-agnostic training loop: accepts `model_name` to select architecture via `build_model()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-eval",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X_train, y_train, X_val, y_val, F_train=None, F_val=None,\n",
    "                config=CONFIG, model_name='LNN', verbose=True, seed=None):\n",
    "    _seed = seed if seed is not None else config['seed']\n",
    "    torch.manual_seed(_seed)\n",
    "    np.random.seed(_seed)\n",
    "    fp_enabled = F_train is not None\n",
    "\n",
    "    X_tr = torch.tensor(X_train).to(device)\n",
    "    y_tr = torch.tensor(y_train).unsqueeze(1).to(device)\n",
    "    X_va = torch.tensor(X_val).to(device)\n",
    "    y_va = torch.tensor(y_val).unsqueeze(1).to(device)\n",
    "\n",
    "    if fp_enabled:\n",
    "        F_tr = torch.tensor(F_train).to(device)\n",
    "        F_va = torch.tensor(F_val).to(device)\n",
    "        loader = DataLoader(TensorDataset(X_tr, y_tr, F_tr),\n",
    "                            batch_size=config['batch_size'], shuffle=True)\n",
    "    else:\n",
    "        loader = DataLoader(TensorDataset(X_tr, y_tr),\n",
    "                            batch_size=config['batch_size'], shuffle=True)\n",
    "\n",
    "    model = build_model(model_name, config)\n",
    "\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=config['lr'],\n",
    "                            weight_decay=config['weight_decay'])\n",
    "    T = config['max_epochs']; W = config['warmup_epochs']\n",
    "    def lr_lambda(e):\n",
    "        if e < W: return (e+1)/W\n",
    "        return max(0.01, 0.5*(1+math.cos(math.pi*(e-W)/max(1,T-W))))\n",
    "    scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "    history = {'train_loss':[], 'val_loss':[], 'train_acc':[], 'val_acc':[]}\n",
    "    best_val_acc = 0; best_state = None; patience_counter = 0\n",
    "\n",
    "    for epoch in range(T):\n",
    "        model.train(); tl, tc, tt = 0, 0, 0\n",
    "        for batch in loader:\n",
    "            if fp_enabled:\n",
    "                bx, by, bf = batch\n",
    "            else:\n",
    "                bx, by = batch; bf = None\n",
    "            optimizer.zero_grad()\n",
    "            pred, _, _ = model(bx, fp_features=bf)\n",
    "            loss = criterion(pred, by); loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), config['grad_clip'])\n",
    "            optimizer.step()\n",
    "            tl += loss.item()*len(bx); tc += ((pred>0.5).float()==by).sum().item(); tt += len(bx)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_fp = F_va if fp_enabled else None\n",
    "            vp, _, _ = model(X_va, fp_features=val_fp)\n",
    "            vl = criterion(vp, y_va).item()\n",
    "            va = ((vp>0.5).float()==y_va).float().mean().item()\n",
    "        history['train_loss'].append(tl/tt); history['val_loss'].append(vl)\n",
    "        history['train_acc'].append(tc/tt);  history['val_acc'].append(va)\n",
    "        scheduler.step()\n",
    "\n",
    "        if va > best_val_acc:\n",
    "            best_val_acc = va; best_state = copy.deepcopy(model.state_dict()); patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if verbose and (epoch%10==0 or epoch==T-1):\n",
    "            print(f'  Ep {epoch:>3} | Loss: {tl/tt:.4f} | Val Acc: {100*va:.2f}% | Best: {100*best_val_acc:.2f}%')\n",
    "        if patience_counter >= config.get('patience', T):\n",
    "            if verbose: print(f'  Early stopping at epoch {epoch}')\n",
    "            break\n",
    "\n",
    "    model.load_state_dict(best_state)\n",
    "    if verbose: print(f'  Best Val Acc: {100*best_val_acc:.2f}%')\n",
    "    return model, history\n",
    "\n",
    "\n",
    "def evaluate(model, X, y_true, F=None):\n",
    "    \"\"\"Returns metrics dict for a given model and data.\"\"\"\n",
    "    model.eval()\n",
    "    X_t = torch.tensor(X).to(device)\n",
    "    F_t = torch.tensor(F).to(device) if F is not None else None\n",
    "    with torch.no_grad():\n",
    "        pred, _, _ = model(X_t, fp_features=F_t)\n",
    "    y_prob = pred.cpu().numpy().flatten()\n",
    "    y_pred = (y_prob > 0.5).astype(float)\n",
    "    y_true = y_true.flatten()\n",
    "    rep  = classification_report(y_true, y_pred, target_names=['LOS','NLOS'],\n",
    "                                 output_dict=True, zero_division=0)\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
    "    return {\n",
    "        'acc':      rep['accuracy'],\n",
    "        'f1_macro': rep['macro avg']['f1-score'],\n",
    "        'f1_los':   rep['LOS']['f1-score'],\n",
    "        'f1_nlos':  rep['NLOS']['f1-score'],\n",
    "        'auc':      auc(fpr, tpr),\n",
    "        'fpr': fpr, 'tpr': tpr,\n",
    "        'cm':  confusion_matrix(y_true, y_pred),\n",
    "    }\n",
    "\n",
    "print('train_model() and evaluate() ready.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-single-split",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 4: Single 70/15/15 Split — 4 Architectures\n",
    "Quick baseline on the combined 3600-sample dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "single-split-run",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(csv_name, label, model_name='LNN', config=CONFIG, seed=42):\n",
    "    print(f\"\\n{'='*60}\\nExperiment: {label} | Model: {model_name}\\n{'='*60}\")\n",
    "    X_all, y_all, F_all = load_cir_dataset(DATA_DIR + csv_name)\n",
    "    X_tr, X_tmp, y_tr, y_tmp, F_tr, F_tmp = train_test_split(\n",
    "        X_all, y_all, F_all, test_size=config['val_ratio']+config['test_ratio'],\n",
    "        stratify=y_all, random_state=seed)\n",
    "    X_va, X_te, y_va, y_te, F_va, F_te = train_test_split(\n",
    "        X_tmp, y_tmp, F_tmp,\n",
    "        test_size=config['test_ratio']/(config['val_ratio']+config['test_ratio']),\n",
    "        stratify=y_tmp, random_state=seed)\n",
    "    print(f'  Train={len(X_tr)}, Val={len(X_va)}, Test={len(X_te)}')\n",
    "    model, history = train_model(X_tr, y_tr, X_va, y_va, F_train=F_tr, F_val=F_va,\n",
    "                                  config=config, model_name=model_name)\n",
    "    m = evaluate(model, X_te, y_te, F=F_te)\n",
    "    m['label'] = label; m['history'] = history; m['model_name'] = model_name\n",
    "    n_params = sum(p.numel() for p in model.parameters())\n",
    "    m['params'] = n_params\n",
    "    print(f\"\\n  Test Acc={m['acc']:.4f} | Macro F1={m['f1_macro']:.4f} | AUC={m['auc']:.4f} | Params={n_params:,}\")\n",
    "    return m\n",
    "\n",
    "\n",
    "# Run all 4 models on the combined dataset\n",
    "results_split = {}\n",
    "for model_name in MODEL_ORDER:\n",
    "    results_split[model_name] = run_experiment(\n",
    "        'combined_uwb_dataset.csv', 'Combined (3600)', model_name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "single-split-plots",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Single-split plots: 4 architectures ─────────────────────────────────\n",
    "_colors = [MODEL_COLORS[mn] for mn in MODEL_ORDER]\n",
    "width = 0.18\n",
    "_res = [results_split[mn] for mn in MODEL_ORDER]\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(24, 6))\n",
    "\n",
    "# Col 0: Key metrics bar chart\n",
    "ax = axs[0]\n",
    "mk = ['acc', 'f1_macro', 'auc']; mn_labels = ['Accuracy', 'Macro F1', 'AUC']\n",
    "for i, (r, c, mn) in enumerate(zip(_res, _colors, MODEL_ORDER)):\n",
    "    vals = [r[k] for k in mk]\n",
    "    bars = ax.bar(np.arange(3) + i*width, vals, width, label=mn, color=c, alpha=0.85)\n",
    "    for bar, v in zip(bars, vals):\n",
    "        ax.text(bar.get_x()+bar.get_width()/2, bar.get_height()+0.005,\n",
    "                f'{v:.3f}', ha='center', va='bottom', fontsize=6.5, fontweight='bold')\n",
    "ax.set_xticks(np.arange(3) + width*1.5); ax.set_xticklabels(mn_labels)\n",
    "ax.set_ylim([0.3, 1.15]); ax.set_title('Metrics', fontweight='bold')\n",
    "ax.legend(fontsize=7); ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Col 1: ROC curves\n",
    "ax = axs[1]\n",
    "for r, c, mn in zip(_res, _colors, MODEL_ORDER):\n",
    "    ax.plot(r['fpr'], r['tpr'], color=c, lw=2,\n",
    "            label=f\"{mn} (AUC={r['auc']:.3f})\")\n",
    "ax.plot([0,1],[0,1],'k--',lw=1,alpha=0.5)\n",
    "ax.set_title('ROC Curves', fontweight='bold')\n",
    "ax.set_xlabel('FPR'); ax.set_ylabel('TPR')\n",
    "ax.legend(fontsize=7); ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Col 2: Val accuracy curves\n",
    "ax = axs[2]\n",
    "for r, c, mn in zip(_res, _colors, MODEL_ORDER):\n",
    "    ax.plot(r['history']['val_acc'], color=c, lw=2, label=mn)\n",
    "ax.set_title('Val Accuracy', fontweight='bold')\n",
    "ax.set_xlabel('Epoch'); ax.set_ylabel('Val Accuracy')\n",
    "ax.set_ylim([0.3, 1.05]); ax.legend(fontsize=7); ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Stage 1: 4-Architecture Comparison — Single Split (3600 samples)',\n",
    "             fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# Summary table\n",
    "print(f\"\\n{'='*75}\")\n",
    "print(f\"{'Model':<15} {'Params':>8} {'Acc':>8} {'F1':>8} {'F1-LOS':>8} {'F1-NLOS':>8} {'AUC':>8}\")\n",
    "print(f\"{'-'*75}\")\n",
    "for mn in MODEL_ORDER:\n",
    "    r = results_split[mn]\n",
    "    print(f\"{mn:<15} {r['params']:>8,} {r['acc']:>8.4f} {r['f1_macro']:>8.4f} \"\n",
    "          f\"{r['f1_los']:>8.4f} {r['f1_nlos']:>8.4f} {r['auc']:>8.4f}\")\n",
    "print(f\"{'='*75}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-kfold",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 5: Stratified 5-Fold Cross-Validation — 4 Architectures\n",
    "More reliable estimate: averages over 5 different train/test splits per model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kfold-run",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_kfold(csv_name, label, model_name='LNN', n_splits=5, config=CONFIG, seed=42):\n",
    "    print(f\"\\n{'='*60}\\n5-Fold CV: {label} | Model: {model_name}\\n{'='*60}\")\n",
    "    X_all, y_all, F_all = load_cir_dataset(DATA_DIR + csv_name)\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    fold_metrics = []\n",
    "    for fold, (tv_idx, te_idx) in enumerate(skf.split(X_all, y_all)):\n",
    "        X_tv, X_te = X_all[tv_idx], X_all[te_idx]\n",
    "        y_tv, y_te = y_all[tv_idx], y_all[te_idx]\n",
    "        F_tv, F_te = F_all[tv_idx], F_all[te_idx]\n",
    "        X_tr, X_va, y_tr, y_va, F_tr, F_va = train_test_split(\n",
    "            X_tv, y_tv, F_tv, test_size=0.15, stratify=y_tv, random_state=seed)\n",
    "        fold_seed = seed + fold\n",
    "        with contextlib.redirect_stdout(io.StringIO()):\n",
    "            model, _ = train_model(X_tr, y_tr, X_va, y_va, F_train=F_tr, F_val=F_va,\n",
    "                                   config=config, model_name=model_name,\n",
    "                                   verbose=False, seed=fold_seed)\n",
    "        fm = evaluate(model, X_te, y_te, F=F_te)\n",
    "        fold_metrics.append(fm)\n",
    "        collapsed = \" [COLLAPSED]\" if fm['acc'] <= 0.51 else \"\"\n",
    "        print(f\"  Fold {fold+1}/{n_splits} | Acc={fm['acc']:.4f} | F1={fm['f1_macro']:.4f} | AUC={fm['auc']:.4f}{collapsed}\")\n",
    "    summary = {'label': label, 'model_name': model_name}\n",
    "    print(f\"\\n  {'---'*15}\")\n",
    "    print(f\"  {'Metric':<12} {'Mean':>8} {'Std':>8}\")\n",
    "    print(f\"  {'---'*15}\")\n",
    "    for key in ['acc','f1_macro','f1_los','f1_nlos','auc']:\n",
    "        vals = np.array([m[key] for m in fold_metrics])\n",
    "        summary[key] = {'mean': vals.mean(), 'std': vals.std(), 'all': vals.tolist()}\n",
    "        print(f\"  {key:<12} {vals.mean():>8.4f} {vals.std():>8.4f}\")\n",
    "    print(f\"  {'---'*15}\")\n",
    "    return summary\n",
    "\n",
    "\n",
    "# Run all 4 models\n",
    "kfold_results = {}\n",
    "for model_name in MODEL_ORDER:\n",
    "    kfold_results[model_name] = run_kfold(\n",
    "        'combined_uwb_dataset.csv', 'Combined (3600)', model_name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kfold-plots",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── K-Fold comparison plots: 4 architectures ────────────────────────────\n",
    "_mk = ['acc', 'f1_macro', 'f1_los', 'f1_nlos', 'auc']\n",
    "_mn = ['Accuracy', 'Macro F1', 'F1 LOS', 'F1 NLOS', 'AUC']\n",
    "width = 0.18\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(22, 7))\n",
    "\n",
    "# Bar chart with error bars\n",
    "ax = axs[0]\n",
    "x = np.arange(len(_mk))\n",
    "for i, mn in enumerate(MODEL_ORDER):\n",
    "    kf = kfold_results[mn]\n",
    "    means = [kf[m]['mean'] for m in _mk]\n",
    "    stds  = [kf[m]['std']  for m in _mk]\n",
    "    bars = ax.bar(x + i*width, means, width, yerr=stds,\n",
    "                  label=mn, color=MODEL_COLORS[mn], alpha=0.85,\n",
    "                  capsize=3, error_kw={'elinewidth': 1.2})\n",
    "    for bar, m_val, s_val in zip(bars, means, stds):\n",
    "        ax.text(bar.get_x()+bar.get_width()/2, bar.get_height()+s_val+0.008,\n",
    "                f'{m_val:.3f}', ha='center', va='bottom', fontsize=6, fontweight='bold')\n",
    "ax.set_xticks(x + width*1.5); ax.set_xticklabels(_mn, rotation=10)\n",
    "ax.set_ylim([0.3, 1.2])\n",
    "ax.set_title('5-Fold CV: Mean +/- Std', fontweight='bold')\n",
    "ax.legend(fontsize=7); ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Box plots\n",
    "ax = axs[1]\n",
    "all_data, all_colors, positions = [], [], []\n",
    "gap = len(MODEL_ORDER) + 1\n",
    "for mi, metric in enumerate(_mk):\n",
    "    for ki, mn in enumerate(MODEL_ORDER):\n",
    "        kf = kfold_results[mn]\n",
    "        positions.append(mi * gap + ki)\n",
    "        all_data.append(kf[metric]['all'])\n",
    "        all_colors.append(MODEL_COLORS[mn])\n",
    "bp = ax.boxplot(all_data, positions=positions, widths=0.65, patch_artist=True,\n",
    "                medianprops={'color': 'black', 'linewidth': 2})\n",
    "for patch, c in zip(bp['boxes'], all_colors):\n",
    "    patch.set_facecolor(c); patch.set_alpha(0.7)\n",
    "ax.set_xticks([mi * gap + 1.5 for mi in range(len(_mk))])\n",
    "ax.set_xticklabels(_mn, rotation=10)\n",
    "ax.set_title('Score Distribution per Fold', fontweight='bold')\n",
    "ax.set_ylabel('Score'); ax.grid(True, alpha=0.3, axis='y')\n",
    "legend_patches = [Patch(facecolor=MODEL_COLORS[mn], alpha=0.7, label=mn) for mn in MODEL_ORDER]\n",
    "ax.legend(handles=legend_patches, fontsize=7)\n",
    "\n",
    "plt.suptitle('Stage 1: 4-Architecture Comparison — Stratified 5-Fold CV (3600 samples)',\n",
    "             fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# Summary table\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"{'Model':<15} {'Acc':>14} {'Macro F1':>14} {'F1-LOS':>14} {'F1-NLOS':>14} {'AUC':>14}\")\n",
    "print(f\"{'-'*80}\")\n",
    "for mn in MODEL_ORDER:\n",
    "    kf = kfold_results[mn]\n",
    "    print(f\"{mn:<15}\", end=\"\")\n",
    "    for m in _mk:\n",
    "        print(f\" {kf[m]['mean']:>6.4f}+/-{kf[m]['std']:.4f}\", end=\"\")\n",
    "    print()\n",
    "print(f\"{'='*80}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
