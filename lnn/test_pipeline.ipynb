{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "tp-title",
   "metadata": {},
   "source": [
    "# End-to-End Multi-Model Pipeline — Train / Validate / Test\n",
    "## 70 / 15 / 15 Split — Final Evaluation on Unseen Test Set\n",
    "\n",
    "**Pipeline**:  \n",
    "```\n",
    "Raw CIR → Stage 1 (PI-HLNN: LOS/NLOS?) → Stage 2 (MLP: Single/Multi bounce?) → Stage 3 (MLP: Predict bias) → d_corrected\n",
    "```\n",
    "\n",
    "**Split**: 70% Train / 15% Validation / 15% Test (matching Xu Xueli 2024)  \n",
    "**Test set**: Completely unseen — never used during training or hyperparameter tuning  \n",
    "\n",
    "| Stage | Model | Input | Output |\n",
    "|-------|-------|-------|--------|\n",
    "| 1 | PI-HLNN (Liquid Neural Network) | Raw 60-sample CIR window | LOS (0) / NLOS (1) |\n",
    "| 2 | MLP Classifier (32→16→1) | 6 CIR physics features | Single-bounce (0) / Multi-bounce (1) |\n",
    "| 3 | MLP Regressor (64→32→1) | 6 CIR physics features | NLOS bias (meters) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "tp-imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import copy\n",
    "from scipy.signal import find_peaks\n",
    "from scipy.stats import kurtosis as scipy_kurtosis\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, ConfusionMatrixDisplay,\n",
    "    classification_report, r2_score,\n",
    "    mean_absolute_error, mean_squared_error\n",
    ")\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tp-split-md",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 1: Data Loading & 70/15/15 Split\n",
    "\n",
    "Stratified split ensuring LOS/NLOS ratio preserved in all 3 sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "tp-split",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 3600\n",
      "  LOS:  1800\n",
      "  NLOS: 1800\n",
      "\n",
      "Split Results:\n",
      "  Train: 2520 (70.0%) — LOS=1260, NLOS=1260\n",
      "  Val:   540 (15.0%) — LOS=270, NLOS=270\n",
      "  Test:  540 (15.0%) — LOS=270, NLOS=270\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# GROUND TRUTH\n",
    "# ==========================================\n",
    "GROUND_TRUTH = {\n",
    "    '7.79m':  {'d_direct': 7.79,  'd_bounce': 12.79, 'bias': 5.00},\n",
    "    '10.77m': {'d_direct': 10.77, 'd_bounce': 16.09, 'bias': 5.32},\n",
    "    '14m':    {'d_direct': 14.00, 'd_bounce': 16.80, 'bias': 2.80},\n",
    "}\n",
    "\n",
    "def get_distance_group(fname):\n",
    "    match = re.match(r'^([\\d.]+m)', str(fname))\n",
    "    return match.group(1) if match else None\n",
    "\n",
    "# ==========================================\n",
    "# LOAD FULL DATASET\n",
    "# ==========================================\n",
    "df = pd.read_csv('../dataset/channels/combined_uwb_dataset.csv')\n",
    "print(f'Total samples: {len(df)}')\n",
    "print(f'  LOS:  {(df[\"Label\"]==0).sum()}')\n",
    "print(f'  NLOS: {(df[\"Label\"]==1).sum()}')\n",
    "\n",
    "# ==========================================\n",
    "# 70 / 15 / 15 STRATIFIED SPLIT\n",
    "# ==========================================\n",
    "# First split: 70% train, 30% temp\n",
    "train_df, temp_df = train_test_split(\n",
    "    df, test_size=0.30, random_state=SEED, stratify=df['Label']\n",
    ")\n",
    "# Second split: 50/50 of temp -> 15% val, 15% test\n",
    "val_df, test_df = train_test_split(\n",
    "    temp_df, test_size=0.50, random_state=SEED, stratify=temp_df['Label']\n",
    ")\n",
    "\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "val_df = val_df.reset_index(drop=True)\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "\n",
    "print(f'\\nSplit Results:')\n",
    "print(f'  Train: {len(train_df)} ({100*len(train_df)/len(df):.1f}%) — LOS={int((train_df[\"Label\"]==0).sum())}, NLOS={int((train_df[\"Label\"]==1).sum())}')\n",
    "print(f'  Val:   {len(val_df)} ({100*len(val_df)/len(df):.1f}%) — LOS={int((val_df[\"Label\"]==0).sum())}, NLOS={int((val_df[\"Label\"]==1).sum())}')\n",
    "print(f'  Test:  {len(test_df)} ({100*len(test_df)/len(df):.1f}%) — LOS={int((test_df[\"Label\"]==0).sum())}, NLOS={int((test_df[\"Label\"]==1).sum())}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tp-preproc-md",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 2: Shared Preprocessing & Feature Extraction\n",
    "\n",
    "Functions shared by all 3 stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "tp-preproc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing functions defined.\n",
      "CIR columns: 1016\n"
     ]
    }
   ],
   "source": [
    "CIR_COLS = sorted(\n",
    "    [c for c in df.columns if c.startswith('CIR')],\n",
    "    key=lambda x: int(x.replace('CIR', ''))\n",
    ")\n",
    "\n",
    "# ==========================================\n",
    "# ROI ALIGNMENT (shared)\n",
    "# ==========================================\n",
    "def get_roi_alignment(sig, search_start=700, search_end=800):\n",
    "    region = sig[search_start:search_end]\n",
    "    if len(region) == 0:\n",
    "        return np.argmax(sig)\n",
    "    peak_local = np.argmax(region)\n",
    "    peak_idx = search_start + peak_local\n",
    "    peak_val = sig[peak_idx]\n",
    "    noise_section = sig[:search_start]\n",
    "    if len(noise_section) > 10:\n",
    "        noise_mean = np.mean(noise_section)\n",
    "        noise_std = np.std(noise_section)\n",
    "        threshold = max(noise_mean + 3 * noise_std, 0.05 * peak_val)\n",
    "    else:\n",
    "        threshold = 0.05 * peak_val\n",
    "    leading_edge = peak_idx\n",
    "    for i in range(peak_idx, max(search_start - 20, 0), -1):\n",
    "        if sig[i] < threshold:\n",
    "            leading_edge = i + 1\n",
    "            break\n",
    "    return leading_edge\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# STAGE 1 PREPROCESSING: Raw CIR -> 60-sample window\n",
    "# ==========================================\n",
    "S1_CONFIG = {\n",
    "    'pre_crop': 10, 'post_crop': 50, 'total_len': 60,\n",
    "    'hidden_size': 64, 'input_size': 1, 'dropout': 0.35,\n",
    "    'batch_size': 32, 'max_epochs': 50, 'lr': 3e-3,\n",
    "    'weight_decay': 1e-4, 'patience': 12,\n",
    "    'scheduler_patience': 7, 'scheduler_factor': 0.5,\n",
    "    'grad_clip': 1.0,\n",
    "}\n",
    "\n",
    "def preprocess_stage1(data_df):\n",
    "    \"\"\"Convert DataFrame to Stage 1 input: (N, 60, 1) CIR windows.\"\"\"\n",
    "    PRE, TOTAL = S1_CONFIG['pre_crop'], S1_CONFIG['total_len']\n",
    "    seqs, labels = [], []\n",
    "    for _, row in data_df.iterrows():\n",
    "        sig = pd.to_numeric(row[CIR_COLS], errors='coerce').fillna(0).astype(float).values\n",
    "        rxpacc = float(row.get('RXPACC', row.get('RX_PACC', 128.0)))\n",
    "        if rxpacc > 0:\n",
    "            sig = sig / rxpacc\n",
    "        le = get_roi_alignment(sig)\n",
    "        start = max(0, le - PRE)\n",
    "        end = start + TOTAL\n",
    "        if end > len(sig):\n",
    "            end = len(sig)\n",
    "            start = max(0, end - TOTAL)\n",
    "        crop = sig[start:end]\n",
    "        if len(crop) < TOTAL:\n",
    "            crop = np.pad(crop, (0, TOTAL - len(crop)), mode='constant')\n",
    "        local_min, local_max = np.min(crop), np.max(crop)\n",
    "        rng = local_max - local_min\n",
    "        crop = (crop - local_min) / rng if rng > 0 else np.zeros(TOTAL)\n",
    "        seqs.append(crop)\n",
    "        labels.append(float(row['Label']))\n",
    "    X = np.array(seqs).reshape(-1, TOTAL, 1).astype(np.float32)\n",
    "    y = np.array(labels).astype(np.float32)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# STAGES 2 & 3 FEATURE EXTRACTION: CIR -> 6 physics features\n",
    "# ==========================================\n",
    "FEATURE_NAMES = [\n",
    "    'Kurtosis', 'Rise_Time', 'RMS_Delay_Spread',\n",
    "    'Mean_Excess_Delay', 'Power_Ratio', 'Num_Peaks'\n",
    "]\n",
    "\n",
    "PEAK_CONFIG = {\n",
    "    'peak_prominence': 0.20, 'peak_min_distance': 5,\n",
    "    'single_bounce_max_peaks': 2,\n",
    "    'search_start': 700, 'search_end': 800,\n",
    "}\n",
    "\n",
    "def extract_cir_features(sig, leading_edge):\n",
    "    ss, se = PEAK_CONFIG['search_start'], PEAK_CONFIG['search_end']\n",
    "    peak_idx = np.argmax(sig[ss:se]) + ss\n",
    "    win_start = max(0, peak_idx - 10)\n",
    "    win_end = min(len(sig), peak_idx + 30)\n",
    "    window = sig[win_start:win_end]\n",
    "    kurt = scipy_kurtosis(window, fisher=True) if len(window) > 4 else 0.0\n",
    "    rise_time = float(peak_idx - leading_edge)\n",
    "    pdp_start = max(0, leading_edge)\n",
    "    pdp_sig = sig[pdp_start:min(pdp_start + 150, len(sig))]\n",
    "    pdp = pdp_sig ** 2\n",
    "    total_pdp = np.sum(pdp)\n",
    "    if total_pdp > 0:\n",
    "        times = np.arange(len(pdp), dtype=float)\n",
    "        mean_delay = np.sum(pdp * times) / total_pdp\n",
    "        second_moment = np.sum(pdp * times ** 2) / total_pdp\n",
    "        rms_spread = np.sqrt(max(0, second_moment - mean_delay ** 2))\n",
    "    else:\n",
    "        mean_delay, rms_spread = 0.0, 0.0\n",
    "    fp_start = max(0, leading_edge - 1)\n",
    "    fp_end = min(len(sig), leading_edge + 2)\n",
    "    fp_energy = np.sum(sig[fp_start:fp_end] ** 2)\n",
    "    total_energy = np.sum(sig ** 2)\n",
    "    power_ratio = fp_energy / total_energy if total_energy > 0 else 0.0\n",
    "    roi_start = max(0, leading_edge - 5)\n",
    "    roi_end = min(len(sig), leading_edge + 120)\n",
    "    roi = sig[roi_start:roi_end]\n",
    "    if len(roi) > 0 and np.max(roi) > 0:\n",
    "        roi_norm = roi / np.max(roi)\n",
    "        peaks, _ = find_peaks(roi_norm, prominence=PEAK_CONFIG['peak_prominence'],\n",
    "                              distance=PEAK_CONFIG['peak_min_distance'])\n",
    "        num_peaks = len(peaks)\n",
    "    else:\n",
    "        num_peaks = 0\n",
    "    return {\n",
    "        'Kurtosis': kurt, 'Rise_Time': rise_time,\n",
    "        'RMS_Delay_Spread': rms_spread, 'Mean_Excess_Delay': mean_delay,\n",
    "        'Power_Ratio': power_ratio, 'Num_Peaks': float(num_peaks),\n",
    "    }\n",
    "\n",
    "\n",
    "def extract_features_from_df(data_df):\n",
    "    \"\"\"Extract 6 CIR features + metadata from DataFrame.\"\"\"\n",
    "    features_list, source_files, distances = [], [], []\n",
    "    for _, row in data_df.iterrows():\n",
    "        sig = pd.to_numeric(row[CIR_COLS], errors='coerce').fillna(0).astype(float).values\n",
    "        rxpacc = float(row.get('RXPACC', row.get('RX_PACC', 128.0)))\n",
    "        if rxpacc > 0:\n",
    "            sig = sig / rxpacc\n",
    "        le = get_roi_alignment(sig)\n",
    "        feats = extract_cir_features(sig, le)\n",
    "        features_list.append(feats)\n",
    "        source_files.append(str(row['Source_File']))\n",
    "        distances.append(float(row['Distance']))\n",
    "    return pd.DataFrame(features_list), source_files, distances\n",
    "\n",
    "\n",
    "print('Preprocessing functions defined.')\n",
    "print(f'CIR columns: {len(CIR_COLS)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tp-models-md",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 3: Model Architectures\n",
    "\n",
    "All 3 model classes — identical to the individual stage notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "tp-models",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All 3 model architectures defined.\n",
      "  Stage 1 (PI-HLNN):         10,625 params\n",
      "  Stage 2 (BounceClassifier): 769 params\n",
      "  Stage 3 (NLOSBiasPredictor):2,561 params\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# STAGE 1: PI-HLNN (Liquid Neural Network)\n",
    "# ==========================================\n",
    "class PILiquidCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.synapse = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.tau_net = nn.Sequential(\n",
    "            nn.Linear(input_size + hidden_size, 32),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(32, hidden_size),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.A = nn.Parameter(torch.ones(hidden_size) * -0.5)\n",
    "\n",
    "    def forward(self, x_t, h_prev, dt=1.0):\n",
    "        combined = torch.cat([x_t, h_prev], dim=1)\n",
    "        tau = 1.0 + 5.0 * self.tau_net(combined)\n",
    "        S_t = torch.tanh(self.synapse(combined))\n",
    "        numerator = h_prev + (dt * S_t * self.A)\n",
    "        denominator = 1.0 + (dt / tau)\n",
    "        h_new = numerator / denominator\n",
    "        return h_new, tau\n",
    "\n",
    "\n",
    "class PI_HLNN(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_size=64, dropout=0.35):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.cell = PILiquidCell(input_size, hidden_size)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 32),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(32, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x_seq, return_dynamics=False):\n",
    "        batch_size, seq_len, _ = x_seq.size()\n",
    "        h_t = torch.zeros(batch_size, self.hidden_size, device=x_seq.device)\n",
    "        h_sum = torch.zeros_like(h_t)\n",
    "        tau_sum = torch.zeros_like(h_t)\n",
    "        for t in range(seq_len):\n",
    "            h_t, tau_t = self.cell(x_seq[:, t, :], h_t)\n",
    "            h_sum += h_t\n",
    "            tau_sum += tau_t\n",
    "        h_pooled = h_sum / seq_len\n",
    "        tau_mean = tau_sum / seq_len\n",
    "        prediction = self.classifier(h_pooled)\n",
    "        return prediction, tau_mean\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# STAGE 2: Bounce Classifier (MLP)\n",
    "# ==========================================\n",
    "class BounceClassifier(nn.Module):\n",
    "    def __init__(self, input_size=6, hidden_layers=[32, 16], dropout=0.3):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        prev_size = input_size\n",
    "        for h_size in hidden_layers:\n",
    "            layers.extend([nn.Linear(prev_size, h_size), nn.ReLU(), nn.Dropout(dropout)])\n",
    "            prev_size = h_size\n",
    "        layers.append(nn.Linear(prev_size, 1))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# STAGE 3: NLOS Bias Predictor (MLP)\n",
    "# ==========================================\n",
    "class NLOSBiasPredictor(nn.Module):\n",
    "    def __init__(self, input_size=6, hidden_layers=[64, 32], dropout=0.2):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        prev_size = input_size\n",
    "        for h_size in hidden_layers:\n",
    "            layers.extend([nn.Linear(prev_size, h_size), nn.ReLU(), nn.Dropout(dropout)])\n",
    "            prev_size = h_size\n",
    "        layers.append(nn.Linear(prev_size, 1))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "print('All 3 model architectures defined.')\n",
    "print(f'  Stage 1 (PI-HLNN):         {sum(p.numel() for p in PI_HLNN().parameters()):,} params')\n",
    "print(f'  Stage 2 (BounceClassifier): {sum(p.numel() for p in BounceClassifier().parameters()):,} params')\n",
    "print(f'  Stage 3 (NLOSBiasPredictor):{sum(p.numel() for p in NLOSBiasPredictor().parameters()):,} params')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tp-train-md",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 4: Train All 3 Stages on 70% Train Set\n",
    "\n",
    "Each stage trained independently. Validation set (15%) used for early stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "tp-train-s1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STAGE 1: Training PI-HLNN on 70% train set...\n",
      "============================================================\n",
      "  Train: (2520, 60, 1), Val: (540, 60, 1)\n",
      "  Ep   0 | Val Acc: 75.19% | Best: 75.19%\n",
      "  Ep   5 | Val Acc: 97.41% | Best: 97.41%\n",
      "  Ep  10 | Val Acc: 97.41% | Best: 97.96%\n",
      "  Ep  15 | Val Acc: 99.63% | Best: 99.63%\n",
      "  Ep  20 | Val Acc: 98.15% | Best: 99.81%\n",
      "  Ep  25 | Val Acc: 96.11% | Best: 99.81%\n",
      "  Ep  30 | Val Acc: 99.81% | Best: 100.00%\n",
      "  Ep  35 | Val Acc: 99.63% | Best: 100.00%\n",
      "  Ep  40 | Val Acc: 99.81% | Best: 100.00%\n",
      "  Early stopping at epoch 40\n",
      "\n",
      "Stage 1 Best Val Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# STAGE 1 TRAINING: PI-HLNN (LOS/NLOS)\n",
    "# ==========================================\n",
    "print('STAGE 1: Training PI-HLNN on 70% train set...')\n",
    "print('='*60)\n",
    "\n",
    "# Preprocess\n",
    "X_train_s1, y_train_s1 = preprocess_stage1(train_df)\n",
    "X_val_s1, y_val_s1 = preprocess_stage1(val_df)\n",
    "print(f'  Train: {X_train_s1.shape}, Val: {X_val_s1.shape}')\n",
    "\n",
    "# To tensors\n",
    "X_tr1 = torch.tensor(X_train_s1).to(device)\n",
    "y_tr1 = torch.tensor(y_train_s1).unsqueeze(1).to(device)\n",
    "X_va1 = torch.tensor(X_val_s1).to(device)\n",
    "y_va1 = torch.tensor(y_val_s1).unsqueeze(1).to(device)\n",
    "\n",
    "train_ds1 = TensorDataset(X_tr1, y_tr1)\n",
    "train_loader1 = DataLoader(train_ds1, batch_size=S1_CONFIG['batch_size'], shuffle=True)\n",
    "\n",
    "# Model\n",
    "model_s1 = PI_HLNN(\n",
    "    input_size=S1_CONFIG['input_size'],\n",
    "    hidden_size=S1_CONFIG['hidden_size'],\n",
    "    dropout=S1_CONFIG['dropout']\n",
    ").to(device)\n",
    "\n",
    "criterion_s1 = nn.BCELoss()\n",
    "optimizer_s1 = optim.AdamW(model_s1.parameters(), lr=S1_CONFIG['lr'],\n",
    "                           weight_decay=S1_CONFIG['weight_decay'])\n",
    "scheduler_s1 = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer_s1, mode='max', factor=S1_CONFIG['scheduler_factor'],\n",
    "    patience=S1_CONFIG['scheduler_patience'])\n",
    "\n",
    "best_val_acc_s1 = 0\n",
    "best_state_s1 = None\n",
    "patience_s1 = 0\n",
    "\n",
    "for epoch in range(S1_CONFIG['max_epochs']):\n",
    "    model_s1.train()\n",
    "    for bx, by in train_loader1:\n",
    "        optimizer_s1.zero_grad()\n",
    "        pred, tau = model_s1(bx)\n",
    "        loss = criterion_s1(pred, by)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model_s1.parameters(), S1_CONFIG['grad_clip'])\n",
    "        optimizer_s1.step()\n",
    "\n",
    "    model_s1.eval()\n",
    "    with torch.no_grad():\n",
    "        vp, _ = model_s1(X_va1)\n",
    "        val_acc = ((vp > 0.5).float() == y_va1).float().mean().item()\n",
    "    scheduler_s1.step(val_acc)\n",
    "\n",
    "    if val_acc > best_val_acc_s1:\n",
    "        best_val_acc_s1 = val_acc\n",
    "        best_state_s1 = copy.deepcopy(model_s1.state_dict())\n",
    "        patience_s1 = 0\n",
    "    else:\n",
    "        patience_s1 += 1\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        print(f'  Ep {epoch:>3} | Val Acc: {100*val_acc:.2f}% | Best: {100*best_val_acc_s1:.2f}%')\n",
    "    if patience_s1 >= S1_CONFIG['patience']:\n",
    "        print(f'  Early stopping at epoch {epoch}')\n",
    "        break\n",
    "\n",
    "model_s1.load_state_dict(best_state_s1)\n",
    "print(f'\\nStage 1 Best Val Accuracy: {100*best_val_acc_s1:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "tp-train-s2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STAGE 2: Training Bounce Classifier on NLOS subset of train set...\n",
      "============================================================\n",
      "  Train NLOS: 1260 — single=584, multi=676\n",
      "  Val NLOS:   270 — single=139, multi=131\n",
      "  Ep   0 | Val Acc: 85.56% | Best: 85.56%\n",
      "  Ep  10 | Val Acc: 94.44% | Best: 94.44%\n",
      "  Ep  20 | Val Acc: 99.26% | Best: 99.26%\n",
      "  Ep  30 | Val Acc: 99.63% | Best: 99.63%\n",
      "  Ep  40 | Val Acc: 100.00% | Best: 100.00%\n",
      "  Early stopping at epoch 48\n",
      "\n",
      "Stage 2 Best Val Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# STAGE 2 TRAINING: Bounce Classifier (NLOS only)\n",
    "# ==========================================\n",
    "print('STAGE 2: Training Bounce Classifier on NLOS subset of train set...')\n",
    "print('='*60)\n",
    "\n",
    "S2_CONFIG = {\n",
    "    'hidden_layers': [32, 16], 'dropout': 0.3,\n",
    "    'batch_size': 32, 'max_epochs': 100, 'lr': 1e-3,\n",
    "    'weight_decay': 1e-4, 'patience': 15,\n",
    "    'scheduler_patience': 7, 'scheduler_factor': 0.5,\n",
    "}\n",
    "\n",
    "# Extract features from NLOS train & val\n",
    "nlos_train = train_df[train_df['Label'] == 1].reset_index(drop=True)\n",
    "nlos_val = val_df[val_df['Label'] == 1].reset_index(drop=True)\n",
    "\n",
    "feat_train_s2, _, _ = extract_features_from_df(nlos_train)\n",
    "feat_val_s2, _, _ = extract_features_from_df(nlos_val)\n",
    "\n",
    "# Auto-label: single-bounce (0) if Num_Peaks <= 2, multi-bounce (1) otherwise\n",
    "threshold = PEAK_CONFIG['single_bounce_max_peaks']\n",
    "y_train_s2 = (feat_train_s2['Num_Peaks'] > threshold).astype(float).values\n",
    "y_val_s2 = (feat_val_s2['Num_Peaks'] > threshold).astype(float).values\n",
    "\n",
    "print(f'  Train NLOS: {len(nlos_train)} — single={int((y_train_s2==0).sum())}, multi={int((y_train_s2==1).sum())}')\n",
    "print(f'  Val NLOS:   {len(nlos_val)} — single={int((y_val_s2==0).sum())}, multi={int((y_val_s2==1).sum())}')\n",
    "\n",
    "# Scale features\n",
    "scaler_s2 = StandardScaler()\n",
    "X_tr2 = torch.tensor(scaler_s2.fit_transform(feat_train_s2[FEATURE_NAMES].values.astype(np.float32))).to(device)\n",
    "X_va2 = torch.tensor(scaler_s2.transform(feat_val_s2[FEATURE_NAMES].values.astype(np.float32))).to(device)\n",
    "y_tr2 = torch.tensor(y_train_s2).unsqueeze(1).to(device)\n",
    "y_va2 = torch.tensor(y_val_s2).unsqueeze(1).to(device)\n",
    "\n",
    "# Class weight\n",
    "n_single = int((y_train_s2 == 0).sum())\n",
    "n_multi = int((y_train_s2 == 1).sum())\n",
    "pos_weight_s2 = torch.tensor([n_single / max(n_multi, 1)], dtype=torch.float32).to(device)\n",
    "\n",
    "train_ds2 = TensorDataset(X_tr2, y_tr2)\n",
    "train_loader2 = DataLoader(train_ds2, batch_size=S2_CONFIG['batch_size'], shuffle=True)\n",
    "\n",
    "model_s2 = BounceClassifier(\n",
    "    input_size=len(FEATURE_NAMES),\n",
    "    hidden_layers=S2_CONFIG['hidden_layers'],\n",
    "    dropout=S2_CONFIG['dropout']\n",
    ").to(device)\n",
    "\n",
    "criterion_s2 = nn.BCEWithLogitsLoss(pos_weight=pos_weight_s2)\n",
    "optimizer_s2 = optim.AdamW(model_s2.parameters(), lr=S2_CONFIG['lr'],\n",
    "                           weight_decay=S2_CONFIG['weight_decay'])\n",
    "scheduler_s2 = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer_s2, mode='max', factor=S2_CONFIG['scheduler_factor'],\n",
    "    patience=S2_CONFIG['scheduler_patience'])\n",
    "\n",
    "best_val_acc_s2 = 0\n",
    "best_state_s2 = None\n",
    "patience_s2 = 0\n",
    "\n",
    "for epoch in range(S2_CONFIG['max_epochs']):\n",
    "    model_s2.train()\n",
    "    for bx, by in train_loader2:\n",
    "        optimizer_s2.zero_grad()\n",
    "        loss = criterion_s2(model_s2(bx), by)\n",
    "        loss.backward()\n",
    "        optimizer_s2.step()\n",
    "\n",
    "    model_s2.eval()\n",
    "    with torch.no_grad():\n",
    "        vl = model_s2(X_va2)\n",
    "        val_acc = ((torch.sigmoid(vl) > 0.5).float() == y_va2).float().mean().item()\n",
    "    scheduler_s2.step(val_acc)\n",
    "\n",
    "    if val_acc > best_val_acc_s2:\n",
    "        best_val_acc_s2 = val_acc\n",
    "        best_state_s2 = copy.deepcopy(model_s2.state_dict())\n",
    "        patience_s2 = 0\n",
    "    else:\n",
    "        patience_s2 += 1\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'  Ep {epoch:>3} | Val Acc: {100*val_acc:.2f}% | Best: {100*best_val_acc_s2:.2f}%')\n",
    "    if patience_s2 >= S2_CONFIG['patience']:\n",
    "        print(f'  Early stopping at epoch {epoch}')\n",
    "        break\n",
    "\n",
    "model_s2.load_state_dict(best_state_s2)\n",
    "print(f'\\nStage 2 Best Val Accuracy: {100*best_val_acc_s2:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "tp-train-s3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STAGE 3: Training Bias Predictor on single-bounce NLOS...\n",
      "============================================================\n",
      "  Train single-bounce NLOS: 584\n",
      "  Val single-bounce NLOS:   139\n",
      "  Ep   0 | Val MAE: 4.6260m | Best: 4.6260m\n",
      "  Ep  20 | Val MAE: 0.3671m | Best: 0.3671m\n",
      "  Ep  40 | Val MAE: 0.2310m | Best: 0.2310m\n",
      "  Ep  60 | Val MAE: 0.1762m | Best: 0.1762m\n",
      "  Ep  80 | Val MAE: 0.1806m | Best: 0.1693m\n",
      "  Ep 100 | Val MAE: 0.1311m | Best: 0.1311m\n",
      "  Ep 120 | Val MAE: 0.1348m | Best: 0.1197m\n",
      "  Ep 140 | Val MAE: 0.1558m | Best: 0.1073m\n",
      "  Early stopping at epoch 149\n",
      "\n",
      "Stage 3 Best Val MAE: 0.1073m\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# STAGE 3 TRAINING: NLOS Bias Predictor (single-bounce NLOS only)\n",
    "# ==========================================\n",
    "print('STAGE 3: Training Bias Predictor on single-bounce NLOS...')\n",
    "print('='*60)\n",
    "\n",
    "S3_CONFIG = {\n",
    "    'hidden_layers': [64, 32], 'dropout': 0.2,\n",
    "    'batch_size': 32, 'max_epochs': 200, 'lr': 1e-3,\n",
    "    'weight_decay': 1e-4, 'patience': 20,\n",
    "    'scheduler_patience': 10, 'scheduler_factor': 0.5,\n",
    "}\n",
    "\n",
    "MEASURED_NLOS_BIAS = {'7.79m': 5.00, '10.77m': 5.32, '14m': 2.80}\n",
    "\n",
    "# Filter single-bounce NLOS from train & val\n",
    "def get_s3_data(feat_df, source_files):\n",
    "    \"\"\"Filter to single-bounce NLOS with known ground truth bias.\"\"\"\n",
    "    features, biases = [], []\n",
    "    for i, fname in enumerate(source_files):\n",
    "        grp = get_distance_group(fname)\n",
    "        if grp not in MEASURED_NLOS_BIAS:\n",
    "            continue\n",
    "        if feat_df.iloc[i]['Num_Peaks'] > PEAK_CONFIG['single_bounce_max_peaks']:\n",
    "            continue\n",
    "        features.append(feat_df.iloc[i][FEATURE_NAMES].values)\n",
    "        biases.append(MEASURED_NLOS_BIAS[grp])\n",
    "    return np.array(features, dtype=np.float32), np.array(biases, dtype=np.float32)\n",
    "\n",
    "feat_train_nlos, src_train_nlos, _ = extract_features_from_df(nlos_train)\n",
    "feat_val_nlos, src_val_nlos, _ = extract_features_from_df(nlos_val)\n",
    "\n",
    "X_train_s3_raw, y_train_s3 = get_s3_data(feat_train_nlos, src_train_nlos)\n",
    "X_val_s3_raw, y_val_s3 = get_s3_data(feat_val_nlos, src_val_nlos)\n",
    "\n",
    "print(f'  Train single-bounce NLOS: {len(X_train_s3_raw)}')\n",
    "print(f'  Val single-bounce NLOS:   {len(X_val_s3_raw)}')\n",
    "\n",
    "# Scale\n",
    "scaler_s3 = StandardScaler()\n",
    "X_tr3 = torch.tensor(scaler_s3.fit_transform(X_train_s3_raw)).to(device)\n",
    "X_va3 = torch.tensor(scaler_s3.transform(X_val_s3_raw)).to(device)\n",
    "y_tr3 = torch.tensor(y_train_s3).unsqueeze(1).to(device)\n",
    "y_va3 = torch.tensor(y_val_s3).unsqueeze(1).to(device)\n",
    "\n",
    "train_ds3 = TensorDataset(X_tr3, y_tr3)\n",
    "train_loader3 = DataLoader(train_ds3, batch_size=S3_CONFIG['batch_size'], shuffle=True)\n",
    "\n",
    "model_s3 = NLOSBiasPredictor(\n",
    "    input_size=len(FEATURE_NAMES),\n",
    "    hidden_layers=S3_CONFIG['hidden_layers'],\n",
    "    dropout=S3_CONFIG['dropout']\n",
    ").to(device)\n",
    "\n",
    "criterion_s3 = nn.MSELoss()\n",
    "optimizer_s3 = optim.AdamW(model_s3.parameters(), lr=S3_CONFIG['lr'],\n",
    "                           weight_decay=S3_CONFIG['weight_decay'])\n",
    "scheduler_s3 = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer_s3, mode='min', factor=S3_CONFIG['scheduler_factor'],\n",
    "    patience=S3_CONFIG['scheduler_patience'])\n",
    "\n",
    "best_val_mae_s3 = float('inf')\n",
    "best_state_s3 = None\n",
    "patience_s3 = 0\n",
    "\n",
    "for epoch in range(S3_CONFIG['max_epochs']):\n",
    "    model_s3.train()\n",
    "    for bx, by in train_loader3:\n",
    "        optimizer_s3.zero_grad()\n",
    "        loss = criterion_s3(model_s3(bx), by)\n",
    "        loss.backward()\n",
    "        optimizer_s3.step()\n",
    "\n",
    "    model_s3.eval()\n",
    "    with torch.no_grad():\n",
    "        vp3 = model_s3(X_va3)\n",
    "        val_mae = torch.abs(vp3 - y_va3).mean().item()\n",
    "    scheduler_s3.step(val_mae)\n",
    "\n",
    "    if val_mae < best_val_mae_s3:\n",
    "        best_val_mae_s3 = val_mae\n",
    "        best_state_s3 = copy.deepcopy(model_s3.state_dict())\n",
    "        patience_s3 = 0\n",
    "    else:\n",
    "        patience_s3 += 1\n",
    "\n",
    "    if epoch % 20 == 0:\n",
    "        print(f'  Ep {epoch:>3} | Val MAE: {val_mae:.4f}m | Best: {best_val_mae_s3:.4f}m')\n",
    "    if patience_s3 >= S3_CONFIG['patience']:\n",
    "        print(f'  Early stopping at epoch {epoch}')\n",
    "        break\n",
    "\n",
    "model_s3.load_state_dict(best_state_s3)\n",
    "print(f'\\nStage 3 Best Val MAE: {best_val_mae_s3:.4f}m')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tp-test-md",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 5: End-to-End Pipeline Test on Unseen 15% Test Set\n",
    "\n",
    "The test set has **never** been seen during training or validation.  \n",
    "Run the full chained pipeline:\n",
    "```\n",
    "CIR → Stage 1 (LOS/NLOS?) → Stage 2 (bounce type?) → Stage 3 (bias) → d_corrected\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "tp-test",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END-TO-END PIPELINE TEST ON UNSEEN TEST SET\n",
      "======================================================================\n",
      "Test set: 540 samples (completely unseen)\n",
      "\n",
      "STAGE 1 (LOS/NLOS Classification):\n",
      "  Accuracy: 99.63%\n",
      "  Predicted LOS: 272, NLOS: 268\n",
      "  Actual    LOS: 270, NLOS: 270\n",
      "\n",
      "STAGE 2 (Bounce Classification — on Stage 1 NLOS predictions):\n",
      "  Samples entering Stage 2: 268\n",
      "  Accuracy: 98.51%\n",
      "  Predicted single-bounce: 126, multi: 142\n",
      "  Actual    single-bounce: 122, multi: 146\n",
      "\n",
      "STAGE 3 (Bias Prediction — on predicted single-bounce NLOS):\n",
      "  Samples entering Stage 3: 126\n",
      "  Bias MAE:  0.1781m\n",
      "  Bias RMSE: 0.4719m\n",
      "  Correction MAE: 4.0628m\n",
      "\n",
      "======================================================================\n",
      "END-TO-END PIPELINE SUMMARY (on unseen test set)\n",
      "======================================================================\n",
      "  Stage 1 (LOS/NLOS):        99.63% accuracy\n",
      "  Stage 2 (bounce type):     98.51% accuracy\n",
      "  Stage 3 (bias prediction): MAE = 0.1781m\n",
      "  Distance correction:       MAE = 4.0628m\n",
      "\n",
      "  Pipeline flow: 540 total → 268 NLOS → 126 single-bounce → 126 with GT\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# END-TO-END PIPELINE TEST\n",
    "# ==========================================\n",
    "print('END-TO-END PIPELINE TEST ON UNSEEN TEST SET')\n",
    "print('='*70)\n",
    "print(f'Test set: {len(test_df)} samples (completely unseen)\\n')\n",
    "\n",
    "# --- Stage 1: LOS/NLOS Classification ---\n",
    "X_test_s1, y_test_s1 = preprocess_stage1(test_df)\n",
    "X_t1 = torch.tensor(X_test_s1).to(device)\n",
    "\n",
    "model_s1.eval()\n",
    "with torch.no_grad():\n",
    "    s1_probs, _ = model_s1(X_t1)\n",
    "    s1_preds = (s1_probs > 0.5).float().cpu().numpy().flatten()\n",
    "\n",
    "s1_true = y_test_s1\n",
    "s1_acc = (s1_preds == s1_true).mean()\n",
    "print(f'STAGE 1 (LOS/NLOS Classification):')\n",
    "print(f'  Accuracy: {100*s1_acc:.2f}%')\n",
    "print(f'  Predicted LOS: {int((s1_preds==0).sum())}, NLOS: {int((s1_preds==1).sum())}')\n",
    "print(f'  Actual    LOS: {int((s1_true==0).sum())}, NLOS: {int((s1_true==1).sum())}\\n')\n",
    "\n",
    "# --- Stage 2: Bounce Classification (on predicted NLOS) ---\n",
    "# Get test samples that Stage 1 predicts as NLOS\n",
    "nlos_mask_pred = s1_preds == 1\n",
    "nlos_test_df = test_df[nlos_mask_pred].reset_index(drop=True)\n",
    "nlos_test_actual_labels = s1_true[nlos_mask_pred]\n",
    "\n",
    "feat_test_nlos, src_test_nlos, dist_test_nlos = extract_features_from_df(nlos_test_df)\n",
    "\n",
    "# Ground truth bounce labels\n",
    "y_test_bounce_true = (feat_test_nlos['Num_Peaks'] > PEAK_CONFIG['single_bounce_max_peaks']).astype(float).values\n",
    "\n",
    "# Predict\n",
    "X_t2 = torch.tensor(scaler_s2.transform(\n",
    "    feat_test_nlos[FEATURE_NAMES].values.astype(np.float32)\n",
    ")).to(device)\n",
    "\n",
    "model_s2.eval()\n",
    "with torch.no_grad():\n",
    "    s2_logits = model_s2(X_t2)\n",
    "    s2_preds = (torch.sigmoid(s2_logits) > 0.5).float().cpu().numpy().flatten()\n",
    "\n",
    "s2_acc = (s2_preds == y_test_bounce_true).mean()\n",
    "print(f'STAGE 2 (Bounce Classification — on Stage 1 NLOS predictions):')\n",
    "print(f'  Samples entering Stage 2: {len(nlos_test_df)}')\n",
    "print(f'  Accuracy: {100*s2_acc:.2f}%')\n",
    "print(f'  Predicted single-bounce: {int((s2_preds==0).sum())}, multi: {int((s2_preds==1).sum())}')\n",
    "print(f'  Actual    single-bounce: {int((y_test_bounce_true==0).sum())}, multi: {int((y_test_bounce_true==1).sum())}\\n')\n",
    "\n",
    "# --- Stage 3: Bias Prediction (on predicted single-bounce NLOS) ---\n",
    "single_mask_pred = s2_preds == 0  # Stage 2 predicts single-bounce\n",
    "single_test_df = nlos_test_df[single_mask_pred].reset_index(drop=True)\n",
    "single_feat = feat_test_nlos[single_mask_pred].reset_index(drop=True)\n",
    "single_src = [src_test_nlos[i] for i, m in enumerate(single_mask_pred) if m]\n",
    "single_dist = [dist_test_nlos[i] for i, m in enumerate(single_mask_pred) if m]\n",
    "\n",
    "# Get ground truth bias for these samples\n",
    "results = []\n",
    "X_s3_list = []\n",
    "valid_indices = []\n",
    "\n",
    "for i, fname in enumerate(single_src):\n",
    "    grp = get_distance_group(fname)\n",
    "    if grp not in MEASURED_NLOS_BIAS:\n",
    "        continue\n",
    "    gt = GROUND_TRUTH[grp]\n",
    "    feat_vals = single_feat.iloc[i][FEATURE_NAMES].values.astype(np.float32)\n",
    "    X_s3_list.append(feat_vals)\n",
    "    valid_indices.append(i)\n",
    "    results.append({\n",
    "        'source_file': fname, 'group': grp,\n",
    "        'd_uwb': single_dist[i],\n",
    "        'd_direct': gt['d_direct'],\n",
    "        'd_bounce': gt['d_bounce'],\n",
    "        'actual_bias': gt['bias'],\n",
    "    })\n",
    "\n",
    "if len(X_s3_list) > 0:\n",
    "    X_t3 = torch.tensor(scaler_s3.transform(\n",
    "        np.array(X_s3_list)\n",
    "    )).to(device)\n",
    "\n",
    "    model_s3.eval()\n",
    "    with torch.no_grad():\n",
    "        s3_preds = model_s3(X_t3).cpu().numpy().flatten()\n",
    "\n",
    "    for j, r in enumerate(results):\n",
    "        r['ml_bias'] = s3_preds[j]\n",
    "        r['d_corrected'] = r['d_uwb'] - s3_preds[j]\n",
    "        r['bias_error'] = abs(s3_preds[j] - r['actual_bias'])\n",
    "        r['correction_error'] = abs(r['d_corrected'] - r['d_direct'])\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "test_mae = results_df['bias_error'].mean()\n",
    "test_rmse = np.sqrt((results_df['bias_error']**2).mean())\n",
    "\n",
    "print(f'STAGE 3 (Bias Prediction — on predicted single-bounce NLOS):')\n",
    "print(f'  Samples entering Stage 3: {len(results_df)}')\n",
    "print(f'  Bias MAE:  {test_mae:.4f}m')\n",
    "print(f'  Bias RMSE: {test_rmse:.4f}m')\n",
    "print(f'  Correction MAE: {results_df[\"correction_error\"].mean():.4f}m')\n",
    "\n",
    "print(f'\\n{\"=\"*70}')\n",
    "print(f'END-TO-END PIPELINE SUMMARY (on unseen test set)')\n",
    "print(f'{\"=\"*70}')\n",
    "print(f'  Stage 1 (LOS/NLOS):        {100*s1_acc:.2f}% accuracy')\n",
    "print(f'  Stage 2 (bounce type):     {100*s2_acc:.2f}% accuracy')\n",
    "print(f'  Stage 3 (bias prediction): MAE = {test_mae:.4f}m')\n",
    "print(f'  Distance correction:       MAE = {results_df[\"correction_error\"].mean():.4f}m')\n",
    "print(f'\\n  Pipeline flow: {len(test_df)} total → {int(nlos_mask_pred.sum())} NLOS → {int(single_mask_pred.sum())} single-bounce → {len(results_df)} with GT')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tp-viz-md",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 6: Test Set Visualization\n",
    "\n",
    "Comprehensive plots showing pipeline performance on the unseen test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tp-viz",
   "metadata": {},
   "outputs": [],
   "source": "# ==========================================\n# VISUALIZATION: 4-PANEL RESULTS\n# ==========================================\nfig, axs = plt.subplots(2, 2, figsize=(20, 16))\nplt.subplots_adjust(hspace=0.35, wspace=0.3)\n\n# --- Panel 1: Stage 1 Confusion Matrix ---\nax = axs[0, 0]\ncm1 = confusion_matrix(s1_true, s1_preds)\ndisp1 = ConfusionMatrixDisplay(cm1, display_labels=['LOS', 'NLOS'])\ndisp1.plot(ax=ax, cmap='Blues', colorbar=False)\nax.set_title(f'Stage 1: LOS/NLOS Classification\\nAccuracy: {100*s1_acc:.2f}%',\n             fontsize=14, fontweight='bold')\n\n# --- Panel 2: Stage 2 Confusion Matrix ---\nax = axs[0, 1]\ncm2 = confusion_matrix(y_test_bounce_true, s2_preds)\ndisp2 = ConfusionMatrixDisplay(cm2, display_labels=['Single', 'Multi'])\ndisp2.plot(ax=ax, cmap='Greens', colorbar=False)\nax.set_title(f'Stage 2: Bounce Classification\\nAccuracy: {100*s2_acc:.2f}%',\n             fontsize=14, fontweight='bold')\n\n# --- Panel 3: Stage 3 Actual vs Predicted Bias ---\nax = axs[1, 0]\ncolors_map = {'7.79m': '#e74c3c', '10.77m': '#3498db', '14m': '#27ae60'}\nfor grp in sorted(GROUND_TRUTH.keys()):\n    mask = results_df['group'] == grp\n    sub = results_df[mask]\n    if len(sub) > 0:\n        ax.scatter(sub['actual_bias'], sub['ml_bias'], alpha=0.5, s=30,\n                   color=colors_map[grp], label=f'{grp} (n={len(sub)})',\n                   edgecolors='white', linewidth=0.5)\n\nall_bias = np.concatenate([results_df['actual_bias'].values, results_df['ml_bias'].values])\nlims = [all_bias.min() - 0.5, all_bias.max() + 0.5]\nax.plot(lims, lims, 'k--', lw=2, alpha=0.7, label='Perfect prediction')\nax.set_xlabel('Actual NLOS Bias (m)', fontsize=12)\nax.set_ylabel('Predicted NLOS Bias (m)', fontsize=12)\nax.set_title(f'Stage 3: Bias Prediction (Test Set)\\nMAE = {test_mae:.3f}m',\n             fontsize=14, fontweight='bold')\nax.legend(fontsize=10)\nax.grid(True, alpha=0.3)\n\n# --- Panel 4: Distance Correction ---\nax = axs[1, 1]\ngroups_sorted = sorted(GROUND_TRUTH.keys())\nx_pos = np.arange(len(groups_sorted))\nwidth = 0.25\n\nd_uwb_vals, d_corr_vals, d_direct_vals = [], [], []\nfor grp in groups_sorted:\n    gt = GROUND_TRUTH[grp]\n    sub = results_df[results_df['group'] == grp]\n    d_uwb_vals.append(sub['d_uwb'].mean() if len(sub) > 0 else 0)\n    d_corr_vals.append(sub['d_corrected'].mean() if len(sub) > 0 else 0)\n    d_direct_vals.append(gt['d_direct'])\n\nax.bar(x_pos - width, d_uwb_vals, width, label='d_UWB (biased)',\n       color='#e74c3c', alpha=0.8, edgecolor='black')\nax.bar(x_pos, d_corr_vals, width, label='d_corrected (ML)',\n       color='#27ae60', alpha=0.8, edgecolor='black')\nax.bar(x_pos + width, d_direct_vals, width, label='d_direct (truth)',\n       color='#333333', alpha=0.8, edgecolor='black')\n\nfor i, (u, c, d) in enumerate(zip(d_uwb_vals, d_corr_vals, d_direct_vals)):\n    if u > 0:\n        ax.text(i - width, u + 0.15, f'{u:.1f}', ha='center', fontsize=10, fontweight='bold', color='#c0392b')\n        ax.text(i, c + 0.15, f'{c:.1f}', ha='center', fontsize=10, fontweight='bold', color='#1e8449')\n        ax.text(i + width, d + 0.15, f'{d:.1f}', ha='center', fontsize=10, fontweight='bold')\n\nax.set_xticks(x_pos)\nax.set_xticklabels(groups_sorted, fontsize=12)\nax.set_ylabel('Distance (m)', fontsize=12)\nax.set_title(f'Distance Correction: d_corrected = d_UWB - bias\\nCorrection MAE = {results_df[\"correction_error\"].mean():.3f}m',\n             fontsize=14, fontweight='bold')\nax.legend(fontsize=10)\nax.grid(True, alpha=0.3, axis='y')\n\nfig.suptitle(\n    f'End-to-End Pipeline Results — Unseen Test Set ({len(test_df)} samples)\\n'\n    f'70% Train / 15% Val / 15% Test Split',\n    fontsize=16, fontweight='bold', y=1.02\n)\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tp-bounce-benchmark",
   "metadata": {},
   "outputs": [],
   "source": "# ==========================================\n# BOUNCE DISTANCE BENCHMARK vs Xu Xueli (2024)\n# ==========================================\n# Derive d_bounce from bias predictions\nresults_df['actual_d_bounce'] = results_df['group'].map(\n    {g: v['d_bounce'] for g, v in GROUND_TRUTH.items()}\n)\nresults_df['predicted_d_bounce'] = results_df.apply(\n    lambda r: GROUND_TRUTH[r['group']]['d_direct'] + r['ml_bias'], axis=1\n)\n\nour_db_mae = mean_absolute_error(results_df['actual_d_bounce'], results_df['predicted_d_bounce'])\nour_db_rmse = np.sqrt(mean_squared_error(results_df['actual_d_bounce'], results_df['predicted_d_bounce']))\nour_db_r2 = r2_score(results_df['actual_d_bounce'], results_df['predicted_d_bounce'])\n\n# Xu Xueli's reported metrics\nxu_mae, xu_rmse, xu_r2 = 0.34594, 0.60008, 0.61868\n\nfig, axs = plt.subplots(1, 2, figsize=(20, 8))\n\n# Scatter plot\nax = axs[0]\nfor grp in sorted(GROUND_TRUTH.keys()):\n    sub = results_df[results_df['group'] == grp]\n    if len(sub) > 0:\n        ax.scatter(sub['actual_d_bounce'], sub['predicted_d_bounce'],\n                   alpha=0.5, s=40, color=colors_map[grp],\n                   edgecolors='white', linewidth=0.5,\n                   label=f'{grp} (n={len(sub)})')\n\nall_db = np.concatenate([results_df['actual_d_bounce'].values, results_df['predicted_d_bounce'].values])\nlims = [all_db.min() - 1, all_db.max() + 1]\nax.plot(lims, lims, 'k--', lw=2, alpha=0.7)\nax.set_xlabel('Actual 1-Bounce Distance (m)', fontsize=13)\nax.set_ylabel('Predicted 1-Bounce Distance (m)', fontsize=13)\nax.set_title(f'Ours — Unseen Test Set\\nMAE={our_db_mae:.3f}m  R²={our_db_r2:.3f}', fontsize=14, fontweight='bold')\nax.legend(fontsize=11)\nax.grid(True, alpha=0.3)\nax.set_xlim(lims); ax.set_ylim(lims)\nax.set_aspect('equal')\n\n# Metrics comparison\nax = axs[1]\nlabels = ['MAE (m)', 'RMSE (m)', 'R²']\nxu_v = [xu_mae, xu_rmse, xu_r2]\nour_v = [our_db_mae, our_db_rmse, our_db_r2]\nx = np.arange(len(labels))\nw = 0.35\nax.bar(x - w/2, xu_v, w, label='Xu Xueli (2024)\\nEncoder + RF', color='#e74c3c', alpha=0.8, edgecolor='black')\nax.bar(x + w/2, our_v, w, label='Ours (Test Set)\\nMLP + CIR Features', color='#27ae60', alpha=0.8, edgecolor='black')\nfor bar, val in zip(ax.containers[0], xu_v):\n    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,\n            f'{val:.3f}', ha='center', fontsize=12, fontweight='bold', color='#c0392b')\nfor bar, val in zip(ax.containers[1], our_v):\n    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,\n            f'{val:.3f}', ha='center', fontsize=12, fontweight='bold', color='#1e8449')\nax.set_xticks(x)\nax.set_xticklabels(labels, fontsize=12)\nax.set_title('Bounce Distance: Ours vs Xu Xueli (2024)', fontsize=14, fontweight='bold')\nax.legend(fontsize=11)\nax.grid(True, alpha=0.3, axis='y')\n\nfig.suptitle('1-Bounce Distance Prediction — Unseen Test Set vs Prior Work',\n             fontsize=15, fontweight='bold', y=1.02)\nplt.tight_layout()\nplt.show()\n\nprint(f'\\nBounce Distance Benchmark (Unseen Test Set):')\nprint(f'  {\"Metric\":<8} | {\"Xu Xueli\":<12} | {\"Ours\":<12} | Improvement')\nprint(f'  {\"-\"*48}')\nprint(f'  {\"MAE\":<8} | {xu_mae:<11.3f}m | {our_db_mae:<11.3f}m | {xu_mae/our_db_mae:.1f}x lower')\nprint(f'  {\"RMSE\":<8} | {xu_rmse:<11.3f}m | {our_db_rmse:<11.3f}m | {xu_rmse/our_db_rmse:.1f}x lower')\nprint(f'  {\"R²\":<8} | {xu_r2:<12.3f} | {our_db_r2:<12.3f} | +{our_db_r2 - xu_r2:.3f}')"
  },
  {
   "cell_type": "markdown",
   "id": "tp-save-md",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 7: Save Production Models\n",
    "\n",
    "Save the models trained on the 70% train split for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "tp-save",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved production models:\n",
      "  prod_stage1_pi_hlnn.pt\n",
      "  prod_stage2_bounce.pt\n",
      "  prod_stage3_bias.pt\n",
      "  prod_pipeline_config.pt\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# SAVE PRODUCTION MODELS\n",
    "# ==========================================\n",
    "torch.save(model_s1.state_dict(), 'prod_stage1_pi_hlnn.pt')\n",
    "torch.save(model_s2.state_dict(), 'prod_stage2_bounce.pt')\n",
    "torch.save(model_s3.state_dict(), 'prod_stage3_bias.pt')\n",
    "\n",
    "torch.save({\n",
    "    'stage1_config': S1_CONFIG,\n",
    "    'stage2_config': S2_CONFIG,\n",
    "    'stage3_config': S3_CONFIG,\n",
    "    'scaler_s2_mean': scaler_s2.mean_,\n",
    "    'scaler_s2_scale': scaler_s2.scale_,\n",
    "    'scaler_s3_mean': scaler_s3.mean_,\n",
    "    'scaler_s3_scale': scaler_s3.scale_,\n",
    "    'feature_names': FEATURE_NAMES,\n",
    "    'ground_truth': GROUND_TRUTH,\n",
    "    'measured_nlos_bias': MEASURED_NLOS_BIAS,\n",
    "    'peak_config': PEAK_CONFIG,\n",
    "}, 'prod_pipeline_config.pt')\n",
    "\n",
    "print('Saved production models:')\n",
    "print('  prod_stage1_pi_hlnn.pt')\n",
    "print('  prod_stage2_bounce.pt')\n",
    "print('  prod_stage3_bias.pt')\n",
    "print('  prod_pipeline_config.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "tp-summary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "MULTI-MODEL PIPELINE — FINAL RESULTS\n",
      "======================================================================\n",
      "\n",
      "Data Split: 70/15/15 (Train=2520, Val=540, Test=540)\n",
      "\n",
      "Pipeline Architecture:\n",
      "  Stage 1: PI-HLNN (Liquid Neural Network)  — 10,625 params\n",
      "  Stage 2: MLP Bounce Classifier             — 769 params\n",
      "  Stage 3: MLP NLOS Bias Predictor            — 2,561 params\n",
      "  Total pipeline:                             — 13,955 params\n",
      "\n",
      "Results on Unseen Test Set (540 samples):\n",
      "  Stage 1 (LOS/NLOS):      99.63% accuracy\n",
      "  Stage 2 (Bounce type):   98.51% accuracy\n",
      "  Stage 3 (Bias):          MAE = 0.1781m\n",
      "  Distance correction:     MAE = 4.0628m\n",
      "\n",
      "Benchmark vs Xu Xueli (2024):\n",
      "  Bounce distance MAE: 0.178m (ours) vs 0.346m (Xu Xueli)\n",
      "  Bounce distance R²:  0.918 (ours) vs 0.619 (Xu Xueli)\n",
      "\n",
      "Pipeline flow on test set:\n",
      "  540 total\n",
      "  → 268 classified as NLOS (Stage 1)\n",
      "  → 126 classified as single-bounce (Stage 2)\n",
      "  → 126 bias predicted + distance corrected (Stage 3)\n",
      "\n",
      "At inference: d_corrected = d_UWB - predicted_bias\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# FINAL SUMMARY\n",
    "# ==========================================\n",
    "print('='*70)\n",
    "print('MULTI-MODEL PIPELINE — FINAL RESULTS')\n",
    "print('='*70)\n",
    "print(f'\\nData Split: 70/15/15 (Train={len(train_df)}, Val={len(val_df)}, Test={len(test_df)})')\n",
    "print(f'\\nPipeline Architecture:')\n",
    "print(f'  Stage 1: PI-HLNN (Liquid Neural Network)  — {sum(p.numel() for p in model_s1.parameters()):,} params')\n",
    "print(f'  Stage 2: MLP Bounce Classifier             — {sum(p.numel() for p in model_s2.parameters()):,} params')\n",
    "print(f'  Stage 3: MLP NLOS Bias Predictor            — {sum(p.numel() for p in model_s3.parameters()):,} params')\n",
    "total_params = sum(p.numel() for p in model_s1.parameters()) + \\\n",
    "               sum(p.numel() for p in model_s2.parameters()) + \\\n",
    "               sum(p.numel() for p in model_s3.parameters())\n",
    "print(f'  Total pipeline:                             — {total_params:,} params')\n",
    "print(f'\\nResults on Unseen Test Set ({len(test_df)} samples):')\n",
    "print(f'  Stage 1 (LOS/NLOS):      {100*s1_acc:.2f}% accuracy')\n",
    "print(f'  Stage 2 (Bounce type):   {100*s2_acc:.2f}% accuracy')\n",
    "print(f'  Stage 3 (Bias):          MAE = {test_mae:.4f}m')\n",
    "print(f'  Distance correction:     MAE = {results_df[\"correction_error\"].mean():.4f}m')\n",
    "print(f'\\nBenchmark vs Xu Xueli (2024):')\n",
    "print(f'  Bounce distance MAE: {our_db_mae:.3f}m (ours) vs {xu_mae:.3f}m (Xu Xueli)')\n",
    "print(f'  Bounce distance R²:  {our_db_r2:.3f} (ours) vs {xu_r2:.3f} (Xu Xueli)')\n",
    "print(f'\\nPipeline flow on test set:')\n",
    "print(f'  {len(test_df)} total')\n",
    "print(f'  → {int(nlos_mask_pred.sum())} classified as NLOS (Stage 1)')\n",
    "print(f'  → {int(single_mask_pred.sum())} classified as single-bounce (Stage 2)')\n",
    "print(f'  → {len(results_df)} bias predicted + distance corrected (Stage 3)')\n",
    "print(f'\\nAt inference: d_corrected = d_UWB - predicted_bias')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
