{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ca5440",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# ==========================================\n",
    "# 1. ROBUST DATA LOADING (RX_PACC + INSTANCE NORM + HARDWARE RISE TIME)\n",
    "# ==========================================\n",
    "PRE_CROP = 10\n",
    "POST_CROP = 50\n",
    "TOTAL_LEN = 60\n",
    "\n",
    "def load_optimized_dataset(filepath=\"combined_uwb_dataset.csv\"):\n",
    "    processed_seqs, context_features, labels = [], [], []\n",
    "    print(f\"Reading dataset: {filepath}...\")\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(filepath)\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: File not found. Please check the filepath.\")\n",
    "        return None, None, None\n",
    "\n",
    "    cir_cols = sorted([c for c in df.columns if c.startswith('CIR')], key=lambda x: int(x.replace('CIR', '')))\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        # 1. Parse Raw Signal\n",
    "        sig = pd.to_numeric(row[cir_cols], errors='coerce').fillna(0).astype(float).values\n",
    "        \n",
    "        # 2. PHYSICS NORMALIZATION: Divide by RX_PACC\n",
    "        if 'RX_PACC' in row and row['RX_PACC'] > 0:\n",
    "            sig = sig / float(row['RX_PACC'])\n",
    "        \n",
    "        # 3. Dynamic Crop (Center on First Path)\n",
    "        fp_idx = int(row['FP_INDEX']) if 'FP_INDEX' in row and not pd.isna(row['FP_INDEX']) else np.argmax(sig)\n",
    "        center = fp_idx if fp_idx > PRE_CROP else np.argmax(sig)\n",
    "        start, end = int(center - PRE_CROP), int(center + POST_CROP)\n",
    "        \n",
    "        crop = sig[start:end] if start >= 0 and end <= len(sig) else np.zeros(TOTAL_LEN)\n",
    "        if len(crop) != TOTAL_LEN: crop = np.resize(crop, TOTAL_LEN)\n",
    "\n",
    "        # 4. INSTANCE NORMALIZATION (Crucial for LNN)\n",
    "        local_min = np.min(crop)\n",
    "        local_max = np.max(crop)\n",
    "        range_val = local_max - local_min\n",
    "        \n",
    "        if range_val > 0:\n",
    "            crop = (crop - local_min) / range_val\n",
    "        else:\n",
    "            crop = np.zeros_like(crop)\n",
    "            \n",
    "        processed_seqs.append(crop)\n",
    "        \n",
    "        # 5. Extract Physical Context Features\n",
    "        c_max = np.max(sig) \n",
    "        total_energy = np.sum(sig**2) + 1e-9\n",
    "        peak_energy = np.sum(sig[max(0, fp_idx-2):min(len(sig), fp_idx+6)]**2)\n",
    "        c_pow_ratio = peak_energy / total_energy\n",
    "        \n",
    "        # Feature D: Hardware-Style Rise Time (Threshold based)\n",
    "        # Calculates distance from 10% threshold crossing to the Peak\n",
    "        try:\n",
    "            peak_idx = np.argmax(sig)\n",
    "            peak_val = sig[peak_idx]\n",
    "            threshold = 0.1 * peak_val\n",
    "            \n",
    "            # Look at signal BEFORE the peak\n",
    "            pre_peak = sig[:peak_idx]\n",
    "            \n",
    "            # Find where it first crosses the threshold\n",
    "            start_indices = np.where(pre_peak >= threshold)[0]\n",
    "            \n",
    "            if len(start_indices) > 0:\n",
    "                rise_time = peak_idx - start_indices[0]\n",
    "            else:\n",
    "                rise_time = 0\n",
    "        except:\n",
    "            rise_time = 0 \n",
    "        \n",
    "        context_features.append([c_max, total_energy, c_pow_ratio, rise_time])\n",
    "        labels.append(float(row['Label']))\n",
    "    \n",
    "    return np.array(processed_seqs).reshape(-1, TOTAL_LEN, 1), np.array(context_features), np.array(labels)\n",
    "\n",
    "# ==========================================\n",
    "# 2. PURE LIQUID NEURAL NETWORK\n",
    "# ==========================================\n",
    "class LiquidCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, context_size):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.synapse = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.A = nn.Parameter(torch.ones(hidden_size) * -0.5)\n",
    "        \n",
    "        self.tau_controller = nn.Sequential(\n",
    "            nn.Linear(context_size, 16),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(16, hidden_size),\n",
    "            nn.Sigmoid() \n",
    "        )\n",
    "        \n",
    "    def forward(self, x_t, h_prev, context, dt=1.0):\n",
    "        tau_gate = self.tau_controller(context)\n",
    "        tau = 1.0 + (4.0 * tau_gate) \n",
    "        \n",
    "        combined = torch.cat((x_t, h_prev), dim=1)\n",
    "        S_t = torch.tanh(self.synapse(combined))\n",
    "        \n",
    "        numerator = h_prev + (dt * S_t * self.A)\n",
    "        denominator = 1.0 + (dt / tau)\n",
    "        \n",
    "        h_new = numerator / denominator\n",
    "        return h_new, tau\n",
    "\n",
    "class PureLNN(nn.Module):\n",
    "    def __init__(self, context_size=3):\n",
    "        super().__init__()\n",
    "        self.hidden_size = 64\n",
    "        self.cell = LiquidCell(1, self.hidden_size, context_size)\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.hidden_size, 32),\n",
    "            nn.SiLU(), \n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(32, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x_seq, x_ctx, return_dynamics=False):\n",
    "        batch_size, seq_len, _ = x_seq.size()\n",
    "        h_t = torch.zeros(batch_size, self.hidden_size).to(x_seq.device)\n",
    "        \n",
    "        h_hist = [] \n",
    "        h_sum = torch.zeros_like(h_t) \n",
    "        tau_mean = 0\n",
    "        \n",
    "        for t in range(seq_len):\n",
    "            h_t, tau = self.cell(x_seq[:, t, :], h_t, x_ctx)\n",
    "            h_sum += h_t\n",
    "            \n",
    "            if return_dynamics: \n",
    "                h_hist.append(h_t.unsqueeze(1))\n",
    "                if isinstance(tau, torch.Tensor): tau_mean = tau \n",
    "        \n",
    "        h_pooled = h_sum / seq_len\n",
    "        out = self.classifier(h_pooled)\n",
    "        \n",
    "        if return_dynamics:\n",
    "            return out, torch.cat(h_hist, dim=1), tau_mean\n",
    "        return out\n",
    "\n",
    "# ==========================================\n",
    "# 3. ENHANCED DIAGNOSTICS (2x2 Grid)\n",
    "# ==========================================\n",
    "def plot_full_diagnostics(model, X_seq, X_ctx, y_true, train_hist, val_hist):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_probs, h_hist, taus = model(X_seq, X_ctx, return_dynamics=True)\n",
    "    \n",
    "    # 1. Prepare Data\n",
    "    h_hist_np = h_hist.cpu().numpy()\n",
    "    if taus.dim() > 1: taus_np = taus.cpu().numpy().mean(axis=1)\n",
    "    else: taus_np = taus.cpu().numpy()\n",
    "    \n",
    "    y_true_np = y_true.cpu().numpy().flatten()\n",
    "    y_pred_np = (y_probs.cpu().numpy().flatten() > 0.5).astype(float)\n",
    "    batch_size, seq_len, hidden_dim = h_hist_np.shape\n",
    "\n",
    "    # 2. Setup 2x2 Plot\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(18, 12))\n",
    "    plt.subplots_adjust(hspace=0.3)\n",
    "\n",
    "    # --- PLOT 1: Learning Curves ---\n",
    "    axs[0, 0].plot(train_hist, label='Train Loss', color='#3498db', linewidth=2)\n",
    "    axs[0, 0].plot(val_hist, label='Val Loss', color='#e74c3c', linewidth=2, linestyle='--')\n",
    "    axs[0, 0].set_title(\"Learning Dynamics (Loss Curve)\")\n",
    "    axs[0, 0].set_xlabel(\"Epochs\")\n",
    "    axs[0, 0].set_ylabel(\"Binary Cross Entropy\")\n",
    "    axs[0, 0].grid(True, alpha=0.3)\n",
    "    axs[0, 0].legend()\n",
    "\n",
    "    # --- PLOT 2: Confusion Matrix ---\n",
    "    cm = confusion_matrix(y_true_np, y_pred_np)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['LOS', 'NLOS'])\n",
    "    disp.plot(ax=axs[0, 1], cmap='Blues', colorbar=False)\n",
    "    axs[0, 1].set_title(f\"Confusion Matrix (Acc: {100*(y_true_np==y_pred_np).mean():.1f}%)\")\n",
    "\n",
    "    # --- PLOT 3: Phase Space (PCA) ---\n",
    "    # Select subset for clarity\n",
    "    los_indices = np.where(y_true_np == 0)[0]\n",
    "    nlos_indices = np.where(y_true_np == 1)[0]\n",
    "    num_samples = min(len(los_indices), len(nlos_indices), 25)\n",
    "    selected_indices = np.concatenate([los_indices[:num_samples], nlos_indices[:num_samples]])\n",
    "\n",
    "    h_flat = h_hist_np.reshape(-1, hidden_dim)\n",
    "    pca = PCA(n_components=2)\n",
    "    h_pca = pca.fit_transform(h_flat)\n",
    "    h_pca_traj = h_pca.reshape(batch_size, seq_len, 2)\n",
    "\n",
    "    for i in selected_indices:\n",
    "        color = '#2ecc71' if y_true_np[i] == 0 else '#e74c3c'\n",
    "        axs[1, 0].plot(h_pca_traj[i, :, 0], h_pca_traj[i, :, 1], color=color, alpha=0.3)\n",
    "    \n",
    "    axs[1, 0].set_title(f\"Liquid State Phase Space (n={num_samples*2})\")\n",
    "    axs[1, 0].set_xlabel(\"PC1\")\n",
    "    axs[1, 0].set_ylabel(\"PC2\")\n",
    "    axs[1, 0].grid(True, alpha=0.2)\n",
    "\n",
    "    # --- PLOT 4: Tau Distribution ---\n",
    "    sns.kdeplot(taus_np[y_true_np==0], ax=axs[1, 1], fill=True, color=\"#2ecc71\", label=\"LOS Tau\")\n",
    "    sns.kdeplot(taus_np[y_true_np==1], ax=axs[1, 1], fill=True, color=\"#e74c3c\", label=\"NLOS Tau\")\n",
    "    axs[1, 1].set_title(\"Learned Time Constants (Tau)\")\n",
    "    axs[1, 1].set_xlabel(\"Tau (Time Steps)\")\n",
    "    axs[1, 1].legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# ==========================================\n",
    "# 4. MAIN EXECUTION\n",
    "# ==========================================\n",
    "def main():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # 1. Load Data\n",
    "    X_seq_all, X_ctx_all, y_all = load_optimized_dataset(\"combined_uwb_dataset.csv\")\n",
    "    if X_seq_all is None: return \n",
    "\n",
    "    # Scale Context Features\n",
    "    scaler = StandardScaler()\n",
    "    X_ctx_all = scaler.fit_transform(X_ctx_all)\n",
    "    num_context_features = X_ctx_all.shape[1]\n",
    "    print(f\"Context Features: {num_context_features} (Ampl, Energy, Ratio, HW_RiseTime)\")\n",
    "\n",
    "    # 2. Cross Validation\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    fold_accuracies = []\n",
    "    \n",
    "    # Storage for the last fold's history for plotting\n",
    "    final_train_hist = []\n",
    "    final_val_hist = []\n",
    "    final_model = None\n",
    "    final_val_data = None\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(X_seq_all, y_all)):\n",
    "        print(f\"\\n--- FOLD {fold+1} ---\")\n",
    "        \n",
    "        # Prepare Tensors\n",
    "        X_seq_tr = torch.tensor(X_seq_all[train_idx].astype(np.float32)).to(device)\n",
    "        X_ctx_tr = torch.tensor(X_ctx_all[train_idx].astype(np.float32)).to(device)\n",
    "        y_tr = torch.tensor(y_all[train_idx].astype(np.float32)).unsqueeze(1).to(device)\n",
    "        \n",
    "        X_seq_va = torch.tensor(X_seq_all[val_idx].astype(np.float32)).to(device)\n",
    "        X_ctx_va = torch.tensor(X_ctx_all[val_idx].astype(np.float32)).to(device)\n",
    "        y_va = torch.tensor(y_all[val_idx].astype(np.float32)).unsqueeze(1).to(device)\n",
    "\n",
    "        train_ds = TensorDataset(X_seq_tr, X_ctx_tr, y_tr)\n",
    "        train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "\n",
    "        model = PureLNN(context_size=num_context_features).to(device)\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=0.005, weight_decay=1e-4)\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=5)\n",
    "        \n",
    "        # Lists to store loss per epoch\n",
    "        epoch_train_losses = []\n",
    "        epoch_val_losses = []\n",
    "\n",
    "        best_val_acc = 0\n",
    "        \n",
    "        for epoch in range(40):\n",
    "            model.train()\n",
    "            train_loss_accum = 0\n",
    "            \n",
    "            for batch_x, batch_ctx, batch_y in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                preds = model(batch_x, batch_ctx)\n",
    "                loss = nn.BCELoss()(preds, batch_y)\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "                train_loss_accum += loss.item()\n",
    "\n",
    "            # Record Train Loss\n",
    "            avg_train_loss = train_loss_accum / len(train_loader)\n",
    "            epoch_train_losses.append(avg_train_loss)\n",
    "\n",
    "            # Validation Step\n",
    "            model.eval()\n",
    "            val_loss_accum = 0\n",
    "            with torch.no_grad():\n",
    "                val_preds = model(X_seq_va, X_ctx_va)\n",
    "                val_loss = nn.BCELoss()(val_preds, y_va).item()\n",
    "                val_loss_accum = val_loss # Since full batch val here\n",
    "                val_acc = ((val_preds > 0.5).float() == y_va).float().mean().item()\n",
    "            \n",
    "            # Record Val Loss\n",
    "            epoch_val_losses.append(val_loss_accum)\n",
    "            \n",
    "            scheduler.step(val_acc)\n",
    "            \n",
    "            if epoch % 10 == 0:\n",
    "                print(f\"Ep {epoch:<3} | Loss: {avg_train_loss:.4f} | Val Acc: {100*val_acc:.2f}%\")\n",
    "            \n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "\n",
    "        fold_accuracies.append(best_val_acc)\n",
    "        print(f\"Fold {fold+1} Best Accuracy: {100*best_val_acc:.2f}%\")\n",
    "        \n",
    "        # Save data from the last fold for visualization\n",
    "        final_train_hist = epoch_train_losses\n",
    "        final_val_hist = epoch_val_losses\n",
    "        final_model = model\n",
    "        final_val_data = (X_seq_va, X_ctx_va, y_va)\n",
    "\n",
    "    print(f\"\\nüèÜ Final Mean Accuracy: {100*np.mean(fold_accuracies):.2f}%\")\n",
    "    \n",
    "    # 3. Final Diagnostics\n",
    "    print(\"\\nüìä Generating Complete Diagnostics (Loss, Confusion Matrix, PCA, Tau)...\")\n",
    "    if final_model:\n",
    "        plot_full_diagnostics(final_model, *final_val_data, final_train_hist, final_val_hist)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
