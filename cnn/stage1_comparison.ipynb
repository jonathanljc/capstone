{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Stage 1 Comparison: Single-Channel vs Multi-Channel Dataset\n",
    "## 1D-CNN Baseline — Justification for Multi-Channel Data Collection\n",
    "\n",
    "Trains the same **CNN_Classifier** on two 600-sample datasets under **equal sample conditions**:\n",
    "- **Single-Channel**: channel 5 only — 100 samples per scenario\n",
    "- **Multi-Channel**: 4 channels with distinct center frequencies (c1, c3, c4, c7) — **25 samples per channel per scenario**\n",
    "\n",
    "Channels selected to maximise diversity across both center frequency and bandwidth:\n",
    "\n",
    "| Channel | Center Freq | Bandwidth | Character |\n",
    "|---|---|---|---|\n",
    "| c1 | 3494.4 MHz | 499.2 MHz | Low-freq, narrowband |\n",
    "| c3 | 4492.8 MHz | 499.2 MHz | Mid-freq, narrowband |\n",
    "| c4 | 3993.6 MHz | 1331.2 MHz | Mid-freq, **wideband** |\n",
    "| c7 | 6489.6 MHz | 1081.6 MHz | High-freq, **wideband** |\n",
    "\n",
    "All four channels have **distinct center frequencies**, spanning 3.5–6.5 GHz with bandwidths ranging from 499 MHz to 1331 MHz.\n",
    "\n",
    "Three levels of evaluation:\n",
    "1. Single 70/15/15 split\n",
    "2. Stratified 5-Fold CV (mean ± std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'Python 3.13.7' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "CONFIG = {\n",
    "    \"pre_crop\": 10, \"post_crop\": 50, \"total_len\": 60,\n",
    "    \"search_start\": 740, \"search_end\": 890,\n",
    "    \"embedding_size\": 128, \"input_channels\": 1, \"dropout\": 0.4,\n",
    "    \"batch_size\": 64, \"max_epochs\": 50, \"lr\": 1e-3,\n",
    "    \"weight_decay\": 1e-4, \"warmup_epochs\": 3, \"patience\": 40,\n",
    "    \"grad_clip\": 1.0, \"val_ratio\": 0.15, \"test_ratio\": 0.15, \"seed\": 42,\n",
    "}\n",
    "DATA_DIR = \"../dataset/channels/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy, math, contextlib, io\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, ConfusionMatrixDisplay,\n",
    "    classification_report, roc_curve, auc\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if device.type == \"cuda\":\n",
    "    torch.cuda.empty_cache()\n",
    "print(f\"Device: {device}\")\n",
    "torch.manual_seed(CONFIG[\"seed\"])\n",
    "np.random.seed(CONFIG[\"seed\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 1: Data Loading & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_roi_alignment(sig,\n",
    "                      search_start=CONFIG['search_start'],\n",
    "                      search_end=CONFIG['search_end']):\n",
    "    region = sig[search_start:search_end]\n",
    "    if len(region) == 0:\n",
    "        return np.argmax(sig)\n",
    "    peak_idx = search_start + np.argmax(region)\n",
    "    peak_val = sig[peak_idx]\n",
    "    noise_section = sig[:search_start]\n",
    "    if len(noise_section) > 10:\n",
    "        threshold = max(np.mean(noise_section) + 3*np.std(noise_section), 0.05*peak_val)\n",
    "    else:\n",
    "        threshold = 0.05 * peak_val\n",
    "    leading_edge = peak_idx\n",
    "    for i in range(peak_idx, max(search_start - 20, 0), -1):\n",
    "        if sig[i] < threshold:\n",
    "            leading_edge = i + 1\n",
    "            break\n",
    "    return leading_edge\n",
    "\n",
    "\n",
    "def _process_rows(df):\n",
    "    \"\"\"CIR preprocessing — outputs (N, 1, 60) channels-first for Conv1d.\"\"\"\n",
    "    PRE = CONFIG['pre_crop']; TOTAL = CONFIG['total_len']\n",
    "    cir_cols = sorted([c for c in df.columns if c.startswith('CIR')],\n",
    "                      key=lambda x: int(x.replace('CIR', '')))\n",
    "    seqs, labels = [], []\n",
    "    for _, row in df.iterrows():\n",
    "        sig = pd.to_numeric(row[cir_cols], errors='coerce').fillna(0).astype(float).values\n",
    "        rxpacc = float(row.get('RXPACC', 128.0))\n",
    "        if rxpacc > 0:\n",
    "            sig = sig / rxpacc\n",
    "        le = get_roi_alignment(sig)\n",
    "        start = max(0, le - PRE); end = start + TOTAL\n",
    "        if end > len(sig): end = len(sig); start = max(0, end - TOTAL)\n",
    "        crop = sig[start:end]\n",
    "        if len(crop) < TOTAL:\n",
    "            crop = np.pad(crop, (0, TOTAL - len(crop)), mode='constant')\n",
    "        lo, hi = crop.min(), crop.max()\n",
    "        crop = (crop - lo) / (hi - lo) if hi > lo else np.zeros(TOTAL)\n",
    "        seqs.append(crop); labels.append(float(row['Label']))\n",
    "    # CNN channels-first: (N, 1, 60)\n",
    "    X = np.array(seqs).reshape(-1, 1, TOTAL).astype(np.float32)\n",
    "    y = np.array(labels).astype(np.float32)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def load_cir_dataset(filepath):\n",
    "    print(f'Loading: {filepath}')\n",
    "    df = pd.read_csv(filepath)\n",
    "    print(f'  Samples: {len(df)}')\n",
    "    X, y = _process_rows(df)\n",
    "    print(f'  Output: X={X.shape}, y={y.shape} | LOS={int((y==0).sum())}, NLOS={int((y==1).sum())}')\n",
    "    return X, y\n",
    "\n",
    "print(\"Data loading functions ready (CIR only — channels-first for CNN).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 2: CNN_Classifier Architecture\n",
    "1D-CNN with 3 conv blocks + Global Average Pooling — no recurrence, no ODE dynamics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Classifier(nn.Module):\n",
    "    \"\"\"1D-CNN baseline with GAP. No recurrence, no FP_AMPL conditioning.\"\"\"\n",
    "    def __init__(self, input_channels=1, embedding_size=128, dropout=0.4):\n",
    "        super().__init__()\n",
    "        self.embedding_size = embedding_size\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv1d(input_channels, 16, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm1d(16), nn.ReLU(),\n",
    "            nn.Conv1d(16, 32, kernel_size=5, padding=2, stride=2),\n",
    "            nn.BatchNorm1d(32), nn.ReLU(),\n",
    "            nn.Conv1d(32, embedding_size, kernel_size=3, padding=1, stride=2),\n",
    "            nn.BatchNorm1d(embedding_size), nn.ReLU(),\n",
    "        )\n",
    "        self.gap = nn.AdaptiveAvgPool1d(1)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(embedding_size, 32), nn.SiLU(),\n",
    "            nn.Dropout(dropout), nn.Linear(32, 1), nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, 1, 60)\n",
    "        features = self.encoder(x)\n",
    "        embedding = self.gap(features).squeeze(-1)\n",
    "        return self.classifier(embedding)\n",
    "\n",
    "_m = CNN_Classifier()\n",
    "print(f'CNN_Classifier | params: {sum(p.numel() for p in _m.parameters()):,} | embed_dim=128')\n",
    "print(f'  Conv blocks: 1->16->32->128, kernel 5/5/3, stride 1/2/2')\n",
    "print(f'  Pooling: Global Average Pooling')\n",
    "del _m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 3: Training Function\n",
    "Early stopping (patience=40), best-model restoration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X_train, y_train, X_val, y_val,\n",
    "                config=CONFIG, verbose=True, seed=None):\n",
    "    _seed = seed if seed is not None else config['seed']\n",
    "    torch.manual_seed(_seed)\n",
    "    np.random.seed(_seed)\n",
    "\n",
    "    X_tr = torch.tensor(X_train).to(device)\n",
    "    y_tr = torch.tensor(y_train).unsqueeze(1).to(device)\n",
    "    X_va = torch.tensor(X_val).to(device)\n",
    "    y_va = torch.tensor(y_val).unsqueeze(1).to(device)\n",
    "\n",
    "    loader = DataLoader(TensorDataset(X_tr, y_tr),\n",
    "                        batch_size=config['batch_size'], shuffle=True)\n",
    "\n",
    "    model = CNN_Classifier(\n",
    "        input_channels=config['input_channels'],\n",
    "        embedding_size=config['embedding_size'],\n",
    "        dropout=config['dropout']\n",
    "    ).to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=config['lr'],\n",
    "                            weight_decay=config['weight_decay'])\n",
    "    T = config['max_epochs']; W = config['warmup_epochs']\n",
    "    def lr_lambda(e):\n",
    "        if e < W: return (e+1)/W\n",
    "        return max(0.01, 0.5*(1+math.cos(math.pi*(e-W)/max(1,T-W))))\n",
    "    scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "    history = {'train_loss':[], 'val_loss':[], 'train_acc':[], 'val_acc':[]}\n",
    "    best_val_acc = 0; best_state = None; patience_counter = 0\n",
    "\n",
    "    for epoch in range(T):\n",
    "        model.train(); tl, tc, tt = 0, 0, 0\n",
    "        for bx, by in loader:\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(bx)\n",
    "            loss = criterion(pred, by); loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), config['grad_clip'])\n",
    "            optimizer.step()\n",
    "            tl += loss.item()*len(bx); tc += ((pred>0.5).float()==by).sum().item(); tt += len(bx)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            vp = model(X_va)\n",
    "            vl = criterion(vp, y_va).item()\n",
    "            va = ((vp>0.5).float()==y_va).float().mean().item()\n",
    "        history['train_loss'].append(tl/tt); history['val_loss'].append(vl)\n",
    "        history['train_acc'].append(tc/tt);  history['val_acc'].append(va)\n",
    "        scheduler.step()\n",
    "\n",
    "        if va > best_val_acc:\n",
    "            best_val_acc = va; best_state = copy.deepcopy(model.state_dict()); patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if verbose and (epoch%10==0 or epoch==T-1):\n",
    "            print(f'  Ep {epoch:>3} | Loss: {tl/tt:.4f} | Val Acc: {100*va:.2f}% | Best: {100*best_val_acc:.2f}%')\n",
    "        if patience_counter >= config.get('patience', T):\n",
    "            if verbose: print(f'  Early stopping at epoch {epoch}')\n",
    "            break\n",
    "\n",
    "    model.load_state_dict(best_state)\n",
    "    if verbose: print(f'  Best Val Acc: {100*best_val_acc:.2f}%')\n",
    "    return model, history\n",
    "\n",
    "print('train_model() ready — 1D-CNN, patience=40.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, X, y_true):\n",
    "    \"\"\"Returns metrics dict for a given CNN model and data.\"\"\"\n",
    "    model.eval()\n",
    "    X_t = torch.tensor(X).to(device)\n",
    "    with torch.no_grad():\n",
    "        pred = model(X_t)\n",
    "    y_prob = pred.cpu().numpy().flatten()\n",
    "    y_pred = (y_prob > 0.5).astype(float)\n",
    "    y_true = y_true.flatten()\n",
    "    rep  = classification_report(y_true, y_pred, target_names=['LOS','NLOS'],\n",
    "                                 output_dict=True, zero_division=0)\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
    "    return {\n",
    "        'acc':      rep['accuracy'],\n",
    "        'f1_macro': rep['macro avg']['f1-score'],\n",
    "        'f1_los':   rep['LOS']['f1-score'],\n",
    "        'f1_nlos':  rep['NLOS']['f1-score'],\n",
    "        'auc':      auc(fpr, tpr),\n",
    "        'fpr': fpr, 'tpr': tpr,\n",
    "        'cm':  confusion_matrix(y_true, y_pred),\n",
    "    }\n",
    "\n",
    "print('evaluate() helper ready (1D-CNN).')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 4: Single 70/15/15 Split Comparison\n",
    "Quick baseline comparison using one fixed split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(csv_name, label, config=CONFIG, seed=42):\n",
    "    print(f\"\\n{'='*60}\\nExperiment: {label}\\n{'='*60}\")\n",
    "    X_all, y_all = load_cir_dataset(DATA_DIR + csv_name)\n",
    "    X_tr, X_tmp, y_tr, y_tmp = train_test_split(\n",
    "        X_all, y_all, test_size=config['val_ratio']+config['test_ratio'],\n",
    "        stratify=y_all, random_state=seed)\n",
    "    X_va, X_te, y_va, y_te = train_test_split(\n",
    "        X_tmp, y_tmp,\n",
    "        test_size=config['test_ratio']/(config['val_ratio']+config['test_ratio']),\n",
    "        stratify=y_tmp, random_state=seed)\n",
    "    print(f'  Train={len(X_tr)}, Val={len(X_va)}, Test={len(X_te)}')\n",
    "    model, history = train_model(X_tr, y_tr, X_va, y_va, config=config)\n",
    "    m = evaluate(model, X_te, y_te)\n",
    "    m['label'] = label; m['history'] = history\n",
    "    print(f\"\\n  Test Acc={m['acc']:.4f} | Macro F1={m['f1_macro']:.4f} | AUC={m['auc']:.4f}\")\n",
    "    return m\n",
    "\n",
    "\n",
    "results_single = run_experiment('single_channel5_dataset.csv',  'Single-Channel (c5 only)')\n",
    "results_multi  = run_experiment('multi_channel4_dataset.csv',   'Multi-Channel (c1,c3,c4,c7)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Single-split plots ──────────────────────────────────────────────────\n",
    "_res = [results_single, results_multi]\n",
    "_colors = ['#3498db', '#e74c3c']\n",
    "width = 0.3\n",
    "\n",
    "fig, axs = plt.subplots(1, 4, figsize=(24, 6))\n",
    "\n",
    "# Bar: key metrics\n",
    "ax = axs[0]\n",
    "mk = ['acc','f1_macro','auc']; mn = ['Accuracy','Macro F1','AUC']\n",
    "for i, (r, c) in enumerate(zip(_res, _colors)):\n",
    "    vals = [r[k] for k in mk]\n",
    "    bars = ax.bar(np.arange(3)+i*width, vals, width, label=r['label'], color=c, alpha=0.85)\n",
    "    for bar, v in zip(bars, vals):\n",
    "        ax.text(bar.get_x()+bar.get_width()/2, bar.get_height()+0.005,\n",
    "                f'{v:.3f}', ha='center', va='bottom', fontsize=8, fontweight='bold')\n",
    "ax.set_xticks(np.arange(3)+width/2); ax.set_xticklabels(mn)\n",
    "ax.set_ylim([0.4,1.12]); ax.set_title('Metric Comparison', fontweight='bold')\n",
    "ax.legend(fontsize=8); ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# ROC\n",
    "ax = axs[1]\n",
    "for r, c in zip(_res, _colors):\n",
    "    ax.plot(r['fpr'], r['tpr'], color=c, lw=2, label=f\"{r['label']} (AUC={r['auc']:.3f})\")\n",
    "ax.plot([0,1],[0,1],'k--',lw=1,alpha=0.5)\n",
    "ax.set_title('ROC Curves', fontweight='bold'); ax.set_xlabel('FPR'); ax.set_ylabel('TPR')\n",
    "ax.legend(fontsize=8); ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Val acc curves\n",
    "ax = axs[2]\n",
    "for r, c in zip(_res, _colors):\n",
    "    ax.plot(r['history']['val_acc'], color=c, lw=2, label=r['label'])\n",
    "ax.set_title('Val Accuracy per Epoch', fontweight='bold')\n",
    "ax.set_xlabel('Epoch'); ax.set_ylabel('Val Accuracy')\n",
    "ax.set_ylim([0.4,1.05]); ax.legend(fontsize=8); ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Per-class F1\n",
    "ax = axs[3]\n",
    "for i, (r, c) in enumerate(zip(_res, _colors)):\n",
    "    vals = [r['f1_los'], r['f1_nlos']]\n",
    "    bars = ax.bar(np.arange(2)+i*width, vals, width, label=r['label'], color=c, alpha=0.85)\n",
    "    for bar, v in zip(bars, vals):\n",
    "        ax.text(bar.get_x()+bar.get_width()/2, bar.get_height()+0.005,\n",
    "                f'{v:.3f}', ha='center', va='bottom', fontsize=8, fontweight='bold')\n",
    "ax.set_xticks(np.arange(2)+width/2); ax.set_xticklabels(['LOS F1','NLOS F1'])\n",
    "ax.set_ylim([0.4,1.12]); ax.set_title('Per-Class F1', fontweight='bold')\n",
    "ax.legend(fontsize=8); ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.suptitle('1D-CNN Baseline — Stage 1: Single-Channel vs Multi-Channel — Single Split',\n",
    "             fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "print('\\n' + '='*55)\n",
    "print(f\"{'Metric':<20} {'Single-Ch':>12} {'Multi-Ch':>12}  Delta\")\n",
    "print('='*55)\n",
    "for k, n in zip(['acc','f1_los','f1_nlos','f1_macro','auc'],\n",
    "                ['Accuracy','F1 LOS','F1 NLOS','Macro F1','AUC']):\n",
    "    sv=results_single[k]; mv=results_multi[k]; d=mv-sv\n",
    "    print(f\"{n:<20} {sv:>12.4f} {mv:>12.4f}   {'▲' if d>0 else '▼'}{abs(d):.4f}\")\n",
    "print('='*55)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 5: Stratified 5-Fold Cross-Validation\n",
    "More reliable estimate: averages over 5 different train/test splits.\n",
    "Epoch prints are suppressed — only one summary line per fold is shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_kfold(csv_name, label, n_splits=5, config=CONFIG, seed=42):\n",
    "    print(f\"\\n{'='*60}\\n5-Fold CV: {label}\\n{'='*60}\")\n",
    "    X_all, y_all = load_cir_dataset(DATA_DIR + csv_name)\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    fold_metrics = []\n",
    "    for fold, (tv_idx, te_idx) in enumerate(skf.split(X_all, y_all)):\n",
    "        X_tv, X_te = X_all[tv_idx], X_all[te_idx]\n",
    "        y_tv, y_te = y_all[tv_idx], y_all[te_idx]\n",
    "        X_tr, X_va, y_tr, y_va = train_test_split(\n",
    "            X_tv, y_tv, test_size=0.15, stratify=y_tv, random_state=seed)\n",
    "        fold_seed = seed + fold\n",
    "        with contextlib.redirect_stdout(io.StringIO()):\n",
    "            model, _ = train_model(X_tr, y_tr, X_va, y_va,\n",
    "                                   config=config, verbose=False, seed=fold_seed)\n",
    "        fm = evaluate(model, X_te, y_te)\n",
    "        fold_metrics.append(fm)\n",
    "        collapsed = \" [COLLAPSED]\" if fm['acc'] <= 0.51 else \"\"\n",
    "        print(f\"  Fold {fold+1}/{n_splits} | Acc={fm['acc']:.4f} | F1={fm['f1_macro']:.4f} | AUC={fm['auc']:.4f}{collapsed}\")\n",
    "    summary = {'label': label}\n",
    "    print(f\"\\n  {'─'*45}\")\n",
    "    print(f\"  {'Metric':<12} {'Mean':>8} {'Std':>8}\")\n",
    "    print(f\"  {'─'*45}\")\n",
    "    for key in ['acc','f1_macro','f1_los','f1_nlos','auc']:\n",
    "        vals = np.array([m[key] for m in fold_metrics])\n",
    "        summary[key] = {'mean': vals.mean(), 'std': vals.std(), 'all': vals.tolist()}\n",
    "        print(f\"  {key:<12} {vals.mean():>8.4f} {vals.std():>8.4f}\")\n",
    "    print(f\"  {'─'*45}\")\n",
    "    return summary\n",
    "\n",
    "\n",
    "kfold_single = run_kfold('single_channel5_dataset.csv', 'Single-Channel (c5 only)')\n",
    "kfold_multi  = run_kfold('multi_channel4_dataset.csv',  'Multi-Channel (c1,c3,c4,c7)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── K-Fold comparison plots ────────────────────────────────────────────\n",
    "_kf = [kfold_single, kfold_multi]\n",
    "_colors = ['#3498db', '#e74c3c']\n",
    "_mk = ['acc','f1_macro','f1_los','f1_nlos','auc']\n",
    "_mn = ['Accuracy','Macro F1','F1 LOS','F1 NLOS','AUC']\n",
    "width = 0.3\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(18, 6))\n",
    "\n",
    "# Bar with error bars\n",
    "ax = axs[0]\n",
    "x = np.arange(len(_mk))\n",
    "for i, (kf, c) in enumerate(zip(_kf, _colors)):\n",
    "    means = [kf[m]['mean'] for m in _mk]\n",
    "    stds  = [kf[m]['std']  for m in _mk]\n",
    "    bars = ax.bar(x+i*width, means, width, yerr=stds, label=kf['label'],\n",
    "                  color=c, alpha=0.85, capsize=4, error_kw={'elinewidth':1.5})\n",
    "    for bar, m, s in zip(bars, means, stds):\n",
    "        ax.text(bar.get_x()+bar.get_width()/2, bar.get_height()+s+0.008,\n",
    "                f'{m:.3f}', ha='center', va='bottom', fontsize=7.5, fontweight='bold')\n",
    "ax.set_xticks(x+width/2); ax.set_xticklabels(_mn, rotation=10)\n",
    "ax.set_ylim([0.4,1.15]); ax.set_title('5-Fold CV: Mean ± Std', fontweight='bold')\n",
    "ax.set_ylabel('Score'); ax.legend(fontsize=9); ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Box plots\n",
    "ax = axs[1]\n",
    "all_data, all_colors, positions = [], [], []\n",
    "for mi, metric in enumerate(_mk):\n",
    "    for ki, (kf, c) in enumerate(zip(_kf, _colors)):\n",
    "        positions.append(mi*3 + ki)\n",
    "        all_data.append(kf[metric]['all'])\n",
    "        all_colors.append(c)\n",
    "bp = ax.boxplot(all_data, positions=positions, widths=0.7, patch_artist=True,\n",
    "                medianprops={'color':'black','linewidth':2})\n",
    "for patch, c in zip(bp['boxes'], all_colors):\n",
    "    patch.set_facecolor(c); patch.set_alpha(0.7)\n",
    "ax.set_xticks([mi*3+0.5 for mi in range(len(_mk))])\n",
    "ax.set_xticklabels(_mn, rotation=10)\n",
    "ax.set_title('Score Distribution per Fold (blue=single, red=multi)', fontweight='bold')\n",
    "ax.set_ylabel('Score'); ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.suptitle('1D-CNN Baseline — Stage 1: Stratified 5-Fold CV — Single vs Multi Channel',\n",
    "             fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "print('\\n' + '='*65)\n",
    "print(f\"{'Metric':<12} {'Single Mean±Std':>20} {'Multi Mean±Std':>20}  Delta\")\n",
    "print('='*65)\n",
    "for k, n in zip(_mk, _mn):\n",
    "    sm,ss = kfold_single[k]['mean'], kfold_single[k]['std']\n",
    "    mm,ms = kfold_multi[k]['mean'],  kfold_multi[k]['std']\n",
    "    d = mm - sm\n",
    "    print(f\"{n:<12} {sm:>8.4f}±{ss:.4f}      {mm:>8.4f}±{ms:.4f}   {'▲' if d>0 else '▼'}{abs(d):.4f}\")\n",
    "print('='*65)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
