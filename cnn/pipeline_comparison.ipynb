{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Pipeline Comparison: Single-Channel vs Multi-Channel Dataset\n",
    "## 1D-CNN Baseline — Stages 1, 2, 3\n",
    "\n",
    "Trains the same **CNN_Classifier** pipeline on two 600-sample datasets under **equal sample conditions**:\n",
    "- **Single-Channel**: channel 5 only — 100 samples per scenario\n",
    "- **Multi-Channel**: 4 channels (c1, c3, c4, c7) — 25 samples per channel per scenario\n",
    "\n",
    "| Channel | Center Freq | Bandwidth | Character |\n",
    "|---|---|---|---|\n",
    "| c1 | 3494.4 MHz | 499.2 MHz | Low-freq, narrowband |\n",
    "| c3 | 4492.8 MHz | 499.2 MHz | Mid-freq, narrowband |\n",
    "| c4 | 3993.6 MHz | 1331.2 MHz | Mid-freq, **wideband** |\n",
    "| c7 | 6489.6 MHz | 1081.6 MHz | High-freq, **wideband** |\n",
    "\n",
    "Full 3-stage pipeline per dataset:\n",
    "```\n",
    "Raw CIR → Stage 1 (CNN: LOS/NLOS?) → Stage 2 (RF: Single/Multi bounce?) → Stage 3 (RF: Predict bias) → d_corrected\n",
    "```\n",
    "\n",
    "Two levels of evaluation:\n",
    "1. Single 70/15/15 split\n",
    "2. Stratified 5-Fold CV (mean ± std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    'pre_crop': 10, 'post_crop': 50, 'total_len': 60,\n",
    "    'search_start': 740, 'search_end': 890,\n",
    "    'embedding_size': 128, 'input_channels': 1, 'dropout': 0.4,\n",
    "    'batch_size': 64, 'max_epochs': 50, 'lr': 1e-3,\n",
    "    'weight_decay': 1e-4, 'warmup_epochs': 3, 'patience': 40,\n",
    "    'grad_clip': 1.0, 'val_ratio': 0.15, 'test_ratio': 0.15, 'seed': 42,\n",
    "}\n",
    "\n",
    "GROUND_TRUTH = {\n",
    "    '7.79m':  {'d_direct': 7.79,  'd_bounce': 12.79, 'bias': 5.00},\n",
    "    '10.77m': {'d_direct': 10.77, 'd_bounce': 16.09, 'bias': 5.32},\n",
    "    '14m':    {'d_direct': 14.00, 'd_bounce': 16.80, 'bias': 2.80},\n",
    "}\n",
    "MEASURED_NLOS_BIAS = {'7.79m': 5.00, '10.77m': 5.32, '14m': 2.80}\n",
    "\n",
    "PEAK_CONFIG = {\n",
    "    'peak_prominence': 0.20, 'peak_min_distance': 5,\n",
    "    'single_bounce_max_peaks': 2,\n",
    "    'search_start': 740, 'search_end': 890,\n",
    "}\n",
    "\n",
    "DATA_DIR = '../dataset/channels/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy, math, contextlib, io, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from scipy.signal import find_peaks\n",
    "from scipy.stats import kurtosis as scipy_kurtosis\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, ConfusionMatrixDisplay,\n",
    "    classification_report, roc_curve, auc,\n",
    "    mean_absolute_error, mean_squared_error, r2_score\n",
    ")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if device.type == 'cuda':\n",
    "    torch.cuda.empty_cache()\n",
    "print(f'Device: {device}')\n",
    "torch.manual_seed(CONFIG['seed'])\n",
    "np.random.seed(CONFIG['seed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 1: Data Loading, Preprocessing & Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distance_group(fname):\n",
    "    match = re.match(r'^([\\d.]+m)', str(fname))\n",
    "    return match.group(1) if match else None\n",
    "\n",
    "\n",
    "def get_roi_alignment(sig, search_start=CONFIG['search_start'],\n",
    "                      search_end=CONFIG['search_end']):\n",
    "    region = sig[search_start:search_end]\n",
    "    if len(region) == 0:\n",
    "        return np.argmax(sig)\n",
    "    peak_idx = search_start + np.argmax(region)\n",
    "    peak_val = sig[peak_idx]\n",
    "    noise_section = sig[:search_start]\n",
    "    if len(noise_section) > 10:\n",
    "        threshold = max(np.mean(noise_section) + 3*np.std(noise_section), 0.05*peak_val)\n",
    "    else:\n",
    "        threshold = 0.05 * peak_val\n",
    "    leading_edge = peak_idx\n",
    "    for i in range(peak_idx, max(search_start - 20, 0), -1):\n",
    "        if sig[i] < threshold:\n",
    "            leading_edge = i + 1\n",
    "            break\n",
    "    return leading_edge\n",
    "\n",
    "\n",
    "def _process_rows(df):\n",
    "    \"\"\"CIR preprocessing — outputs (N, 1, 60) channels-first for Conv1d.\"\"\"\n",
    "    PRE = CONFIG['pre_crop']; TOTAL = CONFIG['total_len']\n",
    "    cir_cols = sorted([c for c in df.columns if c.startswith('CIR')],\n",
    "                      key=lambda x: int(x.replace('CIR', '')))\n",
    "    seqs, labels = [], []\n",
    "    for _, row in df.iterrows():\n",
    "        sig = pd.to_numeric(row[cir_cols], errors='coerce').fillna(0).astype(float).values\n",
    "        rxpacc = float(row.get('RXPACC', 128.0))\n",
    "        if rxpacc > 0:\n",
    "            sig = sig / rxpacc\n",
    "        le = get_roi_alignment(sig)\n",
    "        start = max(0, le - PRE); end = start + TOTAL\n",
    "        if end > len(sig): end = len(sig); start = max(0, end - TOTAL)\n",
    "        crop = sig[start:end]\n",
    "        if len(crop) < TOTAL:\n",
    "            crop = np.pad(crop, (0, TOTAL - len(crop)), mode='constant')\n",
    "        lo, hi = crop.min(), crop.max()\n",
    "        crop = (crop - lo) / (hi - lo) if hi > lo else np.zeros(TOTAL)\n",
    "        seqs.append(crop); labels.append(float(row['Label']))\n",
    "    X = np.array(seqs).reshape(-1, 1, TOTAL).astype(np.float32)\n",
    "    y = np.array(labels).astype(np.float32)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def extract_cir_features(sig, leading_edge):\n",
    "    ss, se = PEAK_CONFIG['search_start'], PEAK_CONFIG['search_end']\n",
    "    peak_idx = np.argmax(sig[ss:se]) + ss\n",
    "    roi_start = max(0, leading_edge - 5)\n",
    "    roi_end = min(len(sig), leading_edge + 120)\n",
    "    roi = sig[roi_start:roi_end]\n",
    "    if len(roi) > 0 and np.max(roi) > 0:\n",
    "        roi_norm = roi / np.max(roi)\n",
    "        peaks, _ = find_peaks(roi_norm, prominence=PEAK_CONFIG['peak_prominence'],\n",
    "                              distance=PEAK_CONFIG['peak_min_distance'])\n",
    "        num_peaks = len(peaks)\n",
    "    else:\n",
    "        num_peaks = 0\n",
    "    return {'Num_Peaks': float(num_peaks)}\n",
    "\n",
    "\n",
    "def extract_features_from_df(data_df):\n",
    "    cir_cols = sorted([c for c in data_df.columns if c.startswith('CIR')],\n",
    "                      key=lambda x: int(x.replace('CIR', '')))\n",
    "    features_list, source_files, distances = [], [], []\n",
    "    for _, row in data_df.iterrows():\n",
    "        sig = pd.to_numeric(row[cir_cols], errors='coerce').fillna(0).astype(float).values\n",
    "        rxpacc = float(row.get('RXPACC', 128.0))\n",
    "        if rxpacc > 0:\n",
    "            sig = sig / rxpacc\n",
    "        le = get_roi_alignment(sig)\n",
    "        feats = extract_cir_features(sig, le)\n",
    "        features_list.append(feats)\n",
    "        source_files.append(str(row['Source_File']))\n",
    "        distances.append(float(row['Distance']))\n",
    "    return pd.DataFrame(features_list), source_files, distances\n",
    "\n",
    "\n",
    "print('Preprocessing & feature extraction functions ready.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 2: CNN_Classifier Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Classifier(nn.Module):\n",
    "    def __init__(self, input_channels=1, embedding_size=128, dropout=0.4):\n",
    "        super().__init__()\n",
    "        self.embedding_size = embedding_size\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv1d(input_channels, 16, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm1d(16), nn.ReLU(),\n",
    "            nn.Conv1d(16, 32, kernel_size=5, padding=2, stride=2),\n",
    "            nn.BatchNorm1d(32), nn.ReLU(),\n",
    "            nn.Conv1d(32, embedding_size, kernel_size=3, padding=1, stride=2),\n",
    "            nn.BatchNorm1d(embedding_size), nn.ReLU(),\n",
    "        )\n",
    "        self.gap = nn.AdaptiveAvgPool1d(1)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(embedding_size, 32), nn.SiLU(),\n",
    "            nn.Dropout(dropout), nn.Linear(32, 1), nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x, return_embedding=False):\n",
    "        features = self.encoder(x)\n",
    "        embedding = self.gap(features).squeeze(-1)\n",
    "        prediction = self.classifier(embedding)\n",
    "        if return_embedding:\n",
    "            return prediction, embedding\n",
    "        return prediction\n",
    "\n",
    "_m = CNN_Classifier()\n",
    "print(f'CNN_Classifier | params: {sum(p.numel() for p in _m.parameters()):,} | embed_dim=128')\n",
    "del _m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 3: Pipeline Functions\n",
    "Stage 1 training, evaluation, embedding extraction, and full pipeline runner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_s1(X_train, y_train, X_val, y_val, config=CONFIG, verbose=True, seed=None):\n",
    "    _seed = seed if seed is not None else config['seed']\n",
    "    torch.manual_seed(_seed); np.random.seed(_seed)\n",
    "    X_tr = torch.tensor(X_train).to(device)\n",
    "    y_tr = torch.tensor(y_train).unsqueeze(1).to(device)\n",
    "    X_va = torch.tensor(X_val).to(device)\n",
    "    y_va = torch.tensor(y_val).unsqueeze(1).to(device)\n",
    "    loader = DataLoader(TensorDataset(X_tr, y_tr), batch_size=config['batch_size'], shuffle=True)\n",
    "    model = CNN_Classifier(\n",
    "        input_channels=config['input_channels'],\n",
    "        embedding_size=config['embedding_size'],\n",
    "        dropout=config['dropout']\n",
    "    ).to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=config['lr'], weight_decay=config['weight_decay'])\n",
    "    T = config['max_epochs']; W = config['warmup_epochs']\n",
    "    def lr_lambda(e):\n",
    "        if e < W: return (e+1)/W\n",
    "        return max(0.01, 0.5*(1+math.cos(math.pi*(e-W)/max(1,T-W))))\n",
    "    scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "    history = {'train_loss':[], 'val_loss':[], 'train_acc':[], 'val_acc':[]}\n",
    "    best_val_acc = 0; best_state = None; patience_counter = 0\n",
    "    for epoch in range(T):\n",
    "        model.train(); tl, tc, tt = 0, 0, 0\n",
    "        for bx, by in loader:\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(bx); loss = criterion(pred, by); loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), config['grad_clip'])\n",
    "            optimizer.step()\n",
    "            tl += loss.item()*len(bx); tc += ((pred>0.5).float()==by).sum().item(); tt += len(bx)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            vp = model(X_va); vl = criterion(vp, y_va).item()\n",
    "            va = ((vp>0.5).float()==y_va).float().mean().item()\n",
    "        history['train_loss'].append(tl/tt); history['val_loss'].append(vl)\n",
    "        history['train_acc'].append(tc/tt); history['val_acc'].append(va)\n",
    "        scheduler.step()\n",
    "        if va > best_val_acc:\n",
    "            best_val_acc = va; best_state = copy.deepcopy(model.state_dict()); patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        if verbose and (epoch%10==0 or epoch==T-1):\n",
    "            print(f'  Ep {epoch:>3} | Loss: {tl/tt:.4f} | Val Acc: {100*va:.2f}% | Best: {100*best_val_acc:.2f}%')\n",
    "        if patience_counter >= config.get('patience', T):\n",
    "            if verbose: print(f'  Early stopping at epoch {epoch}')\n",
    "            break\n",
    "    model.load_state_dict(best_state)\n",
    "    return model, history\n",
    "\n",
    "\n",
    "def evaluate_s1(model, X, y_true):\n",
    "    model.eval()\n",
    "    X_t = torch.tensor(X).to(device)\n",
    "    with torch.no_grad():\n",
    "        pred = model(X_t)\n",
    "    y_prob = pred.cpu().numpy().flatten()\n",
    "    y_pred = (y_prob > 0.5).astype(float)\n",
    "    y_true = y_true.flatten()\n",
    "    rep = classification_report(y_true, y_pred, target_names=['LOS','NLOS'],\n",
    "                                output_dict=True, zero_division=0)\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
    "    return {\n",
    "        'acc': rep['accuracy'], 'f1_macro': rep['macro avg']['f1-score'],\n",
    "        'f1_los': rep['LOS']['f1-score'], 'f1_nlos': rep['NLOS']['f1-score'],\n",
    "        'auc': auc(fpr, tpr), 'fpr': fpr, 'tpr': tpr,\n",
    "        'cm': confusion_matrix(y_true, y_pred),\n",
    "    }\n",
    "\n",
    "\n",
    "def extract_cnn_embeddings(model, data_df, batch_size=256):\n",
    "    X, _ = _process_rows(data_df)\n",
    "    X_tensor = torch.tensor(X).to(device)\n",
    "    all_emb = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(X_tensor), batch_size):\n",
    "            batch = X_tensor[i:i+batch_size]\n",
    "            _, emb = model(batch, return_embedding=True)\n",
    "            all_emb.append(emb.cpu().numpy())\n",
    "    return np.vstack(all_emb)\n",
    "\n",
    "\n",
    "print('Pipeline functions ready.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pipeline(csv_path, label, config=CONFIG, verbose=True, seed=None):\n",
    "    \"\"\"Run full 3-stage pipeline. Returns dict with all metrics.\"\"\"\n",
    "    _seed = seed if seed is not None else config['seed']\n",
    "    print(f\"\\n{'='*60}\\nPipeline: {label}\\n{'='*60}\")\n",
    "\n",
    "    # ---- Load & split DataFrame ----\n",
    "    df = pd.read_csv(csv_path)\n",
    "    if verbose: print(f'Loaded {len(df)} samples (LOS={int((df[\"Label\"]==0).sum())}, NLOS={int((df[\"Label\"]==1).sum())})')\n",
    "    train_df, temp_df = train_test_split(df, test_size=config['val_ratio']+config['test_ratio'],\n",
    "                                         stratify=df['Label'], random_state=_seed)\n",
    "    val_df, test_df = train_test_split(temp_df, test_size=config['test_ratio']/(config['val_ratio']+config['test_ratio']),\n",
    "                                       stratify=temp_df['Label'], random_state=_seed)\n",
    "    train_df = train_df.reset_index(drop=True)\n",
    "    val_df = val_df.reset_index(drop=True)\n",
    "    test_df = test_df.reset_index(drop=True)\n",
    "    if verbose: print(f'Split: Train={len(train_df)}, Val={len(val_df)}, Test={len(test_df)}')\n",
    "\n",
    "    # ---- Stage 1: Train CNN ----\n",
    "    if verbose: print(f'\\nSTAGE 1: Training 1D-CNN...')\n",
    "    X_tr, y_tr = _process_rows(train_df)\n",
    "    X_va, y_va = _process_rows(val_df)\n",
    "    X_te, y_te = _process_rows(test_df)\n",
    "    model, history = train_s1(X_tr, y_tr, X_va, y_va, config=config, verbose=verbose, seed=_seed)\n",
    "    s1 = evaluate_s1(model, X_te, y_te)\n",
    "    s1['history'] = history\n",
    "    if verbose: print(f\"  Stage 1 Test: Acc={s1['acc']:.4f} | F1={s1['f1_macro']:.4f} | AUC={s1['auc']:.4f}\")\n",
    "\n",
    "    # Freeze\n",
    "    model.eval()\n",
    "    for p in model.parameters(): p.requires_grad = False\n",
    "\n",
    "    # ---- Stage 2: RF Bounce Classifier (NLOS only) ----\n",
    "    if verbose: print(f'\\nSTAGE 2: Training RF Bounce Classifier...')\n",
    "    nlos_train = train_df[train_df['Label'] == 1].reset_index(drop=True)\n",
    "    nlos_val = val_df[val_df['Label'] == 1].reset_index(drop=True)\n",
    "    nlos_test = test_df[test_df['Label'] == 1].reset_index(drop=True)\n",
    "\n",
    "    feat_train_s2, _, _ = extract_features_from_df(nlos_train)\n",
    "    feat_val_s2, _, _ = extract_features_from_df(nlos_val)\n",
    "    feat_test_s2, src_test_nlos, dist_test_nlos = extract_features_from_df(nlos_test)\n",
    "\n",
    "    threshold = PEAK_CONFIG['single_bounce_max_peaks']\n",
    "    y_train_s2 = (feat_train_s2['Num_Peaks'] > threshold).astype(float).values\n",
    "    y_val_s2 = (feat_val_s2['Num_Peaks'] > threshold).astype(float).values\n",
    "    y_test_s2 = (feat_test_s2['Num_Peaks'] > threshold).astype(float).values\n",
    "\n",
    "    X_train_emb = extract_cnn_embeddings(model, nlos_train)\n",
    "    X_val_emb = extract_cnn_embeddings(model, nlos_val)\n",
    "    X_test_emb = extract_cnn_embeddings(model, nlos_test)\n",
    "\n",
    "    rf_s2 = RandomForestClassifier(n_estimators=200, max_depth=None,\n",
    "        min_samples_split=5, min_samples_leaf=2, class_weight='balanced',\n",
    "        random_state=_seed, n_jobs=-1)\n",
    "    rf_s2.fit(X_train_emb, y_train_s2)\n",
    "    s2_preds = rf_s2.predict(X_test_emb)\n",
    "    s2_acc = (s2_preds == y_test_s2).mean()\n",
    "    if verbose: print(f'  Stage 2 Test Accuracy: {100*s2_acc:.2f}% (NLOS samples: {len(nlos_test)})')\n",
    "\n",
    "    # ---- Stage 3: RF Bias Regressor (single-bounce NLOS) ----\n",
    "    if verbose: print(f'\\nSTAGE 3: Training RF Bias Regressor...')\n",
    "    def get_s3_idx(feat_df, source_files):\n",
    "        indices, biases = [], []\n",
    "        for i, fname in enumerate(source_files):\n",
    "            grp = get_distance_group(fname)\n",
    "            if grp not in MEASURED_NLOS_BIAS: continue\n",
    "            if feat_df.iloc[i]['Num_Peaks'] > threshold: continue\n",
    "            indices.append(i); biases.append(MEASURED_NLOS_BIAS[grp])\n",
    "        return np.array(indices), np.array(biases, dtype=np.float32)\n",
    "\n",
    "    feat_train_nlos, src_train_nlos, _ = extract_features_from_df(nlos_train)\n",
    "    feat_val_nlos, src_val_nlos, _ = extract_features_from_df(nlos_val)\n",
    "    train_s3_idx, y_train_s3 = get_s3_idx(feat_train_nlos, src_train_nlos)\n",
    "    val_s3_idx, y_val_s3 = get_s3_idx(feat_val_nlos, src_val_nlos)\n",
    "    test_s3_idx, y_test_s3 = get_s3_idx(feat_test_s2, src_test_nlos)\n",
    "\n",
    "    X_train_s3 = X_train_emb[train_s3_idx]\n",
    "    X_test_s3 = X_test_emb[test_s3_idx]\n",
    "\n",
    "    rf_s3 = RandomForestRegressor(n_estimators=200, max_depth=None,\n",
    "        min_samples_split=5, min_samples_leaf=2, random_state=_seed, n_jobs=-1)\n",
    "    rf_s3.fit(X_train_s3, y_train_s3)\n",
    "\n",
    "    s3_test_preds = rf_s3.predict(X_test_s3)\n",
    "    s3_mae = mean_absolute_error(y_test_s3, s3_test_preds)\n",
    "    if verbose: print(f'  Stage 3 Test MAE: {s3_mae:.4f}m (single-bounce samples: {len(X_test_s3)})')\n",
    "\n",
    "    # ---- Build correction results ----\n",
    "    results_rows = []\n",
    "    for j, orig_i in enumerate(test_s3_idx):\n",
    "        fname = src_test_nlos[orig_i]\n",
    "        grp = get_distance_group(fname)\n",
    "        gt = GROUND_TRUTH[grp]\n",
    "        d_uwb = dist_test_nlos[orig_i]\n",
    "        ml_bias = s3_test_preds[j]\n",
    "        results_rows.append({\n",
    "            'group': grp, 'd_uwb': d_uwb, 'd_direct': gt['d_direct'],\n",
    "            'd_bounce': gt['d_bounce'], 'actual_bias': gt['bias'],\n",
    "            'ml_bias': ml_bias, 'd_corrected': d_uwb - ml_bias,\n",
    "            'bias_error': abs(ml_bias - gt['bias']),\n",
    "            'correction_error': abs((d_uwb - ml_bias) - gt['d_direct']),\n",
    "        })\n",
    "    results_df = pd.DataFrame(results_rows) if results_rows else pd.DataFrame()\n",
    "    correction_mae = results_df['correction_error'].mean() if len(results_df) > 0 else float('nan')\n",
    "\n",
    "    if verbose:\n",
    "        print(f'  Distance Correction MAE: {correction_mae:.4f}m')\n",
    "        print(f\"\\n  Summary: S1 Acc={s1['acc']:.4f} | S2 Acc={s2_acc:.4f} | S3 MAE={s3_mae:.4f}m | Corr MAE={correction_mae:.4f}m\")\n",
    "\n",
    "    return {\n",
    "        'label': label,\n",
    "        's1': s1, 's2_acc': s2_acc, 's3_mae': s3_mae,\n",
    "        'correction_mae': correction_mae, 'results_df': results_df,\n",
    "        's2_cm': confusion_matrix(y_test_s2, s2_preds),\n",
    "    }\n",
    "\n",
    "\n",
    "print('run_pipeline() ready — full 3-stage CNN pipeline.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 4: Single 70/15/15 Split — Full Pipeline Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_single = run_pipeline(DATA_DIR + 'single_channel5_dataset.csv',\n",
    "                          'Single-Channel (c5 only)')\n",
    "res_multi  = run_pipeline(DATA_DIR + 'multi_channel4_dataset.csv',\n",
    "                          'Multi-Channel (c1,c3,c4,c7)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Stage 1 Plots ──────────────────────────────────────────────────\n",
    "_res = [res_single, res_multi]\n",
    "_colors = ['#3498db', '#e74c3c']\n",
    "width = 0.3\n",
    "\n",
    "fig, axs = plt.subplots(1, 4, figsize=(24, 6))\n",
    "\n",
    "# Bar: key metrics\n",
    "ax = axs[0]\n",
    "mk = ['acc','f1_macro','auc']; mn = ['Accuracy','Macro F1','AUC']\n",
    "for i, (r, c) in enumerate(zip(_res, _colors)):\n",
    "    vals = [r['s1'][k] for k in mk]\n",
    "    bars = ax.bar(np.arange(3)+i*width, vals, width, label=r['label'], color=c, alpha=0.85)\n",
    "    for bar, v in zip(bars, vals):\n",
    "        ax.text(bar.get_x()+bar.get_width()/2, bar.get_height()+0.005,\n",
    "                f'{v:.3f}', ha='center', va='bottom', fontsize=8, fontweight='bold')\n",
    "ax.set_xticks(np.arange(3)+width/2); ax.set_xticklabels(mn)\n",
    "ax.set_ylim([0.4,1.12]); ax.set_title('Stage 1 Metrics', fontweight='bold')\n",
    "ax.legend(fontsize=8); ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# ROC\n",
    "ax = axs[1]\n",
    "for r, c in zip(_res, _colors):\n",
    "    ax.plot(r['s1']['fpr'], r['s1']['tpr'], color=c, lw=2,\n",
    "            label=f\"{r['label']} (AUC={r['s1']['auc']:.3f})\")\n",
    "ax.plot([0,1],[0,1],'k--',lw=1,alpha=0.5)\n",
    "ax.set_title('Stage 1 ROC', fontweight='bold'); ax.set_xlabel('FPR'); ax.set_ylabel('TPR')\n",
    "ax.legend(fontsize=8); ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Val acc curves\n",
    "ax = axs[2]\n",
    "for r, c in zip(_res, _colors):\n",
    "    ax.plot(r['s1']['history']['val_acc'], color=c, lw=2, label=r['label'])\n",
    "ax.set_title('Stage 1 Val Accuracy', fontweight='bold')\n",
    "ax.set_xlabel('Epoch'); ax.set_ylabel('Val Accuracy')\n",
    "ax.set_ylim([0.4,1.05]); ax.legend(fontsize=8); ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Per-class F1\n",
    "ax = axs[3]\n",
    "for i, (r, c) in enumerate(zip(_res, _colors)):\n",
    "    vals = [r['s1']['f1_los'], r['s1']['f1_nlos']]\n",
    "    bars = ax.bar(np.arange(2)+i*width, vals, width, label=r['label'], color=c, alpha=0.85)\n",
    "    for bar, v in zip(bars, vals):\n",
    "        ax.text(bar.get_x()+bar.get_width()/2, bar.get_height()+0.005,\n",
    "                f'{v:.3f}', ha='center', va='bottom', fontsize=8, fontweight='bold')\n",
    "ax.set_xticks(np.arange(2)+width/2); ax.set_xticklabels(['LOS F1','NLOS F1'])\n",
    "ax.set_ylim([0.4,1.12]); ax.set_title('Per-Class F1', fontweight='bold')\n",
    "ax.legend(fontsize=8); ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.suptitle('1D-CNN — Stage 1: Single-Channel vs Multi-Channel — Single Split',\n",
    "             fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Stages 2 & 3 Plots ────────────────────────────────────────────\n",
    "fig, axs = plt.subplots(1, 3, figsize=(20, 6))\n",
    "\n",
    "# Stage 2 accuracy bar\n",
    "ax = axs[0]\n",
    "for i, (r, c) in enumerate(zip(_res, _colors)):\n",
    "    bar = ax.bar(i, r['s2_acc'], 0.6, label=r['label'], color=c, alpha=0.85)\n",
    "    ax.text(i, r['s2_acc']+0.01, f\"{100*r['s2_acc']:.1f}%\",\n",
    "            ha='center', fontsize=12, fontweight='bold')\n",
    "ax.set_xticks(range(len(_res))); ax.set_xticklabels([r['label'] for r in _res], fontsize=9)\n",
    "ax.set_ylim([0.4,1.12]); ax.set_ylabel('Accuracy')\n",
    "ax.set_title('Stage 2: Bounce Classification', fontweight='bold')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Stage 3 MAE bar\n",
    "ax = axs[1]\n",
    "for i, (r, c) in enumerate(zip(_res, _colors)):\n",
    "    bar = ax.bar(i, r['s3_mae'], 0.6, label=r['label'], color=c, alpha=0.85)\n",
    "    ax.text(i, r['s3_mae']+0.002, f\"{r['s3_mae']:.4f}m\",\n",
    "            ha='center', fontsize=12, fontweight='bold')\n",
    "ax.set_xticks(range(len(_res))); ax.set_xticklabels([r['label'] for r in _res], fontsize=9)\n",
    "ax.set_ylabel('MAE (m)')\n",
    "ax.set_title('Stage 3: Bias Prediction MAE', fontweight='bold')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Correction MAE bar\n",
    "ax = axs[2]\n",
    "for i, (r, c) in enumerate(zip(_res, _colors)):\n",
    "    bar = ax.bar(i, r['correction_mae'], 0.6, label=r['label'], color=c, alpha=0.85)\n",
    "    ax.text(i, r['correction_mae']+0.05, f\"{r['correction_mae']:.3f}m\",\n",
    "            ha='center', fontsize=12, fontweight='bold')\n",
    "ax.set_xticks(range(len(_res))); ax.set_xticklabels([r['label'] for r in _res], fontsize=9)\n",
    "ax.set_ylabel('MAE (m)')\n",
    "ax.set_title('Distance Correction MAE', fontweight='bold')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.suptitle('1D-CNN — Stages 2 & 3: Single-Channel vs Multi-Channel — Single Split',\n",
    "             fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# Summary table\n",
    "print('\\n' + '='*65)\n",
    "print(f\"{'Metric':<25} {'Single-Ch':>12} {'Multi-Ch':>12}  Delta\")\n",
    "print('='*65)\n",
    "for k, n in [('acc','S1 Accuracy'),('f1_macro','S1 Macro F1'),('auc','S1 AUC')]:\n",
    "    sv=res_single['s1'][k]; mv=res_multi['s1'][k]; d=mv-sv\n",
    "    print(f\"{n:<25} {sv:>12.4f} {mv:>12.4f}   {'\\u25b2' if d>0 else '\\u25bc'}{abs(d):.4f}\")\n",
    "for k, n, flip in [('s2_acc','S2 Bounce Acc',False),('s3_mae','S3 Bias MAE',True),('correction_mae','Correction MAE',True)]:\n",
    "    sv=res_single[k]; mv=res_multi[k]; d=mv-sv\n",
    "    better = d<0 if flip else d>0\n",
    "    print(f\"{n:<25} {sv:>12.4f} {mv:>12.4f}   {'\\u25b2' if better else '\\u25bc'}{abs(d):.4f}\")\n",
    "print('='*65)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 5: Stratified 5-Fold Cross-Validation — Full Pipeline\n",
    "More reliable estimate: averages over 5 different train/test splits across all 3 stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_kfold_pipeline(csv_path, label, n_splits=5, config=CONFIG, seed=42):\n",
    "    print(f\"\\n{'='*60}\\n5-Fold CV Pipeline: {label}\\n{'='*60}\")\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(f'Loaded {len(df)} samples')\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    fold_metrics = []\n",
    "\n",
    "    for fold, (tv_idx, te_idx) in enumerate(skf.split(df, df['Label'])):\n",
    "        df_tv = df.iloc[tv_idx].reset_index(drop=True)\n",
    "        df_te = df.iloc[te_idx].reset_index(drop=True)\n",
    "        df_tr, df_va = train_test_split(df_tv, test_size=0.15, stratify=df_tv['Label'], random_state=seed)\n",
    "        df_tr = df_tr.reset_index(drop=True); df_va = df_va.reset_index(drop=True)\n",
    "\n",
    "        fold_seed = seed + fold\n",
    "        # Stage 1\n",
    "        X_tr, y_tr = _process_rows(df_tr)\n",
    "        X_va, y_va = _process_rows(df_va)\n",
    "        X_te, y_te = _process_rows(df_te)\n",
    "        with contextlib.redirect_stdout(io.StringIO()):\n",
    "            model, _ = train_s1(X_tr, y_tr, X_va, y_va, config=config, verbose=False, seed=fold_seed)\n",
    "        s1 = evaluate_s1(model, X_te, y_te)\n",
    "        model.eval()\n",
    "        for p in model.parameters(): p.requires_grad = False\n",
    "\n",
    "        # Stage 2\n",
    "        nlos_tr = df_tr[df_tr['Label']==1].reset_index(drop=True)\n",
    "        nlos_te = df_te[df_te['Label']==1].reset_index(drop=True)\n",
    "        feat_tr_s2, _, _ = extract_features_from_df(nlos_tr)\n",
    "        feat_te_s2, src_te, dist_te = extract_features_from_df(nlos_te)\n",
    "        thr = PEAK_CONFIG['single_bounce_max_peaks']\n",
    "        y_tr_s2 = (feat_tr_s2['Num_Peaks'] > thr).astype(float).values\n",
    "        y_te_s2 = (feat_te_s2['Num_Peaks'] > thr).astype(float).values\n",
    "        emb_tr = extract_cnn_embeddings(model, nlos_tr)\n",
    "        emb_te = extract_cnn_embeddings(model, nlos_te)\n",
    "        rf2 = RandomForestClassifier(n_estimators=200, max_depth=None,\n",
    "            min_samples_split=5, min_samples_leaf=2, class_weight='balanced',\n",
    "            random_state=fold_seed, n_jobs=-1)\n",
    "        rf2.fit(emb_tr, y_tr_s2)\n",
    "        s2_acc = (rf2.predict(emb_te) == y_te_s2).mean()\n",
    "\n",
    "        # Stage 3\n",
    "        def _s3_idx(feat_df, src_files):\n",
    "            idx, bias = [], []\n",
    "            for i, fn in enumerate(src_files):\n",
    "                g = get_distance_group(fn)\n",
    "                if g not in MEASURED_NLOS_BIAS: continue\n",
    "                if feat_df.iloc[i]['Num_Peaks'] > thr: continue\n",
    "                idx.append(i); bias.append(MEASURED_NLOS_BIAS[g])\n",
    "            return np.array(idx), np.array(bias, dtype=np.float32)\n",
    "\n",
    "        feat_tr_nlos, src_tr, _ = extract_features_from_df(nlos_tr)\n",
    "        tr3_idx, y_tr3 = _s3_idx(feat_tr_nlos, src_tr)\n",
    "        te3_idx, y_te3 = _s3_idx(feat_te_s2, src_te)\n",
    "\n",
    "        s3_mae = float('nan')\n",
    "        correction_mae = float('nan')\n",
    "        if len(tr3_idx) > 0 and len(te3_idx) > 0:\n",
    "            rf3 = RandomForestRegressor(n_estimators=200, max_depth=None,\n",
    "                min_samples_split=5, min_samples_leaf=2, random_state=fold_seed, n_jobs=-1)\n",
    "            rf3.fit(emb_tr[tr3_idx], y_tr3)\n",
    "            s3_preds = rf3.predict(emb_te[te3_idx])\n",
    "            s3_mae = mean_absolute_error(y_te3, s3_preds)\n",
    "            # Correction\n",
    "            corr_errors = []\n",
    "            for j, oi in enumerate(te3_idx):\n",
    "                g = get_distance_group(src_te[oi])\n",
    "                gt = GROUND_TRUTH[g]\n",
    "                d_corr = dist_te[oi] - s3_preds[j]\n",
    "                corr_errors.append(abs(d_corr - gt['d_direct']))\n",
    "            correction_mae = np.mean(corr_errors)\n",
    "\n",
    "        fm = {'s1_acc': s1['acc'], 's1_f1': s1['f1_macro'], 's1_auc': s1['auc'],\n",
    "              's2_acc': s2_acc, 's3_mae': s3_mae, 'correction_mae': correction_mae}\n",
    "        fold_metrics.append(fm)\n",
    "        print(f\"  Fold {fold+1}/{n_splits} | S1={s1['acc']:.3f} | S2={s2_acc:.3f} | S3={s3_mae:.4f}m | Corr={correction_mae:.3f}m\")\n",
    "\n",
    "    summary = {'label': label}\n",
    "    print(f\"\\n  {'\\u2500'*55}\")\n",
    "    print(f\"  {'Metric':<18} {'Mean':>8} {'Std':>8}\")\n",
    "    print(f\"  {'\\u2500'*55}\")\n",
    "    for key in ['s1_acc','s1_f1','s1_auc','s2_acc','s3_mae','correction_mae']:\n",
    "        vals = np.array([m[key] for m in fold_metrics])\n",
    "        valid = vals[~np.isnan(vals)]\n",
    "        summary[key] = {'mean': valid.mean(), 'std': valid.std(), 'all': vals.tolist()}\n",
    "        print(f\"  {key:<18} {valid.mean():>8.4f} {valid.std():>8.4f}\")\n",
    "    print(f\"  {'\\u2500'*55}\")\n",
    "    return summary\n",
    "\n",
    "\n",
    "kfold_single = run_kfold_pipeline(DATA_DIR + 'single_channel5_dataset.csv', 'Single-Channel (c5 only)')\n",
    "kfold_multi  = run_kfold_pipeline(DATA_DIR + 'multi_channel4_dataset.csv',  'Multi-Channel (c1,c3,c4,c7)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── K-Fold comparison plots ────────────────────────────────────────\n",
    "_kf = [kfold_single, kfold_multi]\n",
    "_colors = ['#3498db', '#e74c3c']\n",
    "_mk = ['s1_acc','s1_f1','s1_auc','s2_acc','s3_mae','correction_mae']\n",
    "_mn = ['S1 Accuracy','S1 Macro F1','S1 AUC','S2 Bounce Acc','S3 Bias MAE','Correction MAE']\n",
    "width = 0.3\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(20, 7))\n",
    "\n",
    "# Bar with error bars\n",
    "ax = axs[0]\n",
    "x = np.arange(len(_mk))\n",
    "for i, (kf, c) in enumerate(zip(_kf, _colors)):\n",
    "    means = [kf[m]['mean'] for m in _mk]\n",
    "    stds  = [kf[m]['std']  for m in _mk]\n",
    "    bars = ax.bar(x+i*width, means, width, yerr=stds, label=kf['label'],\n",
    "                  color=c, alpha=0.85, capsize=4, error_kw={'elinewidth':1.5})\n",
    "    for bar, m, s in zip(bars, means, stds):\n",
    "        ax.text(bar.get_x()+bar.get_width()/2, bar.get_height()+s+0.008,\n",
    "                f'{m:.3f}', ha='center', va='bottom', fontsize=7, fontweight='bold')\n",
    "ax.set_xticks(x+width/2); ax.set_xticklabels(_mn, rotation=15, ha='right')\n",
    "ax.set_title('5-Fold CV: Mean \\u00b1 Std — Full Pipeline', fontweight='bold')\n",
    "ax.set_ylabel('Score / MAE'); ax.legend(fontsize=9); ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Box plots\n",
    "ax = axs[1]\n",
    "all_data, all_colors, positions = [], [], []\n",
    "for mi, metric in enumerate(_mk):\n",
    "    for ki, (kf, c) in enumerate(zip(_kf, _colors)):\n",
    "        positions.append(mi*3 + ki)\n",
    "        vals = [v for v in kf[metric]['all'] if not np.isnan(v)]\n",
    "        all_data.append(vals)\n",
    "        all_colors.append(c)\n",
    "bp = ax.boxplot(all_data, positions=positions, widths=0.7, patch_artist=True,\n",
    "                medianprops={'color':'black','linewidth':2})\n",
    "for patch, c in zip(bp['boxes'], all_colors):\n",
    "    patch.set_facecolor(c); patch.set_alpha(0.7)\n",
    "ax.set_xticks([mi*3+0.5 for mi in range(len(_mk))])\n",
    "ax.set_xticklabels(_mn, rotation=15, ha='right')\n",
    "ax.set_title('Score Distribution per Fold (blue=single, red=multi)', fontweight='bold')\n",
    "ax.set_ylabel('Score / MAE'); ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.suptitle('1D-CNN — Full Pipeline: Stratified 5-Fold CV — Single vs Multi Channel',\n",
    "             fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# Summary table\n",
    "print('\\n' + '='*70)\n",
    "print(f\"{'Metric':<18} {'Single Mean\\u00b1Std':>20} {'Multi Mean\\u00b1Std':>20}  Delta\")\n",
    "print('='*70)\n",
    "for k, n in zip(_mk, _mn):\n",
    "    sm, ss = kfold_single[k]['mean'], kfold_single[k]['std']\n",
    "    mm, ms = kfold_multi[k]['mean'],  kfold_multi[k]['std']\n",
    "    d = mm - sm\n",
    "    print(f\"{n:<18} {sm:>8.4f}\\u00b1{ss:.4f}      {mm:>8.4f}\\u00b1{ms:.4f}   {'\\u25b2' if d>0 else '\\u25bc'}{abs(d):.4f}\")\n",
    "print('='*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}