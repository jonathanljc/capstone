{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Stage 1 Comparison: Single-Channel vs Multi-Channel Dataset\n",
    "## LSTM Baseline — Justification for Multi-Channel Data Collection\n",
    "\n",
    "Trains the same **LSTM_Classifier** on two 600-sample datasets under **equal sample conditions**:\n",
    "- **Single-Channel**: channel 5 only — 100 samples per scenario\n",
    "- **Multi-Channel**: 4 channels with distinct center frequencies (c1, c3, c4, c7) — **25 samples per channel per scenario**\n",
    "\n",
    "Channels selected to maximise diversity across both center frequency and bandwidth:\n",
    "\n",
    "| Channel | Center Freq | Bandwidth | Character |\n",
    "|---|---|---|---|\n",
    "| c1 | 3494.4 MHz | 499.2 MHz | Low-freq, narrowband |\n",
    "| c3 | 4492.8 MHz | 499.2 MHz | Mid-freq, narrowband |\n",
    "| c4 | 3993.6 MHz | 1331.2 MHz | Mid-freq, **wideband** |\n",
    "| c7 | 6489.6 MHz | 1081.6 MHz | High-freq, **wideband** |\n",
    "\n",
    "All four channels have **distinct center frequencies**, spanning 3.5–6.5 GHz with bandwidths ranging from 499 MHz to 1331 MHz.\n",
    "\n",
    "Three levels of evaluation:\n",
    "1. Single 70/15/15 split\n",
    "2. Stratified 5-Fold CV (mean ± std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    \"pre_crop\": 10, \"post_crop\": 50, \"total_len\": 60,\n",
    "    \"search_start\": 740, \"search_end\": 890,\n",
    "    \"hidden_size\": 64, \"input_size\": 1, \"dropout\": 0.2,\n",
    "    \"batch_size\": 64, \"max_epochs\": 50, \"lr\": 1e-3,\n",
    "    \"weight_decay\": 1e-4, \"warmup_epochs\": 3, \"patience\": 40,\n",
    "    \"grad_clip\": 1.0, \"val_ratio\": 0.15, \"test_ratio\": 0.15, \"seed\": 42,\n",
    "}\n",
    "DATA_DIR = \"../dataset/channels/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": "import copy, math, contextlib, io\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.metrics import (\n    confusion_matrix, ConfusionMatrixDisplay,\n    classification_report, roc_curve, auc\n)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nif device.type == \"cuda\":\n    torch.cuda.empty_cache()\nprint(f\"Device: {device}\")\ntorch.manual_seed(CONFIG[\"seed\"])\nnp.random.seed(CONFIG[\"seed\"])"
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 1: Data Loading & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loading functions ready (CIR only — no FP_AMPL for LSTM).\n"
     ]
    }
   ],
   "source": [
    "def get_roi_alignment(sig,\n",
    "                      search_start=CONFIG['search_start'],\n",
    "                      search_end=CONFIG['search_end']):\n",
    "    region = sig[search_start:search_end]\n",
    "    if len(region) == 0:\n",
    "        return np.argmax(sig)\n",
    "    peak_idx = search_start + np.argmax(region)\n",
    "    peak_val = sig[peak_idx]\n",
    "    noise_section = sig[:search_start]\n",
    "    if len(noise_section) > 10:\n",
    "        threshold = max(np.mean(noise_section) + 3*np.std(noise_section), 0.05*peak_val)\n",
    "    else:\n",
    "        threshold = 0.05 * peak_val\n",
    "    leading_edge = peak_idx\n",
    "    for i in range(peak_idx, max(search_start - 20, 0), -1):\n",
    "        if sig[i] < threshold:\n",
    "            leading_edge = i + 1\n",
    "            break\n",
    "    return leading_edge\n",
    "\n",
    "\n",
    "def _process_rows(df):\n",
    "    \"\"\"CIR preprocessing — no FP_AMPL for LSTM.\"\"\"\n",
    "    PRE = CONFIG['pre_crop']; TOTAL = CONFIG['total_len']\n",
    "    cir_cols = sorted([c for c in df.columns if c.startswith('CIR')],\n",
    "                      key=lambda x: int(x.replace('CIR', '')))\n",
    "    seqs, labels = [], []\n",
    "    for _, row in df.iterrows():\n",
    "        sig = pd.to_numeric(row[cir_cols], errors='coerce').fillna(0).astype(float).values\n",
    "        rxpacc = float(row.get('RXPACC', 128.0))\n",
    "        if rxpacc > 0:\n",
    "            sig = sig / rxpacc\n",
    "        le = get_roi_alignment(sig)\n",
    "        start = max(0, le - PRE); end = start + TOTAL\n",
    "        if end > len(sig): end = len(sig); start = max(0, end - TOTAL)\n",
    "        crop = sig[start:end]\n",
    "        if len(crop) < TOTAL:\n",
    "            crop = np.pad(crop, (0, TOTAL - len(crop)), mode='constant')\n",
    "        lo, hi = crop.min(), crop.max()\n",
    "        crop = (crop - lo) / (hi - lo) if hi > lo else np.zeros(TOTAL)\n",
    "        seqs.append(crop); labels.append(float(row['Label']))\n",
    "    X = np.array(seqs).reshape(-1, TOTAL, 1).astype(np.float32)\n",
    "    y = np.array(labels).astype(np.float32)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def load_cir_dataset(filepath):\n",
    "    print(f'Loading: {filepath}')\n",
    "    df = pd.read_csv(filepath)\n",
    "    print(f'  Samples: {len(df)}')\n",
    "    X, y = _process_rows(df)\n",
    "    print(f'  Output: X={X.shape}, y={y.shape} | LOS={int((y==0).sum())}, NLOS={int((y==1).sum())}')\n",
    "    return X, y\n",
    "\n",
    "print(\"Data loading functions ready (CIR only — no FP_AMPL for LSTM).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 2: LSTM_Classifier Architecture\n",
    "Standard LSTM with attention pooling — no FP conditioning, no ODE dynamics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM_Classifier | params: 19,330 | embed_dim=64\n",
      "  No FP_AMPL conditioning — standard zero-init h0/c0\n"
     ]
    }
   ],
   "source": [
    "class LSTM_Classifier(nn.Module):\n",
    "    \"\"\"Standard LSTM baseline with attention pooling. No FP_AMPL conditioning.\"\"\"\n",
    "    def __init__(self, input_size=1, hidden_size=64, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size,\n",
    "                           num_layers=1, batch_first=True)\n",
    "        self.attn = nn.Linear(hidden_size, 1)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 32), nn.SiLU(),\n",
    "            nn.Dropout(dropout), nn.Linear(32, 1), nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x_seq):\n",
    "        batch_size = x_seq.size(0)\n",
    "        h0 = torch.zeros(1, batch_size, self.hidden_size, device=x_seq.device)\n",
    "        c0 = torch.zeros(1, batch_size, self.hidden_size, device=x_seq.device)\n",
    "        h_all, _ = self.lstm(x_seq, (h0, c0))\n",
    "        attn_w = F.softmax(self.attn(h_all).squeeze(-1), dim=1).unsqueeze(-1)\n",
    "        h_pooled = (h_all * attn_w).sum(dim=1)\n",
    "        return self.classifier(h_pooled)\n",
    "\n",
    "    def embed(self, x_seq):\n",
    "        batch_size = x_seq.size(0)\n",
    "        h0 = torch.zeros(1, batch_size, self.hidden_size, device=x_seq.device)\n",
    "        c0 = torch.zeros(1, batch_size, self.hidden_size, device=x_seq.device)\n",
    "        h_all, _ = self.lstm(x_seq, (h0, c0))\n",
    "        attn_w = F.softmax(self.attn(h_all).squeeze(-1), dim=1).unsqueeze(-1)\n",
    "        return (h_all * attn_w).sum(dim=1)\n",
    "\n",
    "_m = LSTM_Classifier()\n",
    "print(f'LSTM_Classifier | params: {sum(p.numel() for p in _m.parameters()):,} | embed_dim=64')\n",
    "print(f'  No FP_AMPL conditioning — standard zero-init h0/c0')\n",
    "del _m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 3: Training Function\n",
    "Early stopping (patience=40), best-model restoration. No FP_AMPL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_model() ready — LSTM, no FP_AMPL, patience=40.\n"
     ]
    }
   ],
   "source": [
    "def train_model(X_train, y_train, X_val, y_val,\n",
    "                config=CONFIG, verbose=True, seed=None):\n",
    "    _seed = seed if seed is not None else config['seed']\n",
    "    torch.manual_seed(_seed)\n",
    "    np.random.seed(_seed)\n",
    "\n",
    "    X_tr = torch.tensor(X_train).to(device)\n",
    "    y_tr = torch.tensor(y_train).unsqueeze(1).to(device)\n",
    "    X_va = torch.tensor(X_val).to(device)\n",
    "    y_va = torch.tensor(y_val).unsqueeze(1).to(device)\n",
    "\n",
    "    loader = DataLoader(TensorDataset(X_tr, y_tr),\n",
    "                        batch_size=config['batch_size'], shuffle=True)\n",
    "\n",
    "    model = LSTM_Classifier(\n",
    "        input_size=config['input_size'], hidden_size=config['hidden_size'],\n",
    "        dropout=config['dropout']\n",
    "    ).to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=config['lr'],\n",
    "                            weight_decay=config['weight_decay'])\n",
    "    T = config['max_epochs']; W = config['warmup_epochs']\n",
    "    def lr_lambda(e):\n",
    "        if e < W: return (e+1)/W\n",
    "        return max(0.01, 0.5*(1+math.cos(math.pi*(e-W)/max(1,T-W))))\n",
    "    scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "    history = {'train_loss':[], 'val_loss':[], 'train_acc':[], 'val_acc':[]}\n",
    "    best_val_acc = 0; best_state = None; patience_counter = 0\n",
    "\n",
    "    for epoch in range(T):\n",
    "        model.train(); tl, tc, tt = 0, 0, 0\n",
    "        for bx, by in loader:\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(bx)\n",
    "            loss = criterion(pred, by); loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), config['grad_clip'])\n",
    "            optimizer.step()\n",
    "            tl += loss.item()*len(bx); tc += ((pred>0.5).float()==by).sum().item(); tt += len(bx)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            vp = model(X_va)\n",
    "            vl = criterion(vp, y_va).item()\n",
    "            va = ((vp>0.5).float()==y_va).float().mean().item()\n",
    "        history['train_loss'].append(tl/tt); history['val_loss'].append(vl)\n",
    "        history['train_acc'].append(tc/tt);  history['val_acc'].append(va)\n",
    "        scheduler.step()\n",
    "\n",
    "        if va > best_val_acc:\n",
    "            best_val_acc = va; best_state = copy.deepcopy(model.state_dict()); patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if verbose and (epoch%10==0 or epoch==T-1):\n",
    "            print(f'  Ep {epoch:>3} | Loss: {tl/tt:.4f} | Val Acc: {100*va:.2f}% | Best: {100*best_val_acc:.2f}%')\n",
    "        if patience_counter >= config.get('patience', T):\n",
    "            if verbose: print(f'  Early stopping at epoch {epoch}')\n",
    "            break\n",
    "\n",
    "    model.load_state_dict(best_state)\n",
    "    if verbose: print(f'  Best Val Acc: {100*best_val_acc:.2f}%')\n",
    "    return model, history\n",
    "\n",
    "print('train_model() ready — LSTM, no FP_AMPL, patience=40.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluate() helper ready (LSTM — no FP_AMPL).\n"
     ]
    }
   ],
   "source": [
    "def evaluate(model, X, y_true):\n",
    "    \"\"\"Returns metrics dict for a given LSTM model and data.\"\"\"\n",
    "    model.eval()\n",
    "    X_t = torch.tensor(X).to(device)\n",
    "    with torch.no_grad():\n",
    "        pred = model(X_t)\n",
    "    y_prob = pred.cpu().numpy().flatten()\n",
    "    y_pred = (y_prob > 0.5).astype(float)\n",
    "    y_true = y_true.flatten()\n",
    "    rep  = classification_report(y_true, y_pred, target_names=['LOS','NLOS'],\n",
    "                                 output_dict=True, zero_division=0)\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
    "    return {\n",
    "        'acc':      rep['accuracy'],\n",
    "        'f1_macro': rep['macro avg']['f1-score'],\n",
    "        'f1_los':   rep['LOS']['f1-score'],\n",
    "        'f1_nlos':  rep['NLOS']['f1-score'],\n",
    "        'auc':      auc(fpr, tpr),\n",
    "        'fpr': fpr, 'tpr': tpr,\n",
    "        'cm':  confusion_matrix(y_true, y_pred),\n",
    "    }\n",
    "\n",
    "print('evaluate() helper ready (LSTM — no FP_AMPL).')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 4: Single 70/15/15 Split Comparison\n",
    "Quick baseline comparison using one fixed split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Experiment: Single-Channel (c5 only)\n",
      "============================================================\n",
      "Loading: ../dataset/channels/single_channel5_dataset.csv\n",
      "  Samples: 600\n",
      "  Output: X=(600, 60, 1), y=(600,) | LOS=300, NLOS=300\n",
      "  Train=420, Val=90, Test=90\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "cuDNN error: CUDNN_STATUS_INTERNAL_ERROR_HOST_ALLOCATION_FAILED",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     15\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m  Test Acc=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mm[\u001b[33m'\u001b[39m\u001b[33macc\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | Macro F1=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mm[\u001b[33m'\u001b[39m\u001b[33mf1_macro\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | AUC=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mm[\u001b[33m'\u001b[39m\u001b[33mauc\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     16\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m results_single = \u001b[43mrun_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msingle_channel5_dataset.csv\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mSingle-Channel (c5 only)\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m results_multi  = run_experiment(\u001b[33m'\u001b[39m\u001b[33mmulti_channel4_dataset.csv\u001b[39m\u001b[33m'\u001b[39m,   \u001b[33m'\u001b[39m\u001b[33mMulti-Channel (c1,c3,c4,c7)\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mrun_experiment\u001b[39m\u001b[34m(csv_name, label, config, seed)\u001b[39m\n\u001b[32m      7\u001b[39m X_va, X_te, y_va, y_te = train_test_split(\n\u001b[32m      8\u001b[39m     X_tmp, y_tmp,\n\u001b[32m      9\u001b[39m     test_size=config[\u001b[33m'\u001b[39m\u001b[33mtest_ratio\u001b[39m\u001b[33m'\u001b[39m]/(config[\u001b[33m'\u001b[39m\u001b[33mval_ratio\u001b[39m\u001b[33m'\u001b[39m]+config[\u001b[33m'\u001b[39m\u001b[33mtest_ratio\u001b[39m\u001b[33m'\u001b[39m]),\n\u001b[32m     10\u001b[39m     stratify=y_tmp, random_state=seed)\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m  Train=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(X_tr)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Val=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(X_va)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Test=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(X_te)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m model, history = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_tr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_tr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_va\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_va\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m m = evaluate(model, X_te, y_te)\n\u001b[32m     14\u001b[39m m[\u001b[33m'\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m'\u001b[39m] = label; m[\u001b[33m'\u001b[39m\u001b[33mhistory\u001b[39m\u001b[33m'\u001b[39m] = history\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 35\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(X_train, y_train, X_val, y_val, config, verbose, seed)\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m bx, by \u001b[38;5;129;01min\u001b[39;00m loader:\n\u001b[32m     34\u001b[39m     optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m     pred = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m     loss = criterion(pred, by); loss.backward()\n\u001b[32m     37\u001b[39m     nn.utils.clip_grad_norm_(model.parameters(), config[\u001b[33m'\u001b[39m\u001b[33mgrad_clip\u001b[39m\u001b[33m'\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\preca\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\preca\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 18\u001b[39m, in \u001b[36mLSTM_Classifier.forward\u001b[39m\u001b[34m(self, x_seq)\u001b[39m\n\u001b[32m     16\u001b[39m h0 = torch.zeros(\u001b[32m1\u001b[39m, batch_size, \u001b[38;5;28mself\u001b[39m.hidden_size, device=x_seq.device)\n\u001b[32m     17\u001b[39m c0 = torch.zeros(\u001b[32m1\u001b[39m, batch_size, \u001b[38;5;28mself\u001b[39m.hidden_size, device=x_seq.device)\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m h_all, _ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_seq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mh0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc0\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m attn_w = F.softmax(\u001b[38;5;28mself\u001b[39m.attn(h_all).squeeze(-\u001b[32m1\u001b[39m), dim=\u001b[32m1\u001b[39m).unsqueeze(-\u001b[32m1\u001b[39m)\n\u001b[32m     20\u001b[39m h_pooled = (h_all * attn_w).sum(dim=\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\preca\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\preca\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\preca\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:1124\u001b[39m, in \u001b[36mLSTM.forward\u001b[39m\u001b[34m(self, input, hx)\u001b[39m\n\u001b[32m   1121\u001b[39m         hx = \u001b[38;5;28mself\u001b[39m.permute_hidden(hx, sorted_indices)\n\u001b[32m   1123\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1124\u001b[39m     result = \u001b[43m_VF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1125\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1126\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1127\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   1128\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1129\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1130\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1131\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1132\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1133\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbatch_first\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1134\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1135\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1136\u001b[39m     result = _VF.lstm(\n\u001b[32m   1137\u001b[39m         \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   1138\u001b[39m         batch_sizes,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1145\u001b[39m         \u001b[38;5;28mself\u001b[39m.bidirectional,\n\u001b[32m   1146\u001b[39m     )\n",
      "\u001b[31mRuntimeError\u001b[39m: cuDNN error: CUDNN_STATUS_INTERNAL_ERROR_HOST_ALLOCATION_FAILED"
     ]
    }
   ],
   "source": [
    "def run_experiment(csv_name, label, config=CONFIG, seed=42):\n",
    "    print(f\"\\n{'='*60}\\nExperiment: {label}\\n{'='*60}\")\n",
    "    X_all, y_all = load_cir_dataset(DATA_DIR + csv_name)\n",
    "    X_tr, X_tmp, y_tr, y_tmp = train_test_split(\n",
    "        X_all, y_all, test_size=config['val_ratio']+config['test_ratio'],\n",
    "        stratify=y_all, random_state=seed)\n",
    "    X_va, X_te, y_va, y_te = train_test_split(\n",
    "        X_tmp, y_tmp,\n",
    "        test_size=config['test_ratio']/(config['val_ratio']+config['test_ratio']),\n",
    "        stratify=y_tmp, random_state=seed)\n",
    "    print(f'  Train={len(X_tr)}, Val={len(X_va)}, Test={len(X_te)}')\n",
    "    model, history = train_model(X_tr, y_tr, X_va, y_va, config=config)\n",
    "    m = evaluate(model, X_te, y_te)\n",
    "    m['label'] = label; m['history'] = history\n",
    "    print(f\"\\n  Test Acc={m['acc']:.4f} | Macro F1={m['f1_macro']:.4f} | AUC={m['auc']:.4f}\")\n",
    "    return m\n",
    "\n",
    "\n",
    "results_single = run_experiment('single_channel5_dataset.csv',  'Single-Channel (c5 only)')\n",
    "results_multi  = run_experiment('multi_channel4_dataset.csv',   'Multi-Channel (c1,c3,c4,c7)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Single-split plots ──────────────────────────────────────────────────\n",
    "_res = [results_single, results_multi]\n",
    "_colors = ['#3498db', '#e74c3c']\n",
    "width = 0.3\n",
    "\n",
    "fig, axs = plt.subplots(1, 4, figsize=(24, 6))\n",
    "\n",
    "# Bar: key metrics\n",
    "ax = axs[0]\n",
    "mk = ['acc','f1_macro','auc']; mn = ['Accuracy','Macro F1','AUC']\n",
    "for i, (r, c) in enumerate(zip(_res, _colors)):\n",
    "    vals = [r[k] for k in mk]\n",
    "    bars = ax.bar(np.arange(3)+i*width, vals, width, label=r['label'], color=c, alpha=0.85)\n",
    "    for bar, v in zip(bars, vals):\n",
    "        ax.text(bar.get_x()+bar.get_width()/2, bar.get_height()+0.005,\n",
    "                f'{v:.3f}', ha='center', va='bottom', fontsize=8, fontweight='bold')\n",
    "ax.set_xticks(np.arange(3)+width/2); ax.set_xticklabels(mn)\n",
    "ax.set_ylim([0.4,1.12]); ax.set_title('Metric Comparison', fontweight='bold')\n",
    "ax.legend(fontsize=8); ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# ROC\n",
    "ax = axs[1]\n",
    "for r, c in zip(_res, _colors):\n",
    "    ax.plot(r['fpr'], r['tpr'], color=c, lw=2, label=f\"{r['label']} (AUC={r['auc']:.3f})\")\n",
    "ax.plot([0,1],[0,1],'k--',lw=1,alpha=0.5)\n",
    "ax.set_title('ROC Curves', fontweight='bold'); ax.set_xlabel('FPR'); ax.set_ylabel('TPR')\n",
    "ax.legend(fontsize=8); ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Val acc curves\n",
    "ax = axs[2]\n",
    "for r, c in zip(_res, _colors):\n",
    "    ax.plot(r['history']['val_acc'], color=c, lw=2, label=r['label'])\n",
    "ax.set_title('Val Accuracy per Epoch', fontweight='bold')\n",
    "ax.set_xlabel('Epoch'); ax.set_ylabel('Val Accuracy')\n",
    "ax.set_ylim([0.4,1.05]); ax.legend(fontsize=8); ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Per-class F1\n",
    "ax = axs[3]\n",
    "for i, (r, c) in enumerate(zip(_res, _colors)):\n",
    "    vals = [r['f1_los'], r['f1_nlos']]\n",
    "    bars = ax.bar(np.arange(2)+i*width, vals, width, label=r['label'], color=c, alpha=0.85)\n",
    "    for bar, v in zip(bars, vals):\n",
    "        ax.text(bar.get_x()+bar.get_width()/2, bar.get_height()+0.005,\n",
    "                f'{v:.3f}', ha='center', va='bottom', fontsize=8, fontweight='bold')\n",
    "ax.set_xticks(np.arange(2)+width/2); ax.set_xticklabels(['LOS F1','NLOS F1'])\n",
    "ax.set_ylim([0.4,1.12]); ax.set_title('Per-Class F1', fontweight='bold')\n",
    "ax.legend(fontsize=8); ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.suptitle('LSTM Baseline — Stage 1: Single-Channel vs Multi-Channel — Single Split',\n",
    "             fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "print('\\n' + '='*55)\n",
    "print(f\"{'Metric':<20} {'Single-Ch':>12} {'Multi-Ch':>12}  Delta\")\n",
    "print('='*55)\n",
    "for k, n in zip(['acc','f1_los','f1_nlos','f1_macro','auc'],\n",
    "                ['Accuracy','F1 LOS','F1 NLOS','Macro F1','AUC']):\n",
    "    sv=results_single[k]; mv=results_multi[k]; d=mv-sv\n",
    "    print(f\"{n:<20} {sv:>12.4f} {mv:>12.4f}   {'▲' if d>0 else '▼'}{abs(d):.4f}\")\n",
    "print('='*55)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 5: Stratified 5-Fold Cross-Validation\n",
    "More reliable estimate: averages over 5 different train/test splits.\n",
    "Epoch prints are suppressed — only one summary line per fold is shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_kfold(csv_name, label, n_splits=5, config=CONFIG, seed=42):\n",
    "    print(f\"\\n{'='*60}\\n5-Fold CV: {label}\\n{'='*60}\")\n",
    "    X_all, y_all = load_cir_dataset(DATA_DIR + csv_name)\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    fold_metrics = []\n",
    "    for fold, (tv_idx, te_idx) in enumerate(skf.split(X_all, y_all)):\n",
    "        X_tv, X_te = X_all[tv_idx], X_all[te_idx]\n",
    "        y_tv, y_te = y_all[tv_idx], y_all[te_idx]\n",
    "        X_tr, X_va, y_tr, y_va = train_test_split(\n",
    "            X_tv, y_tv, test_size=0.15, stratify=y_tv, random_state=seed)\n",
    "        fold_seed = seed + fold\n",
    "        with contextlib.redirect_stdout(io.StringIO()):\n",
    "            model, _ = train_model(X_tr, y_tr, X_va, y_va,\n",
    "                                   config=config, verbose=False, seed=fold_seed)\n",
    "        fm = evaluate(model, X_te, y_te)\n",
    "        fold_metrics.append(fm)\n",
    "        collapsed = \" [COLLAPSED]\" if fm['acc'] <= 0.51 else \"\"\n",
    "        print(f\"  Fold {fold+1}/{n_splits} | Acc={fm['acc']:.4f} | F1={fm['f1_macro']:.4f} | AUC={fm['auc']:.4f}{collapsed}\")\n",
    "    summary = {'label': label}\n",
    "    print(f\"\\n  {'─'*45}\")\n",
    "    print(f\"  {'Metric':<12} {'Mean':>8} {'Std':>8}\")\n",
    "    print(f\"  {'─'*45}\")\n",
    "    for key in ['acc','f1_macro','f1_los','f1_nlos','auc']:\n",
    "        vals = np.array([m[key] for m in fold_metrics])\n",
    "        summary[key] = {'mean': vals.mean(), 'std': vals.std(), 'all': vals.tolist()}\n",
    "        print(f\"  {key:<12} {vals.mean():>8.4f} {vals.std():>8.4f}\")\n",
    "    print(f\"  {'─'*45}\")\n",
    "    return summary\n",
    "\n",
    "\n",
    "kfold_single = run_kfold('single_channel5_dataset.csv', 'Single-Channel (c5 only)')\n",
    "kfold_multi  = run_kfold('multi_channel4_dataset.csv',  'Multi-Channel (c1,c3,c4,c7)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── K-Fold comparison plots ────────────────────────────────────────────\n",
    "_kf = [kfold_single, kfold_multi]\n",
    "_colors = ['#3498db', '#e74c3c']\n",
    "_mk = ['acc','f1_macro','f1_los','f1_nlos','auc']\n",
    "_mn = ['Accuracy','Macro F1','F1 LOS','F1 NLOS','AUC']\n",
    "width = 0.3\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(18, 6))\n",
    "\n",
    "# Bar with error bars\n",
    "ax = axs[0]\n",
    "x = np.arange(len(_mk))\n",
    "for i, (kf, c) in enumerate(zip(_kf, _colors)):\n",
    "    means = [kf[m]['mean'] for m in _mk]\n",
    "    stds  = [kf[m]['std']  for m in _mk]\n",
    "    bars = ax.bar(x+i*width, means, width, yerr=stds, label=kf['label'],\n",
    "                  color=c, alpha=0.85, capsize=4, error_kw={'elinewidth':1.5})\n",
    "    for bar, m, s in zip(bars, means, stds):\n",
    "        ax.text(bar.get_x()+bar.get_width()/2, bar.get_height()+s+0.008,\n",
    "                f'{m:.3f}', ha='center', va='bottom', fontsize=7.5, fontweight='bold')\n",
    "ax.set_xticks(x+width/2); ax.set_xticklabels(_mn, rotation=10)\n",
    "ax.set_ylim([0.4,1.15]); ax.set_title('5-Fold CV: Mean ± Std', fontweight='bold')\n",
    "ax.set_ylabel('Score'); ax.legend(fontsize=9); ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Box plots\n",
    "ax = axs[1]\n",
    "all_data, all_colors, positions = [], [], []\n",
    "for mi, metric in enumerate(_mk):\n",
    "    for ki, (kf, c) in enumerate(zip(_kf, _colors)):\n",
    "        positions.append(mi*3 + ki)\n",
    "        all_data.append(kf[metric]['all'])\n",
    "        all_colors.append(c)\n",
    "bp = ax.boxplot(all_data, positions=positions, widths=0.7, patch_artist=True,\n",
    "                medianprops={'color':'black','linewidth':2})\n",
    "for patch, c in zip(bp['boxes'], all_colors):\n",
    "    patch.set_facecolor(c); patch.set_alpha(0.7)\n",
    "ax.set_xticks([mi*3+0.5 for mi in range(len(_mk))])\n",
    "ax.set_xticklabels(_mn, rotation=10)\n",
    "ax.set_title('Score Distribution per Fold (blue=single, red=multi)', fontweight='bold')\n",
    "ax.set_ylabel('Score'); ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.suptitle('LSTM Baseline — Stage 1: Stratified 5-Fold CV — Single vs Multi Channel',\n",
    "             fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "print('\\n' + '='*65)\n",
    "print(f\"{'Metric':<12} {'Single Mean±Std':>20} {'Multi Mean±Std':>20}  Delta\")\n",
    "print('='*65)\n",
    "for k, n in zip(_mk, _mn):\n",
    "    sm,ss = kfold_single[k]['mean'], kfold_single[k]['std']\n",
    "    mm,ms = kfold_multi[k]['mean'],  kfold_multi[k]['std']\n",
    "    d = mm - sm\n",
    "    print(f\"{n:<12} {sm:>8.4f}±{ss:.4f}      {mm:>8.4f}±{ms:.4f}   {'▲' if d>0 else '▼'}{abs(d):.4f}\")\n",
    "print('='*65)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}